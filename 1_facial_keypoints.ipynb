{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Keypoint Detection\n",
    "\n",
    "Welcome back for exercise 9! As we told you, the exercises of this lecture can be subdivided into mainly two parts. The first part in which we re-invented the wheel and implemented the most important methods on our own and the second part where we start using existing libraries (that already have implemented all the methods) and start playing around with more complex network architectures. \n",
    "\n",
    "We already entered stage two, but with the introduction of convolution neural networks this week, we are given a very powerful tool that we want to explore in this exercises. So let us start with this week's exercise, where we ask you to build a convolution neural network to perform facial keypoint detection. \n",
    "\n",
    "Before we start, let's take a look at some example images and corresponding facial keypoints:\n",
    "\n",
    "<img src='images/key_pts_example.png' width=70% height=70%/>\n",
    "\n",
    "The facial keypoints (also called facial landmarks) are the small magenta dots shown on each of the faces in the images above. These keypoints mark important areas of the face: the eyes, corners of the mouth, the nose, etc. and are relevant for a variety of computer vision tasks, such as face filters, emotion recognition, pose recognition, and more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from exercise_code.data.facial_keypoints_dataset import FacialKeypointsDataset\n",
    "from exercise_code.networks.keypoint_nn import (\n",
    "    DummyKeypointModel,\n",
    "    KeypointModel\n",
    ")\n",
    "from exercise_code.util import (\n",
    "    show_all_keypoints,\n",
    "    save_model,\n",
    ")\n",
    "from exercise_code.tests import test_keypoint_nn\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note: Google Colab</h3>\n",
    "    <p>\n",
    "In case you don't have a GPU, you can run this notebook on Google Colab where you can access a GPU for free, but you can also run this notebook on your CPU.\n",
    "         </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Visualize Data\n",
    "To load the data, we have already prepared a Pytorch Dataset class `FacialKeypointsDataset` for you. You can find it in `exercise_code/data/facial_keypoints_dataset.py`. Run the following cell to download the data and initialize your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1546\n",
      "Number of validation samples: 298\n"
     ]
    }
   ],
   "source": [
    "download_url = 'https://vision.in.tum.de/webshare/g/i2dl/facial_keypoints.zip'\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_root = os.path.join(i2dl_exercises_path, \"datasets\", \"facial_keypoints\")\n",
    "train_dataset = FacialKeypointsDataset(\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    root=data_root,\n",
    "    download_url=download_url\n",
    ")\n",
    "val_dataset = FacialKeypointsDataset(\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    "    root=data_root,\n",
    ")\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample in our dataset is a dict `{\"image\": image, \"keypoints\": keypoints}`, where\n",
    " * `image` is a [0-1]-normalized gray-scale image of size 96x96, represented by a torch tensor of shape (CxHxW) with C=1, H=96, W=96\n",
    "    <img style=\"float: right;\" src='images/key_pts_expl.png' width=50% height=50%/>\n",
    " * `keypoints` is the list of K facial keypoints, stored in a torch tensor of shape (Kx2). We have K=15 keypoints that stand for:\n",
    "   * keypoints[0]: Center of the left eye\n",
    "   * keypoints[1]: Center of the right eye\n",
    "   * keypoints[2]: Left eye inner corner\n",
    "   * keypoints[3]: Left eye outer corner\n",
    "   * keypoints[4]: Right eye inner corner\n",
    "   * keypoints[5]: Right eye outer corner\n",
    "   * keypoints[6]: Left eyebrow inner end\n",
    "   * keypoints[7]: Left eyebrow outer end\n",
    "   * keypoints[8]: Right eyebrow inner end\n",
    "   * keypoints[9]: Right eyebrow outer end\n",
    "   * keypoints[10]: Nose tip\n",
    "   * keypoints[11]: Mouth left corner\n",
    "   * keypoints[12]: Mouth right corner\n",
    "   * keypoints[13]: Mouth center top lip\n",
    "   * keypoints[14]: Mouth center bottom lip\n",
    "   \n",
    "Each individual facial keypoint is represented by two coordinates (x,y) that specify the horizontal and vertical location of the keypoint respectively. All keypoint values are normalized to [-1,1], such that:\n",
    "   * (x=-1,y=-1) corresponds to the top left corner, \n",
    "   * (x=-1,y=1) to the bottom left corner,\n",
    "   * (x=1,y=-1) to the top right corner,\n",
    "   * (x=1,y=1) to the bottom right corner,\n",
    "   * and (x=0,y=0) to the center of the image.\n",
    "   \n",
    "      \n",
    "The data downloaded is already preprocessed and hence there is no need to apply transformations in order to prepare the data. Of course, feel free to apply training transformations to improve your performance such as e.g. flipping the training images. Do not forget that these transformations (data augmentation) are only applied to your training data. Your validation and test set remain untouched. Also, when applying transformations such as flipping, make sure that the predicted coordinates of your keypoints change accordingly.\n",
    "\n",
    "Let's have a look at the first training sample to get a better feeling for the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the image: torch.Size([1, 96, 96])\n",
      "Smallest value in the image: tensor(0.0118)\n",
      "Largest value in the image: tensor(1.)\n",
      "tensor([[[0.3804, 0.2039, 0.2275,  ..., 0.9922, 0.9961, 0.9961],\n",
      "         [0.3333, 0.2157, 0.2588,  ..., 0.9922, 0.9961, 0.9961],\n",
      "         [0.2941, 0.2588, 0.2902,  ..., 0.9922, 0.9961, 0.9961],\n",
      "         ...,\n",
      "         [0.1294, 0.1255, 0.1255,  ..., 0.9255, 1.0000, 1.0000],\n",
      "         [0.1294, 0.1255, 0.1216,  ..., 0.9490, 0.9843, 0.9804],\n",
      "         [0.1216, 0.1176, 0.1216,  ..., 0.9255, 1.0000, 0.9922]]])\n",
      "torch.Size([1, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "image, keypoints = train_dataset[0][\"image\"], train_dataset[0][\"keypoints\"]\n",
    "print(\"Shape of the image:\", image.size())\n",
    "print(\"Smallest value in the image:\", torch.min(image))\n",
    "print(\"Largest value in the image:\", torch.max(image))\n",
    "print(image)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4685, -0.2319],\n",
      "        [-0.4253, -0.1953],\n",
      "        [ 0.2908, -0.2214],\n",
      "        [ 0.5992, -0.2214],\n",
      "        [-0.2685, -0.2109],\n",
      "        [-0.5873, -0.1900],\n",
      "        [ 0.1967, -0.3827],\n",
      "        [ 0.7656, -0.4295],\n",
      "        [-0.2035, -0.3758],\n",
      "        [-0.7389, -0.3573],\n",
      "        [ 0.0086,  0.2333],\n",
      "        [ 0.4163,  0.6620],\n",
      "        [-0.3521,  0.6985],\n",
      "        [ 0.0138,  0.6045],\n",
      "        [ 0.0190,  0.9076]])\n",
      "torch.Size([15, 2])\n"
     ]
    }
   ],
   "source": [
    "keypoints = train_dataset[0][\"keypoints\"]\n",
    "print(keypoints)\n",
    "print(keypoints.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `exercise_code/util/vis_utils.py` we also provide you with a function `show_all_keypoints()` that takes in an image and keypoints and displays where the predicted keypoints are in the image. Let's use it to plot the first few images of our training set:\n",
    "\n",
    "**Note:** if your kernel dies when running the following cell, please uncomment the last line of the imports cell `os.environ['KMP_DUPLICATE_LIB_OK']='True'`and try it again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_keypoints(dataset, num_samples=3):\n",
    "    for i in range(num_samples):\n",
    "        image = dataset[i][\"image\"]\n",
    "        key_pts = dataset[i][\"keypoints\"]\n",
    "        show_all_keypoints(image, key_pts)\n",
    "show_keypoints(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Facial Keypoint Detection\n",
    "Your task is to define and train a model for facial keypoint detection.\n",
    "\n",
    "The facial keypoint detection task can be seen as a regression problem, where the goal is to predict 30 different values that correspond to the 15 facial keypoint locations. Thus, we need to build a network that gets a (1x96x96) image as input and predicts 30 continuous outputs between [-1,1].\n",
    "\n",
    "## Dummy Model\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>In <code>exercise_code/networks/keypoint_nn.py</code> we defined a naive <code>DummyKeypointModel</code>, which always predicts the keypoints of the first training image. Let's try it on a few images and visualize our predictions in red:\n",
    " </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_keypoint_predictions(model, dataset, num_samples=3):\n",
    "    for i in range(num_samples):\n",
    "        image = dataset[i][\"image\"]\n",
    "        key_pts = dataset[i][\"keypoints\"]\n",
    "        \n",
    "        #images=images.unsqueeze(0)\n",
    "        \n",
    "        \n",
    "        predicted_keypoints = torch.squeeze(model(image).detach()).view(15,2)\n",
    "        show_all_keypoints(image, key_pts, predicted_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = DummyKeypointModel()\n",
    "show_keypoint_predictions(dummy_model, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the model predicts the first sample perfectly, but for the remaining samples the predictions are quite off.\n",
    "\n",
    "## Loss and Metrics\n",
    "\n",
    "To measure the quality of the model's predictions, we will use the mean squared error (https://en.wikipedia.org/wiki/Mean_squared_error), summed up over all 30 keypoint locations. In PyTorch, the mean squared error is defined in `torch.nn.MSELoss()`, and we can use it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "for i in range(3):\n",
    "    image = train_dataset[i][\"image\"]\n",
    "    keypoints = train_dataset[i][\"keypoints\"]\n",
    "    predicted_keypoints = torch.squeeze(dummy_model(image)).view(15,2)\n",
    "    loss = loss_fn(keypoints, predicted_keypoints)\n",
    "    print(\"Loss on image %d:\" % i, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, our dummy model achieves a loss close to 0 on the first sample, but on all other samples the loss is quite high.\n",
    "\n",
    "To obtain an evaluation score (in the notebook and on the submission server), we will use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss = 0\n",
    "    for batch in dataloader:\n",
    "        image, keypoints = batch[\"image\"], batch[\"keypoints\"]\n",
    "        predicted_keypoints = model(image).view(-1,15,2)\n",
    "        loss += criterion(\n",
    "            torch.squeeze(keypoints),\n",
    "            torch.squeeze(predicted_keypoints)\n",
    "        ).item()\n",
    "    return 1.0 / (2 * (loss/len(dataloader)))\n",
    "\n",
    "print(\"Score of the Dummy Model:\", evaluate_model(dummy_model, val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To pass the assignment, you will need to achieve a score of at least 100**. As you can see, the score is calculated from the average loss, so **your average loss needs to be lower than 0.005**. Our dummy model only gets a score of around 60, so you will have to come up with a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Design your own model\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p> Now it is your turn to build your own model. To do so, you need to design a convolution neural network that takes images of size (Nx1x96x96) as input and produces outputs of shape (Nx30) in the range [-1,1]. Therefore, implement the <code>KeypointModel</code> class in <code>exercise_code/networks/keypoint_nn.py</code>.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "Recall that CNN's are defined by a few types of layers:\n",
    "* Convolution layers\n",
    "* Max-pooling layers\n",
    "* Fully-connected layers\n",
    "\n",
    "You can design your network however you want, but we strongly suggest to include multiple convolution layers. You are also encouraged to use things like dropout and batch normalization to stabilize and regularize your network. If you want to build a really competitive model, have a look at some literature on keypoint detection, such as [this paper](https://arxiv.org/pdf/1710.00977.pdf).\n",
    "\n",
    "#### Define your model in the provided file \n",
    "`exercise_code/networks/keypoint_nn.py` file\n",
    "\n",
    "This file is mostly empty but contains the expected class name, and the methods that your model needs to implement (only `forward()` basically). You are also free to decide whether you want to use PyTorch Lightning or not.\n",
    "The only rules your model design has to follow are:\n",
    "* Inherit from either `torch.nn.Module` or `pytorch_lightning.LightningModule`\n",
    "* Perform the forward pass in forward(), predicting keypoints of shape (Nx30) for images of shape (Nx1x96x96)\n",
    "* Have less than 5 million parameters\n",
    "* Have a model size of less than 20MB after saving\n",
    "\n",
    "Furthermore, you need to pass all your hyperparameters to the model in a single dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    # TODO: if you have any model arguments/hparams, define them here\n",
    "    \"batch_size\":20,\n",
    "    \"n_hidden\": 256,\n",
    "    \"learning_rate\":1.3e-3\n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test whether your model follows the basic rules, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeypointShapeTest passed.\n",
      "ParamCountTest passed. Your model has 3.550 mio. params.\n",
      "FileSizeTest passed. Your model is 14.2 MB large\n",
      "All tests passed for your model. Tests passed: 3/3\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [12, 1, 3, 3], but got 3-dimensional input of size [1, 96, 96] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7c304bf738f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeypointModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_keypoint_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mshow_keypoint_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-75842e6d3db6>\u001b[0m in \u001b[0;36mshow_keypoint_predictions\u001b[0;34m(model, dataset, num_samples)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mpredicted_keypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mshow_all_keypoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_pts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_keypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exercise_09/exercise_code/networks/keypoint_nn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m#x=x.view(-1,1,96,96)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m#x = self.layers(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# \u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [12, 1, 3, 3], but got 3-dimensional input of size [1, 96, 96] instead"
     ]
    }
   ],
   "source": [
    "model = KeypointModel(hparams)\n",
    "test_keypoint_nn(model)\n",
    "#show_keypoint_predictions(model, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train your model\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p> In addition to the network itself, you will also need to write the code for the model training. You can use PyTorch Lightning for that, or you can also write it yourself in standard PyTorch.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "**Hints:**\n",
    "* Use `torch.nn.MSELoss()` as loss function\n",
    "* Have a look at the previous notebooks for PyTorch and PyTorch Lightning in exercises 7 and 8 if you feel lost. In particular, revise `1_Cifar10_PytorchLightning.ipynb` in the optional submission from Week 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 10, Avg. Loss: 6.034353661868307\n",
      "Epoch: 1, Batch: 20, Avg. Loss: 3.8331779908192787\n",
      "Epoch: 1, Batch: 30, Avg. Loss: 2.8937202984916754\n",
      "Epoch: 1, Batch: 40, Avg. Loss: 2.3546132410948095\n",
      "Epoch: 1, Batch: 50, Avg. Loss: 2.0286047960422477\n",
      "Epoch: 1, Batch: 60, Avg. Loss: 1.8351245676561938\n",
      "Epoch: 1, Batch: 70, Avg. Loss: 1.6749349493479384\n",
      "Epoch: 1, Batch: 80, Avg. Loss: 1.520948987784265\n",
      "Epoch: 1, Batch: 90, Avg. Loss: 1.414720544486903\n",
      "Epoch: 1, Batch: 100, Avg. Loss: 1.3393581845242568\n",
      "Epoch: 1, Batch: 110, Avg. Loss: 1.2692462767756314\n",
      "Epoch: 1, Batch: 120, Avg. Loss: 1.1985562333539754\n",
      "Epoch: 1, Batch: 130, Avg. Loss: 1.1576913412689238\n",
      "Epoch: 1, Batch: 140, Avg. Loss: 1.1044620541359882\n",
      "Epoch: 1, Batch: 150, Avg. Loss: 1.060579231621435\n",
      "Epoch: 1, Batch: 160, Avg. Loss: 1.0198332577381495\n",
      "Epoch: 1, Batch: 170, Avg. Loss: 0.9817145943818008\n",
      "Epoch: 1, Batch: 180, Avg. Loss: 0.9447239986178595\n",
      "Epoch: 1, Batch: 190, Avg. Loss: 0.9088688483313908\n",
      "Epoch: 1, Batch: 200, Avg. Loss: 0.8749486880011894\n",
      "Epoch: 1, Batch: 210, Avg. Loss: 0.8440822915811288\n",
      "Epoch: 1, Batch: 220, Avg. Loss: 0.8155664269850679\n",
      "Epoch: 1, Batch: 230, Avg. Loss: 0.7896985746432079\n",
      "Epoch: 1, Batch: 240, Avg. Loss: 0.762856547802562\n",
      "Epoch: 1, Batch: 250, Avg. Loss: 0.740222264934017\n",
      "Epoch: 1, Batch: 260, Avg. Loss: 0.7180347744677518\n",
      "Epoch: 1, Batch: 270, Avg. Loss: 0.6962529008993429\n",
      "Epoch: 1, Batch: 280, Avg. Loss: 0.6765157145624947\n",
      "Epoch: 1, Batch: 290, Avg. Loss: 0.6614896585217397\n",
      "Epoch: 1, Batch: 300, Avg. Loss: 0.6441342835392442\n",
      "Epoch: 1, Batch: 310, Avg. Loss: 0.6287329908713554\n",
      "Epoch: 1, Batch: 320, Avg. Loss: 0.6132111612205221\n",
      "Epoch: 1, Batch: 330, Avg. Loss: 0.5982235258382866\n",
      "Epoch: 1, Batch: 340, Avg. Loss: 0.5837092266409798\n",
      "Epoch: 1, Batch: 350, Avg. Loss: 0.5697121334643794\n",
      "Epoch: 1, Batch: 360, Avg. Loss: 0.5563534098083263\n",
      "Epoch: 1, Batch: 370, Avg. Loss: 0.5428324002560562\n",
      "Epoch: 1, Batch: 380, Avg. Loss: 0.5306365451920473\n",
      "Epoch: 1, Batch: 390, Avg. Loss: 0.5188562494623631\n",
      "Epoch: 1, Batch: 400, Avg. Loss: 0.5073953709170633\n",
      "Epoch: 1, Batch: 410, Avg. Loss: 0.49688186641318\n",
      "Epoch: 1, Batch: 420, Avg. Loss: 0.4865451332421747\n",
      "Epoch: 1, Batch: 430, Avg. Loss: 0.47642147567097126\n",
      "Epoch: 1, Batch: 440, Avg. Loss: 0.467646910747458\n",
      "Epoch: 1, Batch: 450, Avg. Loss: 0.45827432275067326\n",
      "Epoch: 1, Batch: 460, Avg. Loss: 0.45006271804356235\n",
      "Epoch: 1, Batch: 470, Avg. Loss: 0.4424683828828241\n",
      "Epoch: 1, Batch: 480, Avg. Loss: 0.43472571736726606\n",
      "Epoch: 1, Batch: 490, Avg. Loss: 0.4270682758661197\n",
      "Epoch: 1, Batch: 500, Avg. Loss: 0.4200226087421776\n",
      "Epoch: 1, Batch: 510, Avg. Loss: 0.4131916890246519\n",
      "Epoch: 1, Batch: 520, Avg. Loss: 0.4065796035828216\n",
      "Epoch: 1, Batch: 530, Avg. Loss: 0.3999974665800655\n",
      "Epoch: 1, Batch: 540, Avg. Loss: 0.39380182390807955\n",
      "Epoch: 1, Batch: 550, Avg. Loss: 0.3877904286543637\n",
      "Epoch: 1, Batch: 560, Avg. Loss: 0.38163232843996375\n",
      "Epoch: 1, Batch: 570, Avg. Loss: 0.3756628558497961\n",
      "Epoch: 1, Batch: 580, Avg. Loss: 0.3700366738849096\n",
      "Epoch: 1, Batch: 590, Avg. Loss: 0.3643383971069179\n",
      "Epoch: 1, Batch: 600, Avg. Loss: 0.3587991365687129\n",
      "Epoch: 1, Batch: 610, Avg. Loss: 0.35343278071434775\n",
      "Epoch: 1, Batch: 620, Avg. Loss: 0.34820062891461306\n",
      "Epoch: 1, Batch: 630, Avg. Loss: 0.34324728287253026\n",
      "Epoch: 1, Batch: 640, Avg. Loss: 0.3384059359201453\n",
      "Epoch: 1, Batch: 650, Avg. Loss: 0.3336365116897896\n",
      "Epoch: 1, Batch: 660, Avg. Loss: 0.32906244236450183\n",
      "Epoch: 1, Batch: 670, Avg. Loss: 0.3246190804982577\n",
      "Epoch: 1, Batch: 680, Avg. Loss: 0.3202477748833586\n",
      "Epoch: 1, Batch: 690, Avg. Loss: 0.3161417973972587\n",
      "Epoch: 1, Batch: 700, Avg. Loss: 0.3123249975892479\n",
      "Epoch: 1, Batch: 710, Avg. Loss: 0.3085678088451159\n",
      "Epoch: 1, Batch: 720, Avg. Loss: 0.30475675735508656\n",
      "Epoch: 1, Batch: 730, Avg. Loss: 0.3009938069163159\n",
      "Epoch: 1, Batch: 740, Avg. Loss: 0.29733879297356725\n",
      "Epoch: 1, Batch: 750, Avg. Loss: 0.2936902689935269\n",
      "Epoch: 1, Batch: 760, Avg. Loss: 0.2901928848126837\n",
      "Epoch: 1, Batch: 770, Avg. Loss: 0.28674511707016065\n",
      "Epoch: 1, Batch: 780, Avg. Loss: 0.28344781714609907\n",
      "Epoch: 1, Batch: 790, Avg. Loss: 0.28020770565449665\n",
      "Epoch: 1, Batch: 800, Avg. Loss: 0.2770502741841392\n",
      "Epoch: 1, Batch: 810, Avg. Loss: 0.2738554442269159\n",
      "Epoch: 1, Batch: 820, Avg. Loss: 0.270822973968518\n",
      "Epoch: 1, Batch: 830, Avg. Loss: 0.2678388346343855\n",
      "Epoch: 1, Batch: 840, Avg. Loss: 0.26499435046179326\n",
      "Epoch: 1, Batch: 850, Avg. Loss: 0.2621740489261328\n",
      "Epoch: 1, Batch: 860, Avg. Loss: 0.2593839113851001\n",
      "Epoch: 1, Batch: 870, Avg. Loss: 0.25671084116612775\n",
      "Epoch: 1, Batch: 880, Avg. Loss: 0.2540445740136558\n",
      "Epoch: 1, Batch: 890, Avg. Loss: 0.25149772749821936\n",
      "Epoch: 1, Batch: 900, Avg. Loss: 0.24902822643319544\n",
      "Epoch: 1, Batch: 910, Avg. Loss: 0.24652198012979706\n",
      "Epoch: 1, Batch: 920, Avg. Loss: 0.24410624623245947\n",
      "Epoch: 1, Batch: 930, Avg. Loss: 0.24173739864676388\n",
      "Epoch: 1, Batch: 940, Avg. Loss: 0.23944636564443922\n",
      "Epoch: 1, Batch: 950, Avg. Loss: 0.23718799196293594\n",
      "Epoch: 1, Batch: 960, Avg. Loss: 0.23496981550151788\n",
      "Epoch: 1, Batch: 970, Avg. Loss: 0.23278247270833075\n",
      "Epoch: 1, Batch: 980, Avg. Loss: 0.2306546415920551\n",
      "Epoch: 1, Batch: 990, Avg. Loss: 0.2286356621364431\n",
      "Epoch: 1, Batch: 1000, Avg. Loss: 0.22670739007511506\n",
      "Epoch: 1, Batch: 1010, Avg. Loss: 0.2247706712615871\n",
      "Epoch: 1, Batch: 1020, Avg. Loss: 0.22291693831789394\n",
      "Epoch: 1, Batch: 1030, Avg. Loss: 0.2211311517484247\n",
      "Epoch: 1, Batch: 1040, Avg. Loss: 0.21929396529295647\n",
      "Epoch: 1, Batch: 1050, Avg. Loss: 0.2176961248573644\n",
      "Epoch: 1, Batch: 1060, Avg. Loss: 0.21607824445484916\n",
      "Epoch: 1, Batch: 1070, Avg. Loss: 0.21450899201519336\n",
      "Epoch: 1, Batch: 1080, Avg. Loss: 0.21303709889084543\n",
      "Epoch: 1, Batch: 1090, Avg. Loss: 0.21160825637243585\n",
      "Epoch: 1, Batch: 1100, Avg. Loss: 0.2101503334612373\n",
      "Epoch: 1, Batch: 1110, Avg. Loss: 0.20872240690506047\n",
      "Epoch: 1, Batch: 1120, Avg. Loss: 0.20717156619928823\n",
      "Epoch: 1, Batch: 1130, Avg. Loss: 0.20568313080144335\n",
      "Epoch: 1, Batch: 1140, Avg. Loss: 0.2041623239357211\n",
      "Epoch: 1, Batch: 1150, Avg. Loss: 0.20272190877122137\n",
      "Epoch: 1, Batch: 1160, Avg. Loss: 0.20144146785776454\n",
      "Epoch: 1, Batch: 1170, Avg. Loss: 0.20018719470206492\n",
      "Epoch: 1, Batch: 1180, Avg. Loss: 0.19882674805940057\n",
      "Epoch: 1, Batch: 1190, Avg. Loss: 0.19746659220101528\n",
      "Epoch: 1, Batch: 1200, Avg. Loss: 0.19617248707768617\n",
      "Epoch: 1, Batch: 1210, Avg. Loss: 0.19480048731197444\n",
      "Epoch: 1, Batch: 1220, Avg. Loss: 0.19342253713962532\n",
      "Epoch: 1, Batch: 1230, Avg. Loss: 0.19212009716381911\n",
      "Epoch: 1, Batch: 1240, Avg. Loss: 0.19077345389042943\n",
      "Epoch: 1, Batch: 1250, Avg. Loss: 0.18946568304680295\n",
      "Epoch: 1, Batch: 1260, Avg. Loss: 0.18820293029256882\n",
      "Epoch: 1, Batch: 1270, Avg. Loss: 0.18689068547656232\n",
      "Epoch: 1, Batch: 1280, Avg. Loss: 0.1856360775414939\n",
      "Epoch: 1, Batch: 1290, Avg. Loss: 0.18436458113020224\n",
      "Epoch: 1, Batch: 1300, Avg. Loss: 0.1832077151000947\n",
      "Epoch: 1, Batch: 1310, Avg. Loss: 0.18203558650143825\n",
      "Epoch: 1, Batch: 1320, Avg. Loss: 0.18092405146737728\n",
      "Epoch: 1, Batch: 1330, Avg. Loss: 0.17978737334965358\n",
      "Epoch: 1, Batch: 1340, Avg. Loss: 0.17860531717082562\n",
      "Epoch: 1, Batch: 1350, Avg. Loss: 0.17745045027188167\n",
      "Epoch: 1, Batch: 1360, Avg. Loss: 0.1763120721210126\n",
      "Epoch: 1, Batch: 1370, Avg. Loss: 0.17512641795125156\n",
      "Epoch: 1, Batch: 1380, Avg. Loss: 0.17399687500484154\n",
      "Epoch: 1, Batch: 1390, Avg. Loss: 0.17288842587379766\n",
      "Epoch: 1, Batch: 1400, Avg. Loss: 0.1717907242064868\n",
      "Epoch: 1, Batch: 1410, Avg. Loss: 0.17073841266052478\n",
      "Epoch: 1, Batch: 1420, Avg. Loss: 0.16968625884042388\n",
      "Epoch: 1, Batch: 1430, Avg. Loss: 0.1686150233027968\n",
      "Epoch: 1, Batch: 1440, Avg. Loss: 0.1676353612125743\n",
      "Epoch: 1, Batch: 1450, Avg. Loss: 0.16662727426024113\n",
      "Epoch: 1, Batch: 1460, Avg. Loss: 0.1656526336704928\n",
      "Epoch: 1, Batch: 1470, Avg. Loss: 0.1647009067639957\n",
      "Epoch: 1, Batch: 1480, Avg. Loss: 0.16375671590359137\n",
      "Epoch: 1, Batch: 1490, Avg. Loss: 0.1628310799744428\n",
      "Epoch: 1, Batch: 1500, Avg. Loss: 0.1619069767749935\n",
      "Epoch: 1, Batch: 1510, Avg. Loss: 0.16098483505481676\n",
      "Epoch: 1, Batch: 1520, Avg. Loss: 0.16005990315422444\n",
      "Epoch: 1, Batch: 1530, Avg. Loss: 0.15915430215491314\n",
      "Epoch: 1, Batch: 1540, Avg. Loss: 0.15826541221203783\n",
      "Epoch: 2, Batch: 10, Avg. Loss: 0.15682134874752002\n",
      "Epoch: 2, Batch: 20, Avg. Loss: 0.1559186580183264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Batch: 30, Avg. Loss: 0.15502166515127533\n",
      "Epoch: 2, Batch: 40, Avg. Loss: 0.15410007211212245\n",
      "Epoch: 2, Batch: 50, Avg. Loss: 0.1531967959822182\n",
      "Epoch: 2, Batch: 60, Avg. Loss: 0.1523096807289439\n",
      "Epoch: 2, Batch: 70, Avg. Loss: 0.1514372977523289\n",
      "Epoch: 2, Batch: 80, Avg. Loss: 0.15055674394162802\n",
      "Epoch: 2, Batch: 90, Avg. Loss: 0.1496995709665598\n",
      "Epoch: 2, Batch: 100, Avg. Loss: 0.14884218310931624\n",
      "Epoch: 2, Batch: 110, Avg. Loss: 0.14799041150396638\n",
      "Epoch: 2, Batch: 120, Avg. Loss: 0.14714645464812312\n",
      "Epoch: 2, Batch: 130, Avg. Loss: 0.14631476821822684\n",
      "Epoch: 2, Batch: 140, Avg. Loss: 0.14549620774552469\n",
      "Epoch: 2, Batch: 150, Avg. Loss: 0.14467896172230113\n",
      "Epoch: 2, Batch: 160, Avg. Loss: 0.1438733524963795\n",
      "Epoch: 2, Batch: 170, Avg. Loss: 0.14308160093718864\n",
      "Epoch: 2, Batch: 180, Avg. Loss: 0.14229115416228338\n",
      "Epoch: 2, Batch: 190, Avg. Loss: 0.14151096876324168\n",
      "Epoch: 2, Batch: 200, Avg. Loss: 0.14074793347213577\n",
      "Epoch: 2, Batch: 210, Avg. Loss: 0.13999076865963703\n",
      "Epoch: 2, Batch: 220, Avg. Loss: 0.13923730734398024\n",
      "Epoch: 2, Batch: 230, Avg. Loss: 0.13850608701148714\n",
      "Epoch: 2, Batch: 240, Avg. Loss: 0.13777828683544482\n",
      "Epoch: 2, Batch: 250, Avg. Loss: 0.1370703099832499\n",
      "Epoch: 2, Batch: 260, Avg. Loss: 0.1363522612796761\n",
      "Epoch: 2, Batch: 270, Avg. Loss: 0.13564049732863656\n",
      "Epoch: 2, Batch: 280, Avg. Loss: 0.13494701638639178\n",
      "Epoch: 2, Batch: 290, Avg. Loss: 0.13425337498733514\n",
      "Epoch: 2, Batch: 300, Avg. Loss: 0.13356961751202762\n",
      "Epoch: 2, Batch: 310, Avg. Loss: 0.13288666196882906\n",
      "Epoch: 2, Batch: 320, Avg. Loss: 0.13221050150387847\n",
      "Epoch: 2, Batch: 330, Avg. Loss: 0.13153897657692432\n",
      "Epoch: 2, Batch: 340, Avg. Loss: 0.13087343106846713\n",
      "Epoch: 2, Batch: 350, Avg. Loss: 0.1302119249049181\n",
      "Epoch: 2, Batch: 360, Avg. Loss: 0.12956134121378596\n",
      "Epoch: 2, Batch: 370, Avg. Loss: 0.12891110950234705\n",
      "Epoch: 2, Batch: 380, Avg. Loss: 0.1282841778684456\n",
      "Epoch: 2, Batch: 390, Avg. Loss: 0.12766649199226143\n",
      "Epoch: 2, Batch: 400, Avg. Loss: 0.12704407148144584\n",
      "Epoch: 2, Batch: 410, Avg. Loss: 0.12643260986155944\n",
      "Epoch: 2, Batch: 420, Avg. Loss: 0.12582524632466316\n",
      "Epoch: 2, Batch: 430, Avg. Loss: 0.1252116801667534\n",
      "Epoch: 2, Batch: 440, Avg. Loss: 0.12461765188400069\n",
      "Epoch: 2, Batch: 450, Avg. Loss: 0.1240212840947759\n",
      "Epoch: 2, Batch: 460, Avg. Loss: 0.12343594638277727\n",
      "Epoch: 2, Batch: 470, Avg. Loss: 0.12285567398596713\n",
      "Epoch: 2, Batch: 480, Avg. Loss: 0.12227468550343204\n",
      "Epoch: 2, Batch: 490, Avg. Loss: 0.12169785245930369\n",
      "Epoch: 2, Batch: 500, Avg. Loss: 0.12113082228333195\n",
      "Epoch: 2, Batch: 510, Avg. Loss: 0.12057659220145313\n",
      "Epoch: 2, Batch: 520, Avg. Loss: 0.12001654861323517\n",
      "Epoch: 2, Batch: 530, Avg. Loss: 0.11946240371598088\n",
      "Epoch: 2, Batch: 540, Avg. Loss: 0.11892277190955333\n",
      "Epoch: 2, Batch: 550, Avg. Loss: 0.11837751162293987\n",
      "Epoch: 2, Batch: 560, Avg. Loss: 0.11783695039350424\n",
      "Epoch: 2, Batch: 570, Avg. Loss: 0.11730453461589155\n",
      "Epoch: 2, Batch: 580, Avg. Loss: 0.11678447464404298\n",
      "Epoch: 2, Batch: 590, Avg. Loss: 0.11626578733898629\n",
      "Epoch: 2, Batch: 600, Avg. Loss: 0.11575012610267874\n",
      "Epoch: 2, Batch: 610, Avg. Loss: 0.1152379724716772\n",
      "Epoch: 2, Batch: 620, Avg. Loss: 0.11472912834135614\n",
      "Epoch: 2, Batch: 630, Avg. Loss: 0.11423158312548251\n",
      "Epoch: 2, Batch: 640, Avg. Loss: 0.11373309397119474\n",
      "Epoch: 2, Batch: 650, Avg. Loss: 0.11323575078115156\n",
      "Epoch: 2, Batch: 660, Avg. Loss: 0.11274827290094054\n",
      "Epoch: 2, Batch: 670, Avg. Loss: 0.11226101417575898\n",
      "Epoch: 2, Batch: 680, Avg. Loss: 0.11177359312730893\n",
      "Epoch: 2, Batch: 690, Avg. Loss: 0.11130473080463082\n",
      "Epoch: 2, Batch: 700, Avg. Loss: 0.11083703287260063\n",
      "Epoch: 2, Batch: 710, Avg. Loss: 0.11037356400033232\n",
      "Epoch: 2, Batch: 720, Avg. Loss: 0.1099118512209657\n",
      "Epoch: 2, Batch: 730, Avg. Loss: 0.10944628084840888\n",
      "Epoch: 2, Batch: 740, Avg. Loss: 0.1089909618920127\n",
      "Epoch: 2, Batch: 750, Avg. Loss: 0.1085382575074856\n",
      "Epoch: 2, Batch: 760, Avg. Loss: 0.10809526576775726\n",
      "Epoch: 2, Batch: 770, Avg. Loss: 0.10765165420144507\n",
      "Epoch: 2, Batch: 780, Avg. Loss: 0.10721765059226704\n",
      "Epoch: 2, Batch: 790, Avg. Loss: 0.10678889282502442\n",
      "Epoch: 2, Batch: 800, Avg. Loss: 0.10635933272862262\n",
      "Epoch: 2, Batch: 810, Avg. Loss: 0.10592587067589417\n",
      "Epoch: 2, Batch: 820, Avg. Loss: 0.10551020517634192\n",
      "Epoch: 2, Batch: 830, Avg. Loss: 0.10509066060626585\n",
      "Epoch: 2, Batch: 840, Avg. Loss: 0.10468002939264855\n",
      "Epoch: 2, Batch: 850, Avg. Loss: 0.10427178192002959\n",
      "Epoch: 2, Batch: 860, Avg. Loss: 0.10386720432671419\n",
      "Epoch: 2, Batch: 870, Avg. Loss: 0.10345968398049582\n",
      "Epoch: 2, Batch: 880, Avg. Loss: 0.10306225447982703\n",
      "Epoch: 2, Batch: 890, Avg. Loss: 0.10268340997638858\n",
      "Epoch: 2, Batch: 900, Avg. Loss: 0.10231055593106644\n",
      "Epoch: 2, Batch: 910, Avg. Loss: 0.10191742812971871\n",
      "Epoch: 2, Batch: 920, Avg. Loss: 0.10153225194417227\n",
      "Epoch: 2, Batch: 930, Avg. Loss: 0.10114889386862591\n",
      "Epoch: 2, Batch: 940, Avg. Loss: 0.10077398798064942\n",
      "Epoch: 2, Batch: 950, Avg. Loss: 0.10040055063006306\n",
      "Epoch: 2, Batch: 960, Avg. Loss: 0.10002192237884729\n",
      "Epoch: 2, Batch: 970, Avg. Loss: 0.09964613574316307\n",
      "Epoch: 2, Batch: 980, Avg. Loss: 0.09927721937168574\n",
      "Epoch: 2, Batch: 990, Avg. Loss: 0.09892508951401525\n",
      "Epoch: 2, Batch: 1000, Avg. Loss: 0.09860281099159131\n",
      "Epoch: 2, Batch: 1010, Avg. Loss: 0.09830701295678265\n",
      "Epoch: 2, Batch: 1020, Avg. Loss: 0.09799072942551276\n",
      "Epoch: 2, Batch: 1030, Avg. Loss: 0.09767910913781272\n",
      "Epoch: 2, Batch: 1040, Avg. Loss: 0.09735025173959248\n",
      "Epoch: 2, Batch: 1050, Avg. Loss: 0.09706277988163795\n",
      "Epoch: 2, Batch: 1060, Avg. Loss: 0.09677729112124732\n",
      "Epoch: 2, Batch: 1070, Avg. Loss: 0.09650648044516648\n",
      "Epoch: 2, Batch: 1080, Avg. Loss: 0.09620067073662011\n",
      "Epoch: 2, Batch: 1090, Avg. Loss: 0.09589860765767094\n",
      "Epoch: 2, Batch: 1100, Avg. Loss: 0.09560159319450125\n",
      "Epoch: 2, Batch: 1110, Avg. Loss: 0.09529447484710614\n",
      "Epoch: 2, Batch: 1120, Avg. Loss: 0.09497734722069781\n",
      "Epoch: 2, Batch: 1130, Avg. Loss: 0.09467107841127038\n",
      "Epoch: 2, Batch: 1140, Avg. Loss: 0.09436102244921472\n",
      "Epoch: 2, Batch: 1150, Avg. Loss: 0.094059611081728\n",
      "Epoch: 2, Batch: 1160, Avg. Loss: 0.09376512653738357\n",
      "Epoch: 2, Batch: 1170, Avg. Loss: 0.09347337523079091\n",
      "Epoch: 2, Batch: 1180, Avg. Loss: 0.0931697634906602\n",
      "Epoch: 2, Batch: 1190, Avg. Loss: 0.09286953263219712\n",
      "Epoch: 2, Batch: 1200, Avg. Loss: 0.0925867474916135\n",
      "Epoch: 2, Batch: 1210, Avg. Loss: 0.09228462925123325\n",
      "Epoch: 2, Batch: 1220, Avg. Loss: 0.09197999954688452\n",
      "Epoch: 2, Batch: 1230, Avg. Loss: 0.0916978673286199\n",
      "Epoch: 2, Batch: 1240, Avg. Loss: 0.09140319825038046\n",
      "Epoch: 2, Batch: 1250, Avg. Loss: 0.0911080886992211\n",
      "Epoch: 2, Batch: 1260, Avg. Loss: 0.09083082477691619\n",
      "Epoch: 2, Batch: 1270, Avg. Loss: 0.09054239435645803\n",
      "Epoch: 2, Batch: 1280, Avg. Loss: 0.09025633115824147\n",
      "Epoch: 2, Batch: 1290, Avg. Loss: 0.08997531563915785\n",
      "Epoch: 2, Batch: 1300, Avg. Loss: 0.08972509262276411\n",
      "Epoch: 2, Batch: 1310, Avg. Loss: 0.08946326272570301\n",
      "Epoch: 2, Batch: 1320, Avg. Loss: 0.08919605095708258\n",
      "Epoch: 2, Batch: 1330, Avg. Loss: 0.08892344709817807\n",
      "Epoch: 2, Batch: 1340, Avg. Loss: 0.08864468246177032\n",
      "Epoch: 2, Batch: 1350, Avg. Loss: 0.08836940076171593\n",
      "Epoch: 2, Batch: 1360, Avg. Loss: 0.08809317569488823\n",
      "Epoch: 2, Batch: 1370, Avg. Loss: 0.08780960697412338\n",
      "Epoch: 2, Batch: 1380, Avg. Loss: 0.08753370578305271\n",
      "Epoch: 2, Batch: 1390, Avg. Loss: 0.0872604196243864\n",
      "Epoch: 2, Batch: 1400, Avg. Loss: 0.08699098351879554\n",
      "Epoch: 2, Batch: 1410, Avg. Loss: 0.0867248462395689\n",
      "Epoch: 2, Batch: 1420, Avg. Loss: 0.08645350092630928\n",
      "Epoch: 2, Batch: 1430, Avg. Loss: 0.08618825008161365\n",
      "Epoch: 2, Batch: 1440, Avg. Loss: 0.08592799526726642\n",
      "Epoch: 2, Batch: 1450, Avg. Loss: 0.08566564164631292\n",
      "Epoch: 2, Batch: 1460, Avg. Loss: 0.0854068908446814\n",
      "Epoch: 2, Batch: 1470, Avg. Loss: 0.08515179309396267\n",
      "Epoch: 2, Batch: 1480, Avg. Loss: 0.0848999375186491\n",
      "Epoch: 2, Batch: 1490, Avg. Loss: 0.08464203554112004\n",
      "Epoch: 2, Batch: 1500, Avg. Loss: 0.08439711403095565\n",
      "Epoch: 2, Batch: 1510, Avg. Loss: 0.08414096422104772\n",
      "Epoch: 2, Batch: 1520, Avg. Loss: 0.08388326408259486\n",
      "Epoch: 2, Batch: 1530, Avg. Loss: 0.08363147738109517\n",
      "Epoch: 2, Batch: 1540, Avg. Loss: 0.08338089215575235\n",
      "Epoch: 3, Batch: 10, Avg. Loss: 0.08297907802979385\n",
      "Epoch: 3, Batch: 20, Avg. Loss: 0.08272779558937038\n",
      "Epoch: 3, Batch: 30, Avg. Loss: 0.08247785443516621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Batch: 40, Avg. Loss: 0.08222476973900936\n",
      "Epoch: 3, Batch: 50, Avg. Loss: 0.08197284583647227\n",
      "Epoch: 3, Batch: 60, Avg. Loss: 0.08172602658230967\n",
      "Epoch: 3, Batch: 70, Avg. Loss: 0.08147787502903477\n",
      "Epoch: 3, Batch: 80, Avg. Loss: 0.08123316831557237\n",
      "Epoch: 3, Batch: 90, Avg. Loss: 0.08099197333625024\n",
      "Epoch: 3, Batch: 100, Avg. Loss: 0.08075019889314893\n",
      "Epoch: 3, Batch: 110, Avg. Loss: 0.08050960644070607\n",
      "Epoch: 3, Batch: 120, Avg. Loss: 0.08026870571061498\n",
      "Epoch: 3, Batch: 130, Avg. Loss: 0.08002824461741331\n",
      "Epoch: 3, Batch: 140, Avg. Loss: 0.07979345402299276\n",
      "Epoch: 3, Batch: 150, Avg. Loss: 0.07955635470634574\n",
      "Epoch: 3, Batch: 160, Avg. Loss: 0.07932147433772462\n",
      "Epoch: 3, Batch: 170, Avg. Loss: 0.07908666662069175\n",
      "Epoch: 3, Batch: 180, Avg. Loss: 0.0788521515192512\n",
      "Epoch: 3, Batch: 190, Avg. Loss: 0.07862021065043105\n",
      "Epoch: 3, Batch: 200, Avg. Loss: 0.07838968563154133\n",
      "Epoch: 3, Batch: 210, Avg. Loss: 0.0781613048816178\n",
      "Epoch: 3, Batch: 220, Avg. Loss: 0.0779346547491433\n",
      "Epoch: 3, Batch: 230, Avg. Loss: 0.0777084565033091\n",
      "Epoch: 3, Batch: 240, Avg. Loss: 0.07748395967277615\n",
      "Epoch: 3, Batch: 250, Avg. Loss: 0.07726229203930648\n",
      "Epoch: 3, Batch: 260, Avg. Loss: 0.07703984879003893\n",
      "Epoch: 3, Batch: 270, Avg. Loss: 0.0768224872393683\n",
      "Epoch: 3, Batch: 280, Avg. Loss: 0.07660312034017444\n",
      "Epoch: 3, Batch: 290, Avg. Loss: 0.07638840248079128\n",
      "Epoch: 3, Batch: 300, Avg. Loss: 0.0761740452798981\n",
      "Epoch: 3, Batch: 310, Avg. Loss: 0.0759571157320192\n",
      "Epoch: 3, Batch: 320, Avg. Loss: 0.07574209580389737\n",
      "Epoch: 3, Batch: 330, Avg. Loss: 0.07552837187648762\n",
      "Epoch: 3, Batch: 340, Avg. Loss: 0.07531649641616227\n",
      "Epoch: 3, Batch: 350, Avg. Loss: 0.07510812827300706\n",
      "Epoch: 3, Batch: 360, Avg. Loss: 0.07489770538692037\n",
      "Epoch: 3, Batch: 370, Avg. Loss: 0.07468942848579246\n",
      "Epoch: 3, Batch: 380, Avg. Loss: 0.0744846544759725\n",
      "Epoch: 3, Batch: 390, Avg. Loss: 0.07427974635555164\n",
      "Epoch: 3, Batch: 400, Avg. Loss: 0.07407769801802958\n",
      "Epoch: 3, Batch: 410, Avg. Loss: 0.07387525216192212\n",
      "Epoch: 3, Batch: 420, Avg. Loss: 0.07367362661566174\n",
      "Epoch: 3, Batch: 430, Avg. Loss: 0.07347156791100373\n",
      "Epoch: 3, Batch: 440, Avg. Loss: 0.07327039868666674\n",
      "Epoch: 3, Batch: 450, Avg. Loss: 0.07307013741762972\n",
      "Epoch: 3, Batch: 460, Avg. Loss: 0.07287297721403596\n",
      "Epoch: 3, Batch: 470, Avg. Loss: 0.072677482280918\n",
      "Epoch: 3, Batch: 480, Avg. Loss: 0.07248194929726046\n",
      "Epoch: 3, Batch: 490, Avg. Loss: 0.07228531437108779\n",
      "Epoch: 3, Batch: 500, Avg. Loss: 0.07209129409343058\n",
      "Epoch: 3, Batch: 510, Avg. Loss: 0.07189844219126616\n",
      "Epoch: 3, Batch: 520, Avg. Loss: 0.07170573415904806\n",
      "Epoch: 3, Batch: 530, Avg. Loss: 0.07151369811207052\n",
      "Epoch: 3, Batch: 540, Avg. Loss: 0.07132418980077564\n",
      "Epoch: 3, Batch: 550, Avg. Loss: 0.0711337320476166\n",
      "Epoch: 3, Batch: 560, Avg. Loss: 0.07094461779896184\n",
      "Epoch: 3, Batch: 570, Avg. Loss: 0.0707571195824923\n",
      "Epoch: 3, Batch: 580, Avg. Loss: 0.0705707498423469\n",
      "Epoch: 3, Batch: 590, Avg. Loss: 0.07038681170136785\n",
      "Epoch: 3, Batch: 600, Avg. Loss: 0.07020360922567805\n",
      "Epoch: 3, Batch: 610, Avg. Loss: 0.0700199701011438\n",
      "Epoch: 3, Batch: 620, Avg. Loss: 0.06983894670561706\n",
      "Epoch: 3, Batch: 630, Avg. Loss: 0.06966102199989489\n",
      "Epoch: 3, Batch: 640, Avg. Loss: 0.06948209286510647\n",
      "Epoch: 3, Batch: 650, Avg. Loss: 0.06930183008657602\n",
      "Epoch: 3, Batch: 660, Avg. Loss: 0.06912485416463403\n",
      "Epoch: 3, Batch: 670, Avg. Loss: 0.06894902349898517\n",
      "Epoch: 3, Batch: 680, Avg. Loss: 0.06877114491409284\n",
      "Epoch: 3, Batch: 690, Avg. Loss: 0.06859894912636104\n",
      "Epoch: 3, Batch: 700, Avg. Loss: 0.06843404511659965\n",
      "Epoch: 3, Batch: 710, Avg. Loss: 0.06826588700445096\n",
      "Epoch: 3, Batch: 720, Avg. Loss: 0.06809322643853778\n",
      "Epoch: 3, Batch: 730, Avg. Loss: 0.0679219560249197\n",
      "Epoch: 3, Batch: 740, Avg. Loss: 0.06775065959580495\n",
      "Epoch: 3, Batch: 750, Avg. Loss: 0.06758079313468435\n",
      "Epoch: 3, Batch: 760, Avg. Loss: 0.06741326107116698\n",
      "Epoch: 3, Batch: 770, Avg. Loss: 0.0672457762680597\n",
      "Epoch: 3, Batch: 780, Avg. Loss: 0.06708159482110657\n",
      "Epoch: 3, Batch: 790, Avg. Loss: 0.06691729614610839\n",
      "Epoch: 3, Batch: 800, Avg. Loss: 0.06675607929668868\n",
      "Epoch: 3, Batch: 810, Avg. Loss: 0.06659404254301554\n",
      "Epoch: 3, Batch: 820, Avg. Loss: 0.06643509502038412\n",
      "Epoch: 3, Batch: 830, Avg. Loss: 0.06627243076124768\n",
      "Epoch: 3, Batch: 840, Avg. Loss: 0.06611358315951144\n",
      "Epoch: 3, Batch: 850, Avg. Loss: 0.06595347888412773\n",
      "Epoch: 3, Batch: 860, Avg. Loss: 0.0657964078685322\n",
      "Epoch: 3, Batch: 870, Avg. Loss: 0.06563992043747854\n",
      "Epoch: 3, Batch: 880, Avg. Loss: 0.06548321219619661\n",
      "Epoch: 3, Batch: 890, Avg. Loss: 0.06533100088514716\n",
      "Epoch: 3, Batch: 900, Avg. Loss: 0.06517758181210014\n",
      "Epoch: 3, Batch: 910, Avg. Loss: 0.06502412997641953\n",
      "Epoch: 3, Batch: 920, Avg. Loss: 0.06487187713890111\n",
      "Epoch: 3, Batch: 930, Avg. Loss: 0.06472139404879342\n",
      "Epoch: 3, Batch: 940, Avg. Loss: 0.06457411891711366\n",
      "Epoch: 3, Batch: 950, Avg. Loss: 0.06442890781537597\n",
      "Epoch: 3, Batch: 960, Avg. Loss: 0.06428334213361296\n",
      "Epoch: 3, Batch: 970, Avg. Loss: 0.06413742240584379\n",
      "Epoch: 3, Batch: 980, Avg. Loss: 0.06398945593361376\n",
      "Epoch: 3, Batch: 990, Avg. Loss: 0.06384623063937253\n",
      "Epoch: 3, Batch: 1000, Avg. Loss: 0.06371121966271848\n",
      "Epoch: 3, Batch: 1010, Avg. Loss: 0.06360233063681604\n",
      "Epoch: 3, Batch: 1020, Avg. Loss: 0.06348696406438518\n",
      "Epoch: 3, Batch: 1030, Avg. Loss: 0.0633647058577372\n",
      "Epoch: 3, Batch: 1040, Avg. Loss: 0.06323097053273527\n",
      "Epoch: 3, Batch: 1050, Avg. Loss: 0.06310887929823869\n",
      "Epoch: 3, Batch: 1060, Avg. Loss: 0.06298601890758099\n",
      "Epoch: 3, Batch: 1070, Avg. Loss: 0.06285264691746083\n",
      "Epoch: 3, Batch: 1080, Avg. Loss: 0.06272344304610078\n",
      "Epoch: 3, Batch: 1090, Avg. Loss: 0.06261071440011652\n",
      "Epoch: 3, Batch: 1100, Avg. Loss: 0.062481674760666954\n",
      "Epoch: 3, Batch: 1110, Avg. Loss: 0.062360903426069945\n",
      "Epoch: 3, Batch: 1120, Avg. Loss: 0.06222765157718567\n",
      "Epoch: 3, Batch: 1130, Avg. Loss: 0.06211246475117871\n",
      "Epoch: 3, Batch: 1140, Avg. Loss: 0.0619886489785075\n",
      "Epoch: 3, Batch: 1150, Avg. Loss: 0.0618603821330054\n",
      "Epoch: 3, Batch: 1160, Avg. Loss: 0.06174223111164994\n",
      "Epoch: 3, Batch: 1170, Avg. Loss: 0.06161372400551962\n",
      "Epoch: 3, Batch: 1180, Avg. Loss: 0.06149100697677525\n",
      "Epoch: 3, Batch: 1190, Avg. Loss: 0.06136210061872652\n",
      "Epoch: 3, Batch: 1200, Avg. Loss: 0.06124585461064299\n",
      "Epoch: 3, Batch: 1210, Avg. Loss: 0.06111886996731584\n",
      "Epoch: 3, Batch: 1220, Avg. Loss: 0.060992062955666344\n",
      "Epoch: 3, Batch: 1230, Avg. Loss: 0.0608652192469416\n",
      "Epoch: 3, Batch: 1240, Avg. Loss: 0.06073751823458506\n",
      "Epoch: 3, Batch: 1250, Avg. Loss: 0.06061176305863212\n",
      "Epoch: 3, Batch: 1260, Avg. Loss: 0.06049355237843318\n",
      "Epoch: 3, Batch: 1270, Avg. Loss: 0.06036821279098271\n",
      "Epoch: 3, Batch: 1280, Avg. Loss: 0.060245090677393115\n",
      "Epoch: 3, Batch: 1290, Avg. Loss: 0.06012180860654855\n",
      "Epoch: 3, Batch: 1300, Avg. Loss: 0.06000070315190241\n",
      "Epoch: 3, Batch: 1310, Avg. Loss: 0.05988050505633292\n",
      "Epoch: 3, Batch: 1320, Avg. Loss: 0.05976389853553067\n",
      "Epoch: 3, Batch: 1330, Avg. Loss: 0.05964109438092416\n",
      "Epoch: 3, Batch: 1340, Avg. Loss: 0.05951555072255008\n",
      "Epoch: 3, Batch: 1350, Avg. Loss: 0.05939447705199734\n",
      "Epoch: 3, Batch: 1360, Avg. Loss: 0.05927040241049279\n",
      "Epoch: 3, Batch: 1370, Avg. Loss: 0.05914591068052869\n",
      "Epoch: 3, Batch: 1380, Avg. Loss: 0.05902583963606711\n",
      "Epoch: 3, Batch: 1390, Avg. Loss: 0.058908415106330936\n",
      "Epoch: 3, Batch: 1400, Avg. Loss: 0.05879052459954219\n",
      "Epoch: 3, Batch: 1410, Avg. Loss: 0.058676501194730446\n",
      "Epoch: 3, Batch: 1420, Avg. Loss: 0.05856298453334432\n",
      "Epoch: 3, Batch: 1430, Avg. Loss: 0.05844503836675896\n",
      "Epoch: 3, Batch: 1440, Avg. Loss: 0.05832748902681352\n",
      "Epoch: 3, Batch: 1450, Avg. Loss: 0.05820844894657258\n",
      "Epoch: 3, Batch: 1460, Avg. Loss: 0.05809217248641677\n",
      "Epoch: 3, Batch: 1470, Avg. Loss: 0.05797591515149753\n",
      "Epoch: 3, Batch: 1480, Avg. Loss: 0.057861819641261816\n",
      "Epoch: 3, Batch: 1490, Avg. Loss: 0.05774771552420587\n",
      "Epoch: 3, Batch: 1500, Avg. Loss: 0.05763575431588628\n",
      "Epoch: 3, Batch: 1510, Avg. Loss: 0.05751857999876693\n",
      "Epoch: 3, Batch: 1520, Avg. Loss: 0.05740063862329863\n",
      "Epoch: 3, Batch: 1530, Avg. Loss: 0.057286285942396584\n",
      "Epoch: 3, Batch: 1540, Avg. Loss: 0.0571692896144325\n",
      "Epoch: 4, Batch: 10, Avg. Loss: 0.056984254641416605\n",
      "Epoch: 4, Batch: 20, Avg. Loss: 0.056868042844333756\n",
      "Epoch: 4, Batch: 30, Avg. Loss: 0.05675367781480987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Batch: 40, Avg. Loss: 0.05663645818743115\n",
      "Epoch: 4, Batch: 50, Avg. Loss: 0.05651967507367387\n",
      "Epoch: 4, Batch: 60, Avg. Loss: 0.05640663077132485\n",
      "Epoch: 4, Batch: 70, Avg. Loss: 0.05629177094425162\n",
      "Epoch: 4, Batch: 80, Avg. Loss: 0.05617855871290978\n",
      "Epoch: 4, Batch: 90, Avg. Loss: 0.05606551289398872\n",
      "Epoch: 4, Batch: 100, Avg. Loss: 0.05595196015812216\n",
      "Epoch: 4, Batch: 110, Avg. Loss: 0.05583953466163884\n",
      "Epoch: 4, Batch: 120, Avg. Loss: 0.055726072184375586\n",
      "Epoch: 4, Batch: 130, Avg. Loss: 0.055613424418341134\n",
      "Epoch: 4, Batch: 140, Avg. Loss: 0.05550255260249646\n",
      "Epoch: 4, Batch: 150, Avg. Loss: 0.05539246995702128\n",
      "Epoch: 4, Batch: 160, Avg. Loss: 0.055281368797171354\n",
      "Epoch: 4, Batch: 170, Avg. Loss: 0.05517144454729104\n",
      "Epoch: 4, Batch: 180, Avg. Loss: 0.05506012227907483\n",
      "Epoch: 4, Batch: 190, Avg. Loss: 0.05495025424086499\n",
      "Epoch: 4, Batch: 200, Avg. Loss: 0.0548409821441255\n",
      "Epoch: 4, Batch: 210, Avg. Loss: 0.054732341879590835\n",
      "Epoch: 4, Batch: 220, Avg. Loss: 0.054624909668687126\n",
      "Epoch: 4, Batch: 230, Avg. Loss: 0.0545165196977316\n",
      "Epoch: 4, Batch: 240, Avg. Loss: 0.05440858442200547\n",
      "Epoch: 4, Batch: 250, Avg. Loss: 0.054302586005652465\n",
      "Epoch: 4, Batch: 260, Avg. Loss: 0.05419513497606022\n",
      "Epoch: 4, Batch: 270, Avg. Loss: 0.054088990158675306\n",
      "Epoch: 4, Batch: 280, Avg. Loss: 0.05398376663542662\n",
      "Epoch: 4, Batch: 290, Avg. Loss: 0.05387817413237752\n",
      "Epoch: 4, Batch: 300, Avg. Loss: 0.05377265735910796\n",
      "Epoch: 4, Batch: 310, Avg. Loss: 0.05366903034215853\n",
      "Epoch: 4, Batch: 320, Avg. Loss: 0.05356491310021729\n",
      "Epoch: 4, Batch: 330, Avg. Loss: 0.053460027409302315\n",
      "Epoch: 4, Batch: 340, Avg. Loss: 0.053356189888086894\n",
      "Epoch: 4, Batch: 350, Avg. Loss: 0.05325215099206583\n",
      "Epoch: 4, Batch: 360, Avg. Loss: 0.05314871158914406\n",
      "Epoch: 4, Batch: 370, Avg. Loss: 0.05304529465798572\n",
      "Epoch: 4, Batch: 380, Avg. Loss: 0.05294297408620803\n",
      "Epoch: 4, Batch: 390, Avg. Loss: 0.05284137142516136\n",
      "Epoch: 4, Batch: 400, Avg. Loss: 0.05273948956999454\n",
      "Epoch: 4, Batch: 410, Avg. Loss: 0.052638500268353886\n",
      "Epoch: 4, Batch: 420, Avg. Loss: 0.052539028713547965\n",
      "Epoch: 4, Batch: 430, Avg. Loss: 0.05243802248900607\n",
      "Epoch: 4, Batch: 440, Avg. Loss: 0.05233812266699797\n",
      "Epoch: 4, Batch: 450, Avg. Loss: 0.05223850233976066\n",
      "Epoch: 4, Batch: 460, Avg. Loss: 0.05213986839981128\n",
      "Epoch: 4, Batch: 470, Avg. Loss: 0.05204126181709448\n",
      "Epoch: 4, Batch: 480, Avg. Loss: 0.05194224341961123\n",
      "Epoch: 4, Batch: 490, Avg. Loss: 0.05184334450766052\n",
      "Epoch: 4, Batch: 500, Avg. Loss: 0.05174587750892048\n",
      "Epoch: 4, Batch: 510, Avg. Loss: 0.0516482414364422\n",
      "Epoch: 4, Batch: 520, Avg. Loss: 0.05155171039888863\n",
      "Epoch: 4, Batch: 530, Avg. Loss: 0.05145474017359429\n",
      "Epoch: 4, Batch: 540, Avg. Loss: 0.051358173484706\n",
      "Epoch: 4, Batch: 550, Avg. Loss: 0.051261997332826124\n",
      "Epoch: 4, Batch: 560, Avg. Loss: 0.05116564889768477\n",
      "Epoch: 4, Batch: 570, Avg. Loss: 0.05107010505456739\n",
      "Epoch: 4, Batch: 580, Avg. Loss: 0.05097535378461769\n",
      "Epoch: 4, Batch: 590, Avg. Loss: 0.05088214797690149\n",
      "Epoch: 4, Batch: 600, Avg. Loss: 0.05078967112920531\n",
      "Epoch: 4, Batch: 610, Avg. Loss: 0.05069578085224595\n",
      "Epoch: 4, Batch: 620, Avg. Loss: 0.05060235918397774\n",
      "Epoch: 4, Batch: 630, Avg. Loss: 0.05051069019609727\n",
      "Epoch: 4, Batch: 640, Avg. Loss: 0.05041848255604932\n",
      "Epoch: 4, Batch: 650, Avg. Loss: 0.05032645834146297\n",
      "Epoch: 4, Batch: 660, Avg. Loss: 0.05023692745927749\n",
      "Epoch: 4, Batch: 670, Avg. Loss: 0.05014668995053216\n",
      "Epoch: 4, Batch: 680, Avg. Loss: 0.050054907742710604\n",
      "Epoch: 4, Batch: 690, Avg. Loss: 0.049969114182522485\n",
      "Epoch: 4, Batch: 700, Avg. Loss: 0.04988235427023036\n",
      "Epoch: 4, Batch: 710, Avg. Loss: 0.04979566137646538\n",
      "Epoch: 4, Batch: 720, Avg. Loss: 0.049705920961921876\n",
      "Epoch: 4, Batch: 730, Avg. Loss: 0.04961825968175978\n",
      "Epoch: 4, Batch: 740, Avg. Loss: 0.04952922638197544\n",
      "Epoch: 4, Batch: 750, Avg. Loss: 0.04944055527002399\n",
      "Epoch: 4, Batch: 760, Avg. Loss: 0.0493545409381435\n",
      "Epoch: 4, Batch: 770, Avg. Loss: 0.04926814390123137\n",
      "Epoch: 4, Batch: 780, Avg. Loss: 0.04918461571698692\n",
      "Epoch: 4, Batch: 790, Avg. Loss: 0.049097802487657785\n",
      "Epoch: 4, Batch: 800, Avg. Loss: 0.04901258357223627\n",
      "Epoch: 4, Batch: 810, Avg. Loss: 0.048926982608854114\n",
      "Epoch: 4, Batch: 820, Avg. Loss: 0.04884181418713416\n",
      "Epoch: 4, Batch: 830, Avg. Loss: 0.04875625626885519\n",
      "Epoch: 4, Batch: 840, Avg. Loss: 0.04867383362040106\n",
      "Epoch: 4, Batch: 850, Avg. Loss: 0.04859037866207603\n",
      "Epoch: 4, Batch: 860, Avg. Loss: 0.04850813870400659\n",
      "Epoch: 4, Batch: 870, Avg. Loss: 0.04842705437528375\n",
      "Epoch: 4, Batch: 880, Avg. Loss: 0.04834270796910582\n",
      "Epoch: 4, Batch: 890, Avg. Loss: 0.0482602816753358\n",
      "Epoch: 4, Batch: 900, Avg. Loss: 0.048177579435296505\n",
      "Epoch: 4, Batch: 910, Avg. Loss: 0.048094779986399425\n",
      "Epoch: 4, Batch: 920, Avg. Loss: 0.048013547093740554\n",
      "Epoch: 4, Batch: 930, Avg. Loss: 0.047931382026370616\n",
      "Epoch: 4, Batch: 940, Avg. Loss: 0.04785169373111805\n",
      "Epoch: 4, Batch: 950, Avg. Loss: 0.04777529311745568\n",
      "Epoch: 4, Batch: 960, Avg. Loss: 0.047696469929888366\n",
      "Epoch: 4, Batch: 970, Avg. Loss: 0.04761461292559647\n",
      "Epoch: 4, Batch: 980, Avg. Loss: 0.04753592407139118\n",
      "Epoch: 4, Batch: 990, Avg. Loss: 0.047456995659046534\n",
      "Epoch: 4, Batch: 1000, Avg. Loss: 0.04738023071162401\n",
      "Epoch: 4, Batch: 1010, Avg. Loss: 0.047304719696568506\n",
      "Epoch: 4, Batch: 1020, Avg. Loss: 0.04722994529456625\n",
      "Epoch: 4, Batch: 1030, Avg. Loss: 0.04716223090436692\n",
      "Epoch: 4, Batch: 1040, Avg. Loss: 0.04708549817996971\n",
      "Epoch: 4, Batch: 1050, Avg. Loss: 0.04702126417251401\n",
      "Epoch: 4, Batch: 1060, Avg. Loss: 0.04696543479442259\n",
      "Epoch: 4, Batch: 1070, Avg. Loss: 0.04689606497971893\n",
      "Epoch: 4, Batch: 1080, Avg. Loss: 0.04683433399392034\n",
      "Epoch: 4, Batch: 1090, Avg. Loss: 0.04677208423755493\n",
      "Epoch: 4, Batch: 1100, Avg. Loss: 0.04670331288276087\n",
      "Epoch: 4, Batch: 1110, Avg. Loss: 0.04663073222102814\n",
      "Epoch: 4, Batch: 1120, Avg. Loss: 0.046558941101591624\n",
      "Epoch: 4, Batch: 1130, Avg. Loss: 0.046491367851062775\n",
      "Epoch: 4, Batch: 1140, Avg. Loss: 0.0464211368519166\n",
      "Epoch: 4, Batch: 1150, Avg. Loss: 0.04635216525745523\n",
      "Epoch: 4, Batch: 1160, Avg. Loss: 0.046282979902567634\n",
      "Epoch: 4, Batch: 1170, Avg. Loss: 0.04621273290480766\n",
      "Epoch: 4, Batch: 1180, Avg. Loss: 0.046141222551671086\n",
      "Epoch: 4, Batch: 1190, Avg. Loss: 0.04606848792534581\n",
      "Epoch: 4, Batch: 1200, Avg. Loss: 0.04600241926571069\n",
      "Epoch: 4, Batch: 1210, Avg. Loss: 0.045931783365810455\n",
      "Epoch: 4, Batch: 1220, Avg. Loss: 0.045860320858991124\n",
      "Epoch: 4, Batch: 1230, Avg. Loss: 0.045800149305102934\n",
      "Epoch: 4, Batch: 1240, Avg. Loss: 0.04572733107072621\n",
      "Epoch: 4, Batch: 1250, Avg. Loss: 0.04566599726004196\n",
      "Epoch: 4, Batch: 1260, Avg. Loss: 0.04560065141660129\n",
      "Epoch: 4, Batch: 1270, Avg. Loss: 0.0455346293460082\n",
      "Epoch: 4, Batch: 1280, Avg. Loss: 0.04546736930955805\n",
      "Epoch: 4, Batch: 1290, Avg. Loss: 0.04540053569302887\n",
      "Epoch: 4, Batch: 1300, Avg. Loss: 0.04533153660764442\n",
      "Epoch: 4, Batch: 1310, Avg. Loss: 0.04526218580300443\n",
      "Epoch: 4, Batch: 1320, Avg. Loss: 0.0451948964428635\n",
      "Epoch: 4, Batch: 1330, Avg. Loss: 0.04512690933952235\n",
      "Epoch: 4, Batch: 1340, Avg. Loss: 0.0450587792783299\n",
      "Epoch: 4, Batch: 1350, Avg. Loss: 0.04499209809184842\n",
      "Epoch: 4, Batch: 1360, Avg. Loss: 0.044923305983639\n",
      "Epoch: 4, Batch: 1370, Avg. Loss: 0.04485376173334541\n",
      "Epoch: 4, Batch: 1380, Avg. Loss: 0.04478515340783784\n",
      "Epoch: 4, Batch: 1390, Avg. Loss: 0.04471723516720083\n",
      "Epoch: 4, Batch: 1400, Avg. Loss: 0.04464884178586232\n",
      "Epoch: 4, Batch: 1410, Avg. Loss: 0.04458598350226705\n",
      "Epoch: 4, Batch: 1420, Avg. Loss: 0.04451962220419871\n",
      "Epoch: 4, Batch: 1430, Avg. Loss: 0.04445379661609596\n",
      "Epoch: 4, Batch: 1440, Avg. Loss: 0.044389583169207096\n",
      "Epoch: 4, Batch: 1450, Avg. Loss: 0.04432239441574951\n",
      "Epoch: 4, Batch: 1460, Avg. Loss: 0.04425608662720993\n",
      "Epoch: 4, Batch: 1470, Avg. Loss: 0.04419273910920476\n",
      "Epoch: 4, Batch: 1480, Avg. Loss: 0.044128733191390865\n",
      "Epoch: 4, Batch: 1490, Avg. Loss: 0.04406360446919143\n",
      "Epoch: 4, Batch: 1500, Avg. Loss: 0.044001756939988146\n",
      "Epoch: 4, Batch: 1510, Avg. Loss: 0.04393642703211761\n",
      "Epoch: 4, Batch: 1520, Avg. Loss: 0.04387062110526169\n",
      "Epoch: 4, Batch: 1530, Avg. Loss: 0.04380457372687716\n",
      "Epoch: 4, Batch: 1540, Avg. Loss: 0.04373674003704532\n",
      "Epoch: 5, Batch: 10, Avg. Loss: 0.04363161841996243\n",
      "Epoch: 5, Batch: 20, Avg. Loss: 0.04356484975108861\n",
      "Epoch: 5, Batch: 30, Avg. Loss: 0.0434996244353994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Batch: 40, Avg. Loss: 0.04343354571129164\n",
      "Epoch: 5, Batch: 50, Avg. Loss: 0.04336659078477747\n",
      "Epoch: 5, Batch: 60, Avg. Loss: 0.04330035432594441\n",
      "Epoch: 5, Batch: 70, Avg. Loss: 0.0432337799715742\n",
      "Epoch: 5, Batch: 80, Avg. Loss: 0.0431684069063986\n",
      "Epoch: 5, Batch: 90, Avg. Loss: 0.043102900743223274\n",
      "Epoch: 5, Batch: 100, Avg. Loss: 0.04303657288250772\n",
      "Epoch: 5, Batch: 110, Avg. Loss: 0.0429714014350741\n",
      "Epoch: 5, Batch: 120, Avg. Loss: 0.04290521918405015\n",
      "Epoch: 5, Batch: 130, Avg. Loss: 0.04283974032373436\n",
      "Epoch: 5, Batch: 140, Avg. Loss: 0.042774478024961995\n",
      "Epoch: 5, Batch: 150, Avg. Loss: 0.04270995284355076\n",
      "Epoch: 5, Batch: 160, Avg. Loss: 0.04264492538273222\n",
      "Epoch: 5, Batch: 170, Avg. Loss: 0.04258014553307397\n",
      "Epoch: 5, Batch: 180, Avg. Loss: 0.04251531266175603\n",
      "Epoch: 5, Batch: 190, Avg. Loss: 0.042451013439903325\n",
      "Epoch: 5, Batch: 200, Avg. Loss: 0.04238791467195182\n",
      "Epoch: 5, Batch: 210, Avg. Loss: 0.042323437329324816\n",
      "Epoch: 5, Batch: 220, Avg. Loss: 0.042260333545910064\n",
      "Epoch: 5, Batch: 230, Avg. Loss: 0.04219672565058579\n",
      "Epoch: 5, Batch: 240, Avg. Loss: 0.04213352795692219\n",
      "Epoch: 5, Batch: 250, Avg. Loss: 0.04207037401076479\n",
      "Epoch: 5, Batch: 260, Avg. Loss: 0.04200782904365971\n",
      "Epoch: 5, Batch: 270, Avg. Loss: 0.04194484937826165\n",
      "Epoch: 5, Batch: 280, Avg. Loss: 0.041882284288397284\n",
      "Epoch: 5, Batch: 290, Avg. Loss: 0.04181969977313733\n",
      "Epoch: 5, Batch: 300, Avg. Loss: 0.041756738852509134\n",
      "Epoch: 5, Batch: 310, Avg. Loss: 0.04169421639574493\n",
      "Epoch: 5, Batch: 320, Avg. Loss: 0.041632371950332404\n",
      "Epoch: 5, Batch: 330, Avg. Loss: 0.04156983714722702\n",
      "Epoch: 5, Batch: 340, Avg. Loss: 0.041508092785000304\n",
      "Epoch: 5, Batch: 350, Avg. Loss: 0.04144661300805574\n",
      "Epoch: 5, Batch: 360, Avg. Loss: 0.04138613627047482\n",
      "Epoch: 5, Batch: 370, Avg. Loss: 0.041325527344839824\n",
      "Epoch: 5, Batch: 380, Avg. Loss: 0.04126672249402186\n",
      "Epoch: 5, Batch: 390, Avg. Loss: 0.04120615241967467\n",
      "Epoch: 5, Batch: 400, Avg. Loss: 0.041145630946549855\n",
      "Epoch: 5, Batch: 410, Avg. Loss: 0.04108585724288482\n",
      "Epoch: 5, Batch: 420, Avg. Loss: 0.04102573827014479\n",
      "Epoch: 5, Batch: 430, Avg. Loss: 0.040965316514388644\n",
      "Epoch: 5, Batch: 440, Avg. Loss: 0.04090549408282452\n",
      "Epoch: 5, Batch: 450, Avg. Loss: 0.04084560754399435\n",
      "Epoch: 5, Batch: 460, Avg. Loss: 0.04078602667437623\n",
      "Epoch: 5, Batch: 470, Avg. Loss: 0.04072741920998351\n",
      "Epoch: 5, Batch: 480, Avg. Loss: 0.04066868857459357\n",
      "Epoch: 5, Batch: 490, Avg. Loss: 0.040609407063162614\n",
      "Epoch: 5, Batch: 500, Avg. Loss: 0.04055038240135964\n",
      "Epoch: 5, Batch: 510, Avg. Loss: 0.04049146915690217\n",
      "Epoch: 5, Batch: 520, Avg. Loss: 0.040433355012645666\n",
      "Epoch: 5, Batch: 530, Avg. Loss: 0.04037443667299277\n",
      "Epoch: 5, Batch: 540, Avg. Loss: 0.04031706798001205\n",
      "Epoch: 5, Batch: 550, Avg. Loss: 0.04025918550188658\n",
      "Epoch: 5, Batch: 560, Avg. Loss: 0.04020130980942553\n",
      "Epoch: 5, Batch: 570, Avg. Loss: 0.04014332859394414\n",
      "Epoch: 5, Batch: 580, Avg. Loss: 0.040086494641866595\n",
      "Epoch: 5, Batch: 590, Avg. Loss: 0.04002998202743482\n",
      "Epoch: 5, Batch: 600, Avg. Loss: 0.039973666314369126\n",
      "Epoch: 5, Batch: 610, Avg. Loss: 0.03991703583875993\n",
      "Epoch: 5, Batch: 620, Avg. Loss: 0.039861674790380525\n",
      "Epoch: 5, Batch: 630, Avg. Loss: 0.03980572244754188\n",
      "Epoch: 5, Batch: 640, Avg. Loss: 0.03974973647153625\n",
      "Epoch: 5, Batch: 650, Avg. Loss: 0.039693326180784506\n",
      "Epoch: 5, Batch: 660, Avg. Loss: 0.03963809867185727\n",
      "Epoch: 5, Batch: 670, Avg. Loss: 0.03958277464369316\n",
      "Epoch: 5, Batch: 680, Avg. Loss: 0.03952698115414856\n",
      "Epoch: 5, Batch: 690, Avg. Loss: 0.03947581336660938\n",
      "Epoch: 5, Batch: 700, Avg. Loss: 0.03942325415028872\n",
      "Epoch: 5, Batch: 710, Avg. Loss: 0.039374627784486256\n",
      "Epoch: 5, Batch: 720, Avg. Loss: 0.03932215265906797\n",
      "Epoch: 5, Batch: 730, Avg. Loss: 0.03927034147573266\n",
      "Epoch: 5, Batch: 740, Avg. Loss: 0.039217877827752094\n",
      "Epoch: 5, Batch: 750, Avg. Loss: 0.03916421181677261\n",
      "Epoch: 5, Batch: 760, Avg. Loss: 0.03911147898639322\n",
      "Epoch: 5, Batch: 770, Avg. Loss: 0.039059446506525965\n",
      "Epoch: 5, Batch: 780, Avg. Loss: 0.03900846855354252\n",
      "Epoch: 5, Batch: 790, Avg. Loss: 0.03895714167274709\n",
      "Epoch: 5, Batch: 800, Avg. Loss: 0.03890516788087698\n",
      "Epoch: 5, Batch: 810, Avg. Loss: 0.03885316674276733\n",
      "Epoch: 5, Batch: 820, Avg. Loss: 0.03880037463166695\n",
      "Epoch: 5, Batch: 830, Avg. Loss: 0.038747612368557484\n",
      "Epoch: 5, Batch: 840, Avg. Loss: 0.038695664799012146\n",
      "Epoch: 5, Batch: 850, Avg. Loss: 0.038642891510486416\n",
      "Epoch: 5, Batch: 860, Avg. Loss: 0.038592053506856336\n",
      "Epoch: 5, Batch: 870, Avg. Loss: 0.038539667901673534\n",
      "Epoch: 5, Batch: 880, Avg. Loss: 0.03848731226119694\n",
      "Epoch: 5, Batch: 890, Avg. Loss: 0.03843701585751717\n",
      "Epoch: 5, Batch: 900, Avg. Loss: 0.03838715527921324\n",
      "Epoch: 5, Batch: 910, Avg. Loss: 0.03833634314483521\n",
      "Epoch: 5, Batch: 920, Avg. Loss: 0.03828575477511571\n",
      "Epoch: 5, Batch: 930, Avg. Loss: 0.038234589586932155\n",
      "Epoch: 5, Batch: 940, Avg. Loss: 0.0381847101265467\n",
      "Epoch: 5, Batch: 950, Avg. Loss: 0.03813382797897947\n",
      "Epoch: 5, Batch: 960, Avg. Loss: 0.038082645716703736\n",
      "Epoch: 5, Batch: 970, Avg. Loss: 0.03803222072031304\n",
      "Epoch: 5, Batch: 980, Avg. Loss: 0.03798210344341324\n",
      "Epoch: 5, Batch: 990, Avg. Loss: 0.03793478022076749\n",
      "Epoch: 5, Batch: 1000, Avg. Loss: 0.03788725325336317\n",
      "Epoch: 5, Batch: 1010, Avg. Loss: 0.03783828170892753\n",
      "Epoch: 5, Batch: 1020, Avg. Loss: 0.03778926162186067\n",
      "Epoch: 5, Batch: 1030, Avg. Loss: 0.037746031479276375\n",
      "Epoch: 5, Batch: 1040, Avg. Loss: 0.03769815713656351\n",
      "Epoch: 5, Batch: 1050, Avg. Loss: 0.037654762080781785\n",
      "Epoch: 5, Batch: 1060, Avg. Loss: 0.0376119659395725\n",
      "Epoch: 5, Batch: 1070, Avg. Loss: 0.03756754834799396\n",
      "Epoch: 5, Batch: 1080, Avg. Loss: 0.03752534841607727\n",
      "Epoch: 5, Batch: 1090, Avg. Loss: 0.03751165251331683\n",
      "Epoch: 5, Batch: 1100, Avg. Loss: 0.037492735421738164\n",
      "Epoch: 5, Batch: 1110, Avg. Loss: 0.03765187661774909\n",
      "Epoch: 5, Batch: 1120, Avg. Loss: 0.03819335610371607\n",
      "Epoch: 5, Batch: 1130, Avg. Loss: 0.039689197229864526\n",
      "Epoch: 5, Batch: 1140, Avg. Loss: 0.03998637305394167\n",
      "Epoch: 5, Batch: 1150, Avg. Loss: 0.04032938172221411\n",
      "Epoch: 5, Batch: 1160, Avg. Loss: 0.04079386922004955\n",
      "Epoch: 5, Batch: 1170, Avg. Loss: 0.04090717546573273\n",
      "Epoch: 5, Batch: 1180, Avg. Loss: 0.04103497570577983\n",
      "Epoch: 5, Batch: 1190, Avg. Loss: 0.04111397940231812\n",
      "Epoch: 5, Batch: 1200, Avg. Loss: 0.041138437364062655\n",
      "Epoch: 5, Batch: 1210, Avg. Loss: 0.04112671430257931\n",
      "Epoch: 5, Batch: 1220, Avg. Loss: 0.041108094819928165\n",
      "Epoch: 5, Batch: 1230, Avg. Loss: 0.04108788665051219\n",
      "Epoch: 5, Batch: 1240, Avg. Loss: 0.041053698163577866\n",
      "Epoch: 5, Batch: 1250, Avg. Loss: 0.04102912794507269\n",
      "Epoch: 5, Batch: 1260, Avg. Loss: 0.04100258842370242\n",
      "Epoch: 5, Batch: 1270, Avg. Loss: 0.04098596000429209\n",
      "Epoch: 5, Batch: 1280, Avg. Loss: 0.040965044939382037\n",
      "Epoch: 5, Batch: 1290, Avg. Loss: 0.04093407864239677\n",
      "Epoch: 5, Batch: 1300, Avg. Loss: 0.040904730139491585\n",
      "Epoch: 5, Batch: 1310, Avg. Loss: 0.04088208377567468\n",
      "Epoch: 5, Batch: 1320, Avg. Loss: 0.040844227435363895\n",
      "Epoch: 5, Batch: 1330, Avg. Loss: 0.040804687340101536\n",
      "Epoch: 5, Batch: 1340, Avg. Loss: 0.04076575277706424\n",
      "Epoch: 5, Batch: 1350, Avg. Loss: 0.0407348897978245\n",
      "Epoch: 5, Batch: 1360, Avg. Loss: 0.040701437188433345\n",
      "Epoch: 5, Batch: 1370, Avg. Loss: 0.04066137659578339\n",
      "Epoch: 5, Batch: 1380, Avg. Loss: 0.040618940144838564\n",
      "Epoch: 5, Batch: 1390, Avg. Loss: 0.04058166030236148\n",
      "Epoch: 5, Batch: 1400, Avg. Loss: 0.04054356699017845\n",
      "Epoch: 5, Batch: 1410, Avg. Loss: 0.04051100899287746\n",
      "Epoch: 5, Batch: 1420, Avg. Loss: 0.04047069452851309\n",
      "Epoch: 5, Batch: 1430, Avg. Loss: 0.04044366302830378\n",
      "Epoch: 5, Batch: 1440, Avg. Loss: 0.04040568910252991\n",
      "Epoch: 5, Batch: 1450, Avg. Loss: 0.04036572207883263\n",
      "Epoch: 5, Batch: 1460, Avg. Loss: 0.04032414801024191\n",
      "Epoch: 5, Batch: 1470, Avg. Loss: 0.040285765953494576\n",
      "Epoch: 5, Batch: 1480, Avg. Loss: 0.04024936514687063\n",
      "Epoch: 5, Batch: 1490, Avg. Loss: 0.04020798881913721\n",
      "Epoch: 5, Batch: 1500, Avg. Loss: 0.04017117755193697\n",
      "Epoch: 5, Batch: 1510, Avg. Loss: 0.040129299601829004\n",
      "Epoch: 5, Batch: 1520, Avg. Loss: 0.04009007788494641\n",
      "Epoch: 5, Batch: 1530, Avg. Loss: 0.04004692607502965\n",
      "Epoch: 5, Batch: 1540, Avg. Loss: 0.04000163523335557\n",
      "Epoch: 6, Batch: 10, Avg. Loss: 0.039934148132988005\n",
      "Epoch: 6, Batch: 20, Avg. Loss: 0.03989056813672774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Batch: 30, Avg. Loss: 0.03984731180753583\n",
      "Epoch: 6, Batch: 40, Avg. Loss: 0.03980167951491163\n",
      "Epoch: 6, Batch: 50, Avg. Loss: 0.03975612283464024\n",
      "Epoch: 6, Batch: 60, Avg. Loss: 0.03971327051364549\n",
      "Epoch: 6, Batch: 70, Avg. Loss: 0.03966721696852572\n",
      "Epoch: 6, Batch: 80, Avg. Loss: 0.03962304815727507\n",
      "Epoch: 6, Batch: 90, Avg. Loss: 0.03957851717538235\n",
      "Epoch: 6, Batch: 100, Avg. Loss: 0.039531561199354165\n",
      "Epoch: 6, Batch: 110, Avg. Loss: 0.03948515447151614\n",
      "Epoch: 6, Batch: 120, Avg. Loss: 0.03943907111775306\n",
      "Epoch: 6, Batch: 130, Avg. Loss: 0.03939373053311458\n",
      "Epoch: 6, Batch: 140, Avg. Loss: 0.03934814434354217\n",
      "Epoch: 6, Batch: 150, Avg. Loss: 0.03930256624384813\n",
      "Epoch: 6, Batch: 160, Avg. Loss: 0.03925671090391917\n",
      "Epoch: 6, Batch: 170, Avg. Loss: 0.0392122010338204\n",
      "Epoch: 6, Batch: 180, Avg. Loss: 0.039166232166459265\n",
      "Epoch: 6, Batch: 190, Avg. Loss: 0.03912152249992533\n",
      "Epoch: 6, Batch: 200, Avg. Loss: 0.03907595632228047\n",
      "Epoch: 6, Batch: 210, Avg. Loss: 0.03903058385397872\n",
      "Epoch: 6, Batch: 220, Avg. Loss: 0.03898615101179813\n",
      "Epoch: 6, Batch: 230, Avg. Loss: 0.038941930342811536\n",
      "Epoch: 6, Batch: 240, Avg. Loss: 0.03889648022870433\n",
      "Epoch: 6, Batch: 250, Avg. Loss: 0.03885356669667668\n",
      "Epoch: 6, Batch: 260, Avg. Loss: 0.03880919110313827\n",
      "Epoch: 6, Batch: 270, Avg. Loss: 0.038764283598023946\n",
      "Epoch: 6, Batch: 280, Avg. Loss: 0.03872133954610558\n",
      "Epoch: 6, Batch: 290, Avg. Loss: 0.038677716394850084\n",
      "Epoch: 6, Batch: 300, Avg. Loss: 0.038632710230431376\n",
      "Epoch: 6, Batch: 310, Avg. Loss: 0.03858741516020064\n",
      "Epoch: 6, Batch: 320, Avg. Loss: 0.03854332563045949\n",
      "Epoch: 6, Batch: 330, Avg. Loss: 0.03849818565806211\n",
      "Epoch: 6, Batch: 340, Avg. Loss: 0.038453132626228625\n",
      "Epoch: 6, Batch: 350, Avg. Loss: 0.038408627756621995\n",
      "Epoch: 6, Batch: 360, Avg. Loss: 0.038364233073784726\n",
      "Epoch: 6, Batch: 370, Avg. Loss: 0.03831899016477051\n",
      "Epoch: 6, Batch: 380, Avg. Loss: 0.03827453800342856\n",
      "Epoch: 6, Batch: 390, Avg. Loss: 0.03823170760002056\n",
      "Epoch: 6, Batch: 400, Avg. Loss: 0.03818814166077593\n",
      "Epoch: 6, Batch: 410, Avg. Loss: 0.038145056772767826\n",
      "Epoch: 6, Batch: 420, Avg. Loss: 0.03810184092772845\n",
      "Epoch: 6, Batch: 430, Avg. Loss: 0.03805833345370615\n",
      "Epoch: 6, Batch: 440, Avg. Loss: 0.038014304496396416\n",
      "Epoch: 6, Batch: 450, Avg. Loss: 0.03797050912040589\n",
      "Epoch: 6, Batch: 460, Avg. Loss: 0.037927013009032645\n",
      "Epoch: 6, Batch: 470, Avg. Loss: 0.03788461650176981\n",
      "Epoch: 6, Batch: 480, Avg. Loss: 0.037840566672132876\n",
      "Epoch: 6, Batch: 490, Avg. Loss: 0.03779655102489854\n",
      "Epoch: 6, Batch: 500, Avg. Loss: 0.03775399490467265\n",
      "Epoch: 6, Batch: 510, Avg. Loss: 0.03771125112270473\n",
      "Epoch: 6, Batch: 520, Avg. Loss: 0.03766850146059632\n",
      "Epoch: 6, Batch: 530, Avg. Loss: 0.037625181029452004\n",
      "Epoch: 6, Batch: 540, Avg. Loss: 0.03758287570936223\n",
      "Epoch: 6, Batch: 550, Avg. Loss: 0.03753985920822116\n",
      "Epoch: 6, Batch: 560, Avg. Loss: 0.03749698143905511\n",
      "Epoch: 6, Batch: 570, Avg. Loss: 0.03745496235685186\n",
      "Epoch: 6, Batch: 580, Avg. Loss: 0.037414328530011594\n",
      "Epoch: 6, Batch: 590, Avg. Loss: 0.03737359646325348\n",
      "Epoch: 6, Batch: 600, Avg. Loss: 0.03733219119104833\n",
      "Epoch: 6, Batch: 610, Avg. Loss: 0.03729022328515077\n",
      "Epoch: 6, Batch: 620, Avg. Loss: 0.03724879666398841\n",
      "Epoch: 6, Batch: 630, Avg. Loss: 0.03720950893394828\n",
      "Epoch: 6, Batch: 640, Avg. Loss: 0.03716949864954816\n",
      "Epoch: 6, Batch: 650, Avg. Loss: 0.03712789604444166\n",
      "Epoch: 6, Batch: 660, Avg. Loss: 0.03708953381943761\n",
      "Epoch: 6, Batch: 670, Avg. Loss: 0.03704940165781678\n",
      "Epoch: 6, Batch: 680, Avg. Loss: 0.03700765253396638\n",
      "Epoch: 6, Batch: 690, Avg. Loss: 0.03696685524314221\n",
      "Epoch: 6, Batch: 700, Avg. Loss: 0.03692676370854566\n",
      "Epoch: 6, Batch: 710, Avg. Loss: 0.0368868618750432\n",
      "Epoch: 6, Batch: 720, Avg. Loss: 0.03684573374552052\n",
      "Epoch: 6, Batch: 730, Avg. Loss: 0.03680532312442942\n",
      "Epoch: 6, Batch: 740, Avg. Loss: 0.03676488948610667\n",
      "Epoch: 6, Batch: 750, Avg. Loss: 0.036723715297679586\n",
      "Epoch: 6, Batch: 760, Avg. Loss: 0.03668258838613262\n",
      "Epoch: 6, Batch: 770, Avg. Loss: 0.03664270149484119\n",
      "Epoch: 6, Batch: 780, Avg. Loss: 0.03660541202983661\n",
      "Epoch: 6, Batch: 790, Avg. Loss: 0.03656584228873908\n",
      "Epoch: 6, Batch: 800, Avg. Loss: 0.036526257834967775\n",
      "Epoch: 6, Batch: 810, Avg. Loss: 0.03648652616541766\n",
      "Epoch: 6, Batch: 820, Avg. Loss: 0.036446932593660206\n",
      "Epoch: 6, Batch: 830, Avg. Loss: 0.036408193027373095\n",
      "Epoch: 6, Batch: 840, Avg. Loss: 0.03637226904320219\n",
      "Epoch: 6, Batch: 850, Avg. Loss: 0.03633339151074055\n",
      "Epoch: 6, Batch: 860, Avg. Loss: 0.03629638911616816\n",
      "Epoch: 6, Batch: 870, Avg. Loss: 0.0362569348424813\n",
      "Epoch: 6, Batch: 880, Avg. Loss: 0.0362190473480152\n",
      "Epoch: 6, Batch: 890, Avg. Loss: 0.03618093958654837\n",
      "Epoch: 6, Batch: 900, Avg. Loss: 0.03614216093306033\n",
      "Epoch: 6, Batch: 910, Avg. Loss: 0.036102777104748913\n",
      "Epoch: 6, Batch: 920, Avg. Loss: 0.036064184827641683\n",
      "Epoch: 6, Batch: 930, Avg. Loss: 0.036025479996140045\n",
      "Epoch: 6, Batch: 940, Avg. Loss: 0.03598709773417751\n",
      "Epoch: 6, Batch: 950, Avg. Loss: 0.035948299658702186\n",
      "Epoch: 6, Batch: 960, Avg. Loss: 0.0359088555633232\n",
      "Epoch: 6, Batch: 970, Avg. Loss: 0.035870466512645\n",
      "Epoch: 6, Batch: 980, Avg. Loss: 0.03583233253620708\n",
      "Epoch: 6, Batch: 990, Avg. Loss: 0.03579594628179646\n",
      "Epoch: 6, Batch: 1000, Avg. Loss: 0.03576260377493744\n",
      "Epoch: 6, Batch: 1010, Avg. Loss: 0.03572540066357242\n",
      "Epoch: 6, Batch: 1020, Avg. Loss: 0.03568994879962784\n",
      "Epoch: 6, Batch: 1030, Avg. Loss: 0.03566358869327271\n",
      "Epoch: 6, Batch: 1040, Avg. Loss: 0.03563058223843564\n",
      "Epoch: 6, Batch: 1050, Avg. Loss: 0.03559859573073183\n",
      "Epoch: 6, Batch: 1060, Avg. Loss: 0.03556645970077523\n",
      "Epoch: 6, Batch: 1070, Avg. Loss: 0.0355321866987374\n",
      "Epoch: 6, Batch: 1080, Avg. Loss: 0.0354966220959135\n",
      "Epoch: 6, Batch: 1090, Avg. Loss: 0.0354659128085319\n",
      "Epoch: 6, Batch: 1100, Avg. Loss: 0.035435099068974955\n",
      "Epoch: 6, Batch: 1110, Avg. Loss: 0.03540750457294312\n",
      "Epoch: 6, Batch: 1120, Avg. Loss: 0.035375872704437435\n",
      "Epoch: 6, Batch: 1130, Avg. Loss: 0.0353534481536126\n",
      "Epoch: 6, Batch: 1140, Avg. Loss: 0.03532494456601025\n",
      "Epoch: 6, Batch: 1150, Avg. Loss: 0.035293641294035706\n",
      "Epoch: 6, Batch: 1160, Avg. Loss: 0.03527477272504663\n",
      "Epoch: 6, Batch: 1170, Avg. Loss: 0.035245144822787317\n",
      "Epoch: 6, Batch: 1180, Avg. Loss: 0.0352273596903457\n",
      "Epoch: 6, Batch: 1190, Avg. Loss: 0.03520247949190276\n",
      "Epoch: 6, Batch: 1200, Avg. Loss: 0.03517231288719699\n",
      "Epoch: 6, Batch: 1210, Avg. Loss: 0.03514039369058699\n",
      "Epoch: 6, Batch: 1220, Avg. Loss: 0.03510677879178443\n",
      "Epoch: 6, Batch: 1230, Avg. Loss: 0.035074253223901994\n",
      "Epoch: 6, Batch: 1240, Avg. Loss: 0.03503971881217671\n",
      "Epoch: 6, Batch: 1250, Avg. Loss: 0.03501000451435584\n",
      "Epoch: 6, Batch: 1260, Avg. Loss: 0.03497954393176284\n",
      "Epoch: 6, Batch: 1270, Avg. Loss: 0.03495057945006706\n",
      "Epoch: 6, Batch: 1280, Avg. Loss: 0.0349196350573417\n",
      "Epoch: 6, Batch: 1290, Avg. Loss: 0.03489023694202451\n",
      "Epoch: 6, Batch: 1300, Avg. Loss: 0.03485947452474512\n",
      "Epoch: 6, Batch: 1310, Avg. Loss: 0.034827125561273556\n",
      "Epoch: 6, Batch: 1320, Avg. Loss: 0.03479840984174627\n",
      "Epoch: 6, Batch: 1330, Avg. Loss: 0.03476560602224063\n",
      "Epoch: 6, Batch: 1340, Avg. Loss: 0.034732222245904956\n",
      "Epoch: 6, Batch: 1350, Avg. Loss: 0.03470128239239852\n",
      "Epoch: 6, Batch: 1360, Avg. Loss: 0.03466892180490018\n",
      "Epoch: 6, Batch: 1370, Avg. Loss: 0.03463552625065386\n",
      "Epoch: 6, Batch: 1380, Avg. Loss: 0.034601706756568706\n",
      "Epoch: 6, Batch: 1390, Avg. Loss: 0.03456913545750237\n",
      "Epoch: 6, Batch: 1400, Avg. Loss: 0.03453653272132029\n",
      "Epoch: 6, Batch: 1410, Avg. Loss: 0.03450595019507443\n",
      "Epoch: 6, Batch: 1420, Avg. Loss: 0.0344739983889121\n",
      "Epoch: 6, Batch: 1430, Avg. Loss: 0.03444429068666025\n",
      "Epoch: 6, Batch: 1440, Avg. Loss: 0.034412533983638166\n",
      "Epoch: 6, Batch: 1450, Avg. Loss: 0.03437877526101224\n",
      "Epoch: 6, Batch: 1460, Avg. Loss: 0.03434591541932602\n",
      "Epoch: 6, Batch: 1470, Avg. Loss: 0.034314210072421046\n",
      "Epoch: 6, Batch: 1480, Avg. Loss: 0.03428298038390906\n",
      "Epoch: 6, Batch: 1490, Avg. Loss: 0.034252384771085255\n",
      "Epoch: 6, Batch: 1500, Avg. Loss: 0.03422349009262988\n",
      "Epoch: 6, Batch: 1510, Avg. Loss: 0.034192272249290265\n",
      "Epoch: 6, Batch: 1520, Avg. Loss: 0.03415907863674739\n",
      "Epoch: 6, Batch: 1530, Avg. Loss: 0.0341261755855723\n",
      "Epoch: 6, Batch: 1540, Avg. Loss: 0.034091842164961465\n",
      "Epoch: 7, Batch: 10, Avg. Loss: 0.034040223624715614\n",
      "Epoch: 7, Batch: 20, Avg. Loss: 0.034006365122507896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Batch: 30, Avg. Loss: 0.03397340899397352\n",
      "Epoch: 7, Batch: 40, Avg. Loss: 0.03393874243655721\n",
      "Epoch: 7, Batch: 50, Avg. Loss: 0.03390399341849453\n",
      "Epoch: 7, Batch: 60, Avg. Loss: 0.03387008076657448\n",
      "Epoch: 7, Batch: 70, Avg. Loss: 0.03383568292475279\n",
      "Epoch: 7, Batch: 80, Avg. Loss: 0.03380252026099692\n",
      "Epoch: 7, Batch: 90, Avg. Loss: 0.03376920220428256\n",
      "Epoch: 7, Batch: 100, Avg. Loss: 0.033734962076864515\n",
      "Epoch: 7, Batch: 110, Avg. Loss: 0.033701088890287806\n",
      "Epoch: 7, Batch: 120, Avg. Loss: 0.03366674869806712\n",
      "Epoch: 7, Batch: 130, Avg. Loss: 0.03363253666271613\n",
      "Epoch: 7, Batch: 140, Avg. Loss: 0.03359861968080409\n",
      "Epoch: 7, Batch: 150, Avg. Loss: 0.03356514315978088\n",
      "Epoch: 7, Batch: 160, Avg. Loss: 0.03353111499596104\n",
      "Epoch: 7, Batch: 170, Avg. Loss: 0.03349775419541048\n",
      "Epoch: 7, Batch: 180, Avg. Loss: 0.03346362005494927\n",
      "Epoch: 7, Batch: 190, Avg. Loss: 0.033429963486817676\n",
      "Epoch: 7, Batch: 200, Avg. Loss: 0.03339639700646328\n",
      "Epoch: 7, Batch: 210, Avg. Loss: 0.03336291984088781\n",
      "Epoch: 7, Batch: 220, Avg. Loss: 0.033329688731769544\n",
      "Epoch: 7, Batch: 230, Avg. Loss: 0.03329706067586933\n",
      "Epoch: 7, Batch: 240, Avg. Loss: 0.0332640412499951\n",
      "Epoch: 7, Batch: 250, Avg. Loss: 0.033230774243511355\n",
      "Epoch: 7, Batch: 260, Avg. Loss: 0.0331976028261086\n",
      "Epoch: 7, Batch: 270, Avg. Loss: 0.03316522459061685\n",
      "Epoch: 7, Batch: 280, Avg. Loss: 0.033132524936289875\n",
      "Epoch: 7, Batch: 290, Avg. Loss: 0.03309999515848469\n",
      "Epoch: 7, Batch: 300, Avg. Loss: 0.03306701653915416\n",
      "Epoch: 7, Batch: 310, Avg. Loss: 0.03303374798615484\n",
      "Epoch: 7, Batch: 320, Avg. Loss: 0.033000744909917765\n",
      "Epoch: 7, Batch: 330, Avg. Loss: 0.032967699300319606\n",
      "Epoch: 7, Batch: 340, Avg. Loss: 0.03293506702418728\n",
      "Epoch: 7, Batch: 350, Avg. Loss: 0.03290245413929587\n",
      "Epoch: 7, Batch: 360, Avg. Loss: 0.03286988632611824\n",
      "Epoch: 7, Batch: 370, Avg. Loss: 0.03283681248848113\n",
      "Epoch: 7, Batch: 380, Avg. Loss: 0.032804026024996415\n",
      "Epoch: 7, Batch: 390, Avg. Loss: 0.03277178731566236\n",
      "Epoch: 7, Batch: 400, Avg. Loss: 0.03273944756188502\n",
      "Epoch: 7, Batch: 410, Avg. Loss: 0.03270730506469184\n",
      "Epoch: 7, Batch: 420, Avg. Loss: 0.03267591045095005\n",
      "Epoch: 7, Batch: 430, Avg. Loss: 0.032644164283158444\n",
      "Epoch: 7, Batch: 440, Avg. Loss: 0.03261185623458536\n",
      "Epoch: 7, Batch: 450, Avg. Loss: 0.03258001808262045\n",
      "Epoch: 7, Batch: 460, Avg. Loss: 0.032548126571305416\n",
      "Epoch: 7, Batch: 470, Avg. Loss: 0.03251669747345656\n",
      "Epoch: 7, Batch: 480, Avg. Loss: 0.03248463494627751\n",
      "Epoch: 7, Batch: 490, Avg. Loss: 0.03245254621149674\n",
      "Epoch: 7, Batch: 500, Avg. Loss: 0.0324209690002291\n",
      "Epoch: 7, Batch: 510, Avg. Loss: 0.03238939142748014\n",
      "Epoch: 7, Batch: 520, Avg. Loss: 0.032358050578379136\n",
      "Epoch: 7, Batch: 530, Avg. Loss: 0.03232640636512442\n",
      "Epoch: 7, Batch: 540, Avg. Loss: 0.032295179890412494\n",
      "Epoch: 7, Batch: 550, Avg. Loss: 0.03226369157650937\n",
      "Epoch: 7, Batch: 560, Avg. Loss: 0.03223245303534435\n",
      "Epoch: 7, Batch: 570, Avg. Loss: 0.03220136953805611\n",
      "Epoch: 7, Batch: 580, Avg. Loss: 0.03217115843268598\n",
      "Epoch: 7, Batch: 590, Avg. Loss: 0.032140725164367875\n",
      "Epoch: 7, Batch: 600, Avg. Loss: 0.032110296095627114\n",
      "Epoch: 7, Batch: 610, Avg. Loss: 0.03207953784205081\n",
      "Epoch: 7, Batch: 620, Avg. Loss: 0.0320491401138226\n",
      "Epoch: 7, Batch: 630, Avg. Loss: 0.032019439238648494\n",
      "Epoch: 7, Batch: 640, Avg. Loss: 0.03198955425802633\n",
      "Epoch: 7, Batch: 650, Avg. Loss: 0.03195876907930247\n",
      "Epoch: 7, Batch: 660, Avg. Loss: 0.031928956441704986\n",
      "Epoch: 7, Batch: 670, Avg. Loss: 0.03189901291485103\n",
      "Epoch: 7, Batch: 680, Avg. Loss: 0.03186834127292442\n",
      "Epoch: 7, Batch: 690, Avg. Loss: 0.031838248179211694\n",
      "Epoch: 7, Batch: 700, Avg. Loss: 0.03180904284211221\n",
      "Epoch: 7, Batch: 710, Avg. Loss: 0.03177997052312642\n",
      "Epoch: 7, Batch: 720, Avg. Loss: 0.0317497705133411\n",
      "Epoch: 7, Batch: 730, Avg. Loss: 0.0317193359952245\n",
      "Epoch: 7, Batch: 740, Avg. Loss: 0.031689292449816595\n",
      "Epoch: 7, Batch: 750, Avg. Loss: 0.031659039890874374\n",
      "Epoch: 7, Batch: 760, Avg. Loss: 0.03162902165884681\n",
      "Epoch: 7, Batch: 770, Avg. Loss: 0.03159955186570493\n",
      "Epoch: 7, Batch: 780, Avg. Loss: 0.03157230526421794\n",
      "Epoch: 7, Batch: 790, Avg. Loss: 0.031542976249673976\n",
      "Epoch: 7, Batch: 800, Avg. Loss: 0.0315132420456439\n",
      "Epoch: 7, Batch: 810, Avg. Loss: 0.031484277125313626\n",
      "Epoch: 7, Batch: 820, Avg. Loss: 0.031455401114033374\n",
      "Epoch: 7, Batch: 830, Avg. Loss: 0.03142681451763092\n",
      "Epoch: 7, Batch: 840, Avg. Loss: 0.031399287619246825\n",
      "Epoch: 7, Batch: 850, Avg. Loss: 0.03137043446386348\n",
      "Epoch: 7, Batch: 860, Avg. Loss: 0.03134261921145685\n",
      "Epoch: 7, Batch: 870, Avg. Loss: 0.03131423266856996\n",
      "Epoch: 7, Batch: 880, Avg. Loss: 0.03128530161617143\n",
      "Epoch: 7, Batch: 890, Avg. Loss: 0.03125643465797721\n",
      "Epoch: 7, Batch: 900, Avg. Loss: 0.031227796514769125\n",
      "Epoch: 7, Batch: 910, Avg. Loss: 0.031198558734944086\n",
      "Epoch: 7, Batch: 920, Avg. Loss: 0.031169569326030037\n",
      "Epoch: 7, Batch: 930, Avg. Loss: 0.031140760043828753\n",
      "Epoch: 7, Batch: 940, Avg. Loss: 0.0311122332350066\n",
      "Epoch: 7, Batch: 950, Avg. Loss: 0.031083079052230567\n",
      "Epoch: 7, Batch: 960, Avg. Loss: 0.031054089295597765\n",
      "Epoch: 7, Batch: 970, Avg. Loss: 0.03102544697830858\n",
      "Epoch: 7, Batch: 980, Avg. Loss: 0.030997324949025976\n",
      "Epoch: 7, Batch: 990, Avg. Loss: 0.030970187107477832\n",
      "Epoch: 7, Batch: 1000, Avg. Loss: 0.030943697573693012\n",
      "Epoch: 7, Batch: 1010, Avg. Loss: 0.030916215571040385\n",
      "Epoch: 7, Batch: 1020, Avg. Loss: 0.030889589609522844\n",
      "Epoch: 7, Batch: 1030, Avg. Loss: 0.03086643095527812\n",
      "Epoch: 7, Batch: 1040, Avg. Loss: 0.030840559819492255\n",
      "Epoch: 7, Batch: 1050, Avg. Loss: 0.030815095443342818\n",
      "Epoch: 7, Batch: 1060, Avg. Loss: 0.030789788279676315\n",
      "Epoch: 7, Batch: 1070, Avg. Loss: 0.03076374560180775\n",
      "Epoch: 7, Batch: 1080, Avg. Loss: 0.030737964207977003\n",
      "Epoch: 7, Batch: 1090, Avg. Loss: 0.030714699529180798\n",
      "Epoch: 7, Batch: 1100, Avg. Loss: 0.030691769499213893\n",
      "Epoch: 7, Batch: 1110, Avg. Loss: 0.03066991627115786\n",
      "Epoch: 7, Batch: 1120, Avg. Loss: 0.030645758283604295\n",
      "Epoch: 7, Batch: 1130, Avg. Loss: 0.030626251403543765\n",
      "Epoch: 7, Batch: 1140, Avg. Loss: 0.030607258295411208\n",
      "Epoch: 7, Batch: 1150, Avg. Loss: 0.030583602509967205\n",
      "Epoch: 7, Batch: 1160, Avg. Loss: 0.030566766540365713\n",
      "Epoch: 7, Batch: 1170, Avg. Loss: 0.030544694330078063\n",
      "Epoch: 7, Batch: 1180, Avg. Loss: 0.030526538812977386\n",
      "Epoch: 7, Batch: 1190, Avg. Loss: 0.030505061594417165\n",
      "Epoch: 7, Batch: 1200, Avg. Loss: 0.030480709847975254\n",
      "Epoch: 7, Batch: 1210, Avg. Loss: 0.030456280256091924\n",
      "Epoch: 7, Batch: 1220, Avg. Loss: 0.030432064638741752\n",
      "Epoch: 7, Batch: 1230, Avg. Loss: 0.03040806753878397\n",
      "Epoch: 7, Batch: 1240, Avg. Loss: 0.030383440297163618\n",
      "Epoch: 7, Batch: 1250, Avg. Loss: 0.030361510787093947\n",
      "Epoch: 7, Batch: 1260, Avg. Loss: 0.03033839108802641\n",
      "Epoch: 7, Batch: 1270, Avg. Loss: 0.030316194165843566\n",
      "Epoch: 7, Batch: 1280, Avg. Loss: 0.0302940975006814\n",
      "Epoch: 7, Batch: 1290, Avg. Loss: 0.030273009005816538\n",
      "Epoch: 7, Batch: 1300, Avg. Loss: 0.030250487808489306\n",
      "Epoch: 7, Batch: 1310, Avg. Loss: 0.03022799561609988\n",
      "Epoch: 7, Batch: 1320, Avg. Loss: 0.030207423356946456\n",
      "Epoch: 7, Batch: 1330, Avg. Loss: 0.030184585613007353\n",
      "Epoch: 7, Batch: 1340, Avg. Loss: 0.030159408409205266\n",
      "Epoch: 7, Batch: 1350, Avg. Loss: 0.030136255020467455\n",
      "Epoch: 7, Batch: 1360, Avg. Loss: 0.03011309133170367\n",
      "Epoch: 7, Batch: 1370, Avg. Loss: 0.03008890830097062\n",
      "Epoch: 7, Batch: 1380, Avg. Loss: 0.03006434631638263\n",
      "Epoch: 7, Batch: 1390, Avg. Loss: 0.030040661840590677\n",
      "Epoch: 7, Batch: 1400, Avg. Loss: 0.030016687516449127\n",
      "Epoch: 7, Batch: 1410, Avg. Loss: 0.029993824026911056\n",
      "Epoch: 7, Batch: 1420, Avg. Loss: 0.029970085257369338\n",
      "Epoch: 7, Batch: 1430, Avg. Loss: 0.02994626327337861\n",
      "Epoch: 7, Batch: 1440, Avg. Loss: 0.029922983966746277\n",
      "Epoch: 7, Batch: 1450, Avg. Loss: 0.02989863667302246\n",
      "Epoch: 7, Batch: 1460, Avg. Loss: 0.029875558543043506\n",
      "Epoch: 7, Batch: 1470, Avg. Loss: 0.029852422712154717\n",
      "Epoch: 7, Batch: 1480, Avg. Loss: 0.02982989972803559\n",
      "Epoch: 7, Batch: 1490, Avg. Loss: 0.029807287253151984\n",
      "Epoch: 7, Batch: 1500, Avg. Loss: 0.029785775042969836\n",
      "Epoch: 7, Batch: 1510, Avg. Loss: 0.02976250795210445\n",
      "Epoch: 7, Batch: 1520, Avg. Loss: 0.0297388276182477\n",
      "Epoch: 7, Batch: 1530, Avg. Loss: 0.029713897356820153\n",
      "Epoch: 7, Batch: 1540, Avg. Loss: 0.029688615950983332\n",
      "Epoch: 8, Batch: 10, Avg. Loss: 0.02964922462098904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Batch: 20, Avg. Loss: 0.029624302351124853\n",
      "Epoch: 8, Batch: 30, Avg. Loss: 0.02960081358920258\n",
      "Epoch: 8, Batch: 40, Avg. Loss: 0.029575685223523517\n",
      "Epoch: 8, Batch: 50, Avg. Loss: 0.02955003890625408\n",
      "Epoch: 8, Batch: 60, Avg. Loss: 0.029525324046994723\n",
      "Epoch: 8, Batch: 70, Avg. Loss: 0.029499481102358813\n",
      "Epoch: 8, Batch: 80, Avg. Loss: 0.029475055650583268\n",
      "Epoch: 8, Batch: 90, Avg. Loss: 0.02945012482839564\n",
      "Epoch: 8, Batch: 100, Avg. Loss: 0.029424999460776705\n",
      "Epoch: 8, Batch: 110, Avg. Loss: 0.029399977666938668\n",
      "Epoch: 8, Batch: 120, Avg. Loss: 0.029374774085765647\n",
      "Epoch: 8, Batch: 130, Avg. Loss: 0.029349350557267003\n",
      "Epoch: 8, Batch: 140, Avg. Loss: 0.029323846578738686\n",
      "Epoch: 8, Batch: 150, Avg. Loss: 0.029298654038028206\n",
      "Epoch: 8, Batch: 160, Avg. Loss: 0.029273559625357683\n",
      "Epoch: 8, Batch: 170, Avg. Loss: 0.029248460651602633\n",
      "Epoch: 8, Batch: 180, Avg. Loss: 0.02922301188677572\n",
      "Epoch: 8, Batch: 190, Avg. Loss: 0.029197664676597422\n",
      "Epoch: 8, Batch: 200, Avg. Loss: 0.029172768670019385\n",
      "Epoch: 8, Batch: 210, Avg. Loss: 0.029147935333973085\n",
      "Epoch: 8, Batch: 220, Avg. Loss: 0.029123072099719618\n",
      "Epoch: 8, Batch: 230, Avg. Loss: 0.02909810271539394\n",
      "Epoch: 8, Batch: 240, Avg. Loss: 0.029073164115893585\n",
      "Epoch: 8, Batch: 250, Avg. Loss: 0.02904817890148669\n",
      "Epoch: 8, Batch: 260, Avg. Loss: 0.02902346358189304\n",
      "Epoch: 8, Batch: 270, Avg. Loss: 0.02899878770640854\n",
      "Epoch: 8, Batch: 280, Avg. Loss: 0.028974333919041362\n",
      "Epoch: 8, Batch: 290, Avg. Loss: 0.02894982388040759\n",
      "Epoch: 8, Batch: 300, Avg. Loss: 0.028924858655212597\n",
      "Epoch: 8, Batch: 310, Avg. Loss: 0.028899736715191273\n",
      "Epoch: 8, Batch: 320, Avg. Loss: 0.028875461189992204\n",
      "Epoch: 8, Batch: 330, Avg. Loss: 0.028850904470327156\n",
      "Epoch: 8, Batch: 340, Avg. Loss: 0.02882611223890469\n",
      "Epoch: 8, Batch: 350, Avg. Loss: 0.028801358594197538\n",
      "Epoch: 8, Batch: 360, Avg. Loss: 0.028776753673789176\n",
      "Epoch: 8, Batch: 370, Avg. Loss: 0.028751992502057913\n",
      "Epoch: 8, Batch: 380, Avg. Loss: 0.028727870252580168\n",
      "Epoch: 8, Batch: 390, Avg. Loss: 0.028703505783882935\n",
      "Epoch: 8, Batch: 400, Avg. Loss: 0.028679052528291196\n",
      "Epoch: 8, Batch: 410, Avg. Loss: 0.028655027727265574\n",
      "Epoch: 8, Batch: 420, Avg. Loss: 0.028631283615121825\n",
      "Epoch: 8, Batch: 430, Avg. Loss: 0.02860704685318088\n",
      "Epoch: 8, Batch: 440, Avg. Loss: 0.02858294744794238\n",
      "Epoch: 8, Batch: 450, Avg. Loss: 0.028558821957832724\n",
      "Epoch: 8, Batch: 460, Avg. Loss: 0.028534586944605627\n",
      "Epoch: 8, Batch: 470, Avg. Loss: 0.02851091584891229\n",
      "Epoch: 8, Batch: 480, Avg. Loss: 0.028486582428845747\n",
      "Epoch: 8, Batch: 490, Avg. Loss: 0.028462470718483977\n",
      "Epoch: 8, Batch: 500, Avg. Loss: 0.028438780598349245\n",
      "Epoch: 8, Batch: 510, Avg. Loss: 0.028414932861294165\n",
      "Epoch: 8, Batch: 520, Avg. Loss: 0.028391244663464616\n",
      "Epoch: 8, Batch: 530, Avg. Loss: 0.02836730368793367\n",
      "Epoch: 8, Batch: 540, Avg. Loss: 0.028344057334144454\n",
      "Epoch: 8, Batch: 550, Avg. Loss: 0.02832003352717964\n",
      "Epoch: 8, Batch: 560, Avg. Loss: 0.02829603618882894\n",
      "Epoch: 8, Batch: 570, Avg. Loss: 0.028272302116690448\n",
      "Epoch: 8, Batch: 580, Avg. Loss: 0.028249166282604637\n",
      "Epoch: 8, Batch: 590, Avg. Loss: 0.02822612183894639\n",
      "Epoch: 8, Batch: 600, Avg. Loss: 0.028202868019811853\n",
      "Epoch: 8, Batch: 610, Avg. Loss: 0.028179577101980154\n",
      "Epoch: 8, Batch: 620, Avg. Loss: 0.028156607322114398\n",
      "Epoch: 8, Batch: 630, Avg. Loss: 0.028134080564884756\n",
      "Epoch: 8, Batch: 640, Avg. Loss: 0.02811074945595532\n",
      "Epoch: 8, Batch: 650, Avg. Loss: 0.0280872767431392\n",
      "Epoch: 8, Batch: 660, Avg. Loss: 0.028064238066815374\n",
      "Epoch: 8, Batch: 670, Avg. Loss: 0.028041379489664342\n",
      "Epoch: 8, Batch: 680, Avg. Loss: 0.028017917211068843\n",
      "Epoch: 8, Batch: 690, Avg. Loss: 0.02799540981414508\n",
      "Epoch: 8, Batch: 700, Avg. Loss: 0.0279727989406713\n",
      "Epoch: 8, Batch: 710, Avg. Loss: 0.027950514585668416\n",
      "Epoch: 8, Batch: 720, Avg. Loss: 0.02792785560276092\n",
      "Epoch: 8, Batch: 730, Avg. Loss: 0.027905020576420553\n",
      "Epoch: 8, Batch: 740, Avg. Loss: 0.027882783012721218\n",
      "Epoch: 8, Batch: 750, Avg. Loss: 0.027860129410541237\n",
      "Epoch: 8, Batch: 760, Avg. Loss: 0.027837299573620095\n",
      "Epoch: 8, Batch: 770, Avg. Loss: 0.027814755255270298\n",
      "Epoch: 8, Batch: 780, Avg. Loss: 0.027793261566554078\n",
      "Epoch: 8, Batch: 790, Avg. Loss: 0.027770597064167298\n",
      "Epoch: 8, Batch: 800, Avg. Loss: 0.02774811346663121\n",
      "Epoch: 8, Batch: 810, Avg. Loss: 0.027726003343736916\n",
      "Epoch: 8, Batch: 820, Avg. Loss: 0.02770395805481335\n",
      "Epoch: 8, Batch: 830, Avg. Loss: 0.027681908840889884\n",
      "Epoch: 8, Batch: 840, Avg. Loss: 0.027661274950794998\n",
      "Epoch: 8, Batch: 850, Avg. Loss: 0.02763937997290732\n",
      "Epoch: 8, Batch: 860, Avg. Loss: 0.027617759704085153\n",
      "Epoch: 8, Batch: 870, Avg. Loss: 0.02759585838931737\n",
      "Epoch: 8, Batch: 880, Avg. Loss: 0.027573745465622437\n",
      "Epoch: 8, Batch: 890, Avg. Loss: 0.027551636645737718\n",
      "Epoch: 8, Batch: 900, Avg. Loss: 0.027529720820912002\n",
      "Epoch: 8, Batch: 910, Avg. Loss: 0.027507817414178286\n",
      "Epoch: 8, Batch: 920, Avg. Loss: 0.027485899255452577\n",
      "Epoch: 8, Batch: 930, Avg. Loss: 0.027464201780985544\n",
      "Epoch: 8, Batch: 940, Avg. Loss: 0.027442706323743158\n",
      "Epoch: 8, Batch: 950, Avg. Loss: 0.02742160331567405\n",
      "Epoch: 8, Batch: 960, Avg. Loss: 0.027399352814210548\n",
      "Epoch: 8, Batch: 970, Avg. Loss: 0.027377549352796366\n",
      "Epoch: 8, Batch: 980, Avg. Loss: 0.0273566957044176\n",
      "Epoch: 8, Batch: 990, Avg. Loss: 0.02733566620476171\n",
      "Epoch: 8, Batch: 1000, Avg. Loss: 0.027315614738661405\n",
      "Epoch: 8, Batch: 1010, Avg. Loss: 0.02729483664257794\n",
      "Epoch: 8, Batch: 1020, Avg. Loss: 0.027274583900701307\n",
      "Epoch: 8, Batch: 1030, Avg. Loss: 0.027256562726015433\n",
      "Epoch: 8, Batch: 1040, Avg. Loss: 0.027237978382063866\n",
      "Epoch: 8, Batch: 1050, Avg. Loss: 0.027218566789540714\n",
      "Epoch: 8, Batch: 1060, Avg. Loss: 0.02719953351334399\n",
      "Epoch: 8, Batch: 1070, Avg. Loss: 0.027179946428059477\n",
      "Epoch: 8, Batch: 1080, Avg. Loss: 0.027160173347730347\n",
      "Epoch: 8, Batch: 1090, Avg. Loss: 0.027143080455185105\n",
      "Epoch: 8, Batch: 1100, Avg. Loss: 0.02712558941658084\n",
      "Epoch: 8, Batch: 1110, Avg. Loss: 0.02710976761581276\n",
      "Epoch: 8, Batch: 1120, Avg. Loss: 0.027093616033891366\n",
      "Epoch: 8, Batch: 1130, Avg. Loss: 0.02707694780233656\n",
      "Epoch: 8, Batch: 1140, Avg. Loss: 0.02706354635194672\n",
      "Epoch: 8, Batch: 1150, Avg. Loss: 0.02704549441803312\n",
      "Epoch: 8, Batch: 1160, Avg. Loss: 0.027030575644704322\n",
      "Epoch: 8, Batch: 1170, Avg. Loss: 0.027014111735456068\n",
      "Epoch: 8, Batch: 1180, Avg. Loss: 0.0269989482512292\n",
      "Epoch: 8, Batch: 1190, Avg. Loss: 0.026982852747988183\n",
      "Epoch: 8, Batch: 1200, Avg. Loss: 0.026965904964257804\n",
      "Epoch: 8, Batch: 1210, Avg. Loss: 0.026948783055980256\n",
      "Epoch: 8, Batch: 1220, Avg. Loss: 0.026931452813095887\n",
      "Epoch: 8, Batch: 1230, Avg. Loss: 0.02691297599935896\n",
      "Epoch: 8, Batch: 1240, Avg. Loss: 0.02689384749655408\n",
      "Epoch: 8, Batch: 1250, Avg. Loss: 0.02687800172902464\n",
      "Epoch: 8, Batch: 1260, Avg. Loss: 0.02686220128817448\n",
      "Epoch: 8, Batch: 1270, Avg. Loss: 0.026846719996988327\n",
      "Epoch: 8, Batch: 1280, Avg. Loss: 0.0268326995808822\n",
      "Epoch: 8, Batch: 1290, Avg. Loss: 0.026815344540027854\n",
      "Epoch: 8, Batch: 1300, Avg. Loss: 0.026798068580034543\n",
      "Epoch: 8, Batch: 1310, Avg. Loss: 0.02678083649246976\n",
      "Epoch: 8, Batch: 1320, Avg. Loss: 0.026764285394803442\n",
      "Epoch: 8, Batch: 1330, Avg. Loss: 0.02674659388878278\n",
      "Epoch: 8, Batch: 1340, Avg. Loss: 0.026728246623119227\n",
      "Epoch: 8, Batch: 1350, Avg. Loss: 0.026709928995827102\n",
      "Epoch: 8, Batch: 1360, Avg. Loss: 0.026693007571803606\n",
      "Epoch: 8, Batch: 1370, Avg. Loss: 0.026675270310517883\n",
      "Epoch: 8, Batch: 1380, Avg. Loss: 0.026657431310773673\n",
      "Epoch: 8, Batch: 1390, Avg. Loss: 0.026640121742847463\n",
      "Epoch: 8, Batch: 1400, Avg. Loss: 0.02662273828249501\n",
      "Epoch: 8, Batch: 1410, Avg. Loss: 0.02660766699909157\n",
      "Epoch: 8, Batch: 1420, Avg. Loss: 0.026591450174016585\n",
      "Epoch: 8, Batch: 1430, Avg. Loss: 0.026573897004418565\n",
      "Epoch: 8, Batch: 1440, Avg. Loss: 0.02655630889167668\n",
      "Epoch: 8, Batch: 1450, Avg. Loss: 0.026538537638676214\n",
      "Epoch: 8, Batch: 1460, Avg. Loss: 0.026519815312025283\n",
      "Epoch: 8, Batch: 1470, Avg. Loss: 0.026501924729745743\n",
      "Epoch: 8, Batch: 1480, Avg. Loss: 0.026485366791601878\n",
      "Epoch: 8, Batch: 1490, Avg. Loss: 0.02646776951275649\n",
      "Epoch: 8, Batch: 1500, Avg. Loss: 0.02645016421416517\n",
      "Epoch: 8, Batch: 1510, Avg. Loss: 0.02643174931405266\n",
      "Epoch: 8, Batch: 1520, Avg. Loss: 0.02641290782245695\n",
      "Epoch: 8, Batch: 1530, Avg. Loss: 0.02639377099159547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Batch: 1540, Avg. Loss: 0.026374743300277215\n",
      "Epoch: 9, Batch: 10, Avg. Loss: 0.02634473381346851\n",
      "Epoch: 9, Batch: 20, Avg. Loss: 0.026325562853616636\n",
      "Epoch: 9, Batch: 30, Avg. Loss: 0.026306430264101052\n",
      "Epoch: 9, Batch: 40, Avg. Loss: 0.026286832611343295\n",
      "Epoch: 9, Batch: 50, Avg. Loss: 0.02626727995471163\n",
      "Epoch: 9, Batch: 60, Avg. Loss: 0.026248014078061866\n",
      "Epoch: 9, Batch: 70, Avg. Loss: 0.026228434605459414\n",
      "Epoch: 9, Batch: 80, Avg. Loss: 0.026209273986699772\n",
      "Epoch: 9, Batch: 90, Avg. Loss: 0.026189860365977476\n",
      "Epoch: 9, Batch: 100, Avg. Loss: 0.026170389293025256\n",
      "Epoch: 9, Batch: 110, Avg. Loss: 0.026151261994218526\n",
      "Epoch: 9, Batch: 120, Avg. Loss: 0.02613160439794474\n",
      "Epoch: 9, Batch: 130, Avg. Loss: 0.02611227579185321\n",
      "Epoch: 9, Batch: 140, Avg. Loss: 0.026092775108667084\n",
      "Epoch: 9, Batch: 150, Avg. Loss: 0.026073349746732328\n",
      "Epoch: 9, Batch: 160, Avg. Loss: 0.02605450107019121\n",
      "Epoch: 9, Batch: 170, Avg. Loss: 0.02603500211046715\n",
      "Epoch: 9, Batch: 180, Avg. Loss: 0.026015544006376682\n",
      "Epoch: 9, Batch: 190, Avg. Loss: 0.02599606578021563\n",
      "Epoch: 9, Batch: 200, Avg. Loss: 0.025976523135735584\n",
      "Epoch: 9, Batch: 210, Avg. Loss: 0.02595675461617751\n",
      "Epoch: 9, Batch: 220, Avg. Loss: 0.025937275671186998\n",
      "Epoch: 9, Batch: 230, Avg. Loss: 0.02591803931700866\n",
      "Epoch: 9, Batch: 240, Avg. Loss: 0.025898797901713986\n",
      "Epoch: 9, Batch: 250, Avg. Loss: 0.025879498761037054\n",
      "Epoch: 9, Batch: 260, Avg. Loss: 0.025860275453248413\n",
      "Epoch: 9, Batch: 270, Avg. Loss: 0.02584088090234105\n",
      "Epoch: 9, Batch: 280, Avg. Loss: 0.025822150210666837\n",
      "Epoch: 9, Batch: 290, Avg. Loss: 0.025802948755788677\n",
      "Epoch: 9, Batch: 300, Avg. Loss: 0.025783753517355713\n",
      "Epoch: 9, Batch: 310, Avg. Loss: 0.025764490496520392\n",
      "Epoch: 9, Batch: 320, Avg. Loss: 0.02574582803871863\n",
      "Epoch: 9, Batch: 330, Avg. Loss: 0.025726871401308797\n",
      "Epoch: 9, Batch: 340, Avg. Loss: 0.025707746678698903\n",
      "Epoch: 9, Batch: 350, Avg. Loss: 0.025688399171641136\n",
      "Epoch: 9, Batch: 360, Avg. Loss: 0.02566940103686961\n",
      "Epoch: 9, Batch: 370, Avg. Loss: 0.02565011172812219\n",
      "Epoch: 9, Batch: 380, Avg. Loss: 0.02563146876652053\n",
      "Epoch: 9, Batch: 390, Avg. Loss: 0.02561253791610563\n",
      "Epoch: 9, Batch: 400, Avg. Loss: 0.025593682311653924\n",
      "Epoch: 9, Batch: 410, Avg. Loss: 0.025574772183686516\n",
      "Epoch: 9, Batch: 420, Avg. Loss: 0.025555939151356274\n",
      "Epoch: 9, Batch: 430, Avg. Loss: 0.025537028330664756\n",
      "Epoch: 9, Batch: 440, Avg. Loss: 0.025518071147069155\n",
      "Epoch: 9, Batch: 450, Avg. Loss: 0.025499415327564598\n",
      "Epoch: 9, Batch: 460, Avg. Loss: 0.025480711713965088\n",
      "Epoch: 9, Batch: 470, Avg. Loss: 0.025462309746850608\n",
      "Epoch: 9, Batch: 480, Avg. Loss: 0.025443315559991287\n",
      "Epoch: 9, Batch: 490, Avg. Loss: 0.025424721948524766\n",
      "Epoch: 9, Batch: 500, Avg. Loss: 0.025406023101887967\n",
      "Epoch: 9, Batch: 510, Avg. Loss: 0.02538710125053161\n",
      "Epoch: 9, Batch: 520, Avg. Loss: 0.025368386931282648\n",
      "Epoch: 9, Batch: 530, Avg. Loss: 0.025349390877906344\n",
      "Epoch: 9, Batch: 540, Avg. Loss: 0.025330885976516688\n",
      "Epoch: 9, Batch: 550, Avg. Loss: 0.025312349871205005\n",
      "Epoch: 9, Batch: 560, Avg. Loss: 0.02529386876463703\n",
      "Epoch: 9, Batch: 570, Avg. Loss: 0.025275510704190856\n",
      "Epoch: 9, Batch: 580, Avg. Loss: 0.02525739212744952\n",
      "Epoch: 9, Batch: 590, Avg. Loss: 0.025239229760580226\n",
      "Epoch: 9, Batch: 600, Avg. Loss: 0.025220973632376822\n",
      "Epoch: 9, Batch: 610, Avg. Loss: 0.02520257929994333\n",
      "Epoch: 9, Batch: 620, Avg. Loss: 0.025184343401584926\n",
      "Epoch: 9, Batch: 630, Avg. Loss: 0.025166771809973183\n",
      "Epoch: 9, Batch: 640, Avg. Loss: 0.025148410290375248\n",
      "Epoch: 9, Batch: 650, Avg. Loss: 0.02513006637227446\n",
      "Epoch: 9, Batch: 660, Avg. Loss: 0.025112506359836206\n",
      "Epoch: 9, Batch: 670, Avg. Loss: 0.025094732402558852\n",
      "Epoch: 9, Batch: 680, Avg. Loss: 0.02507648060636092\n",
      "Epoch: 9, Batch: 690, Avg. Loss: 0.02506078291749199\n",
      "Epoch: 9, Batch: 700, Avg. Loss: 0.025044132379053237\n",
      "Epoch: 9, Batch: 710, Avg. Loss: 0.02502731841077062\n",
      "Epoch: 9, Batch: 720, Avg. Loss: 0.025009759541289263\n",
      "Epoch: 9, Batch: 730, Avg. Loss: 0.024991913268673824\n",
      "Epoch: 9, Batch: 740, Avg. Loss: 0.024975585065473566\n",
      "Epoch: 9, Batch: 750, Avg. Loss: 0.024958235582347627\n",
      "Epoch: 9, Batch: 760, Avg. Loss: 0.024940315824525137\n",
      "Epoch: 9, Batch: 770, Avg. Loss: 0.024923268632358386\n",
      "Epoch: 9, Batch: 780, Avg. Loss: 0.0249074187931994\n",
      "Epoch: 9, Batch: 790, Avg. Loss: 0.024890600252942152\n",
      "Epoch: 9, Batch: 800, Avg. Loss: 0.02487353726891411\n",
      "Epoch: 9, Batch: 810, Avg. Loss: 0.024856387450552943\n",
      "Epoch: 9, Batch: 820, Avg. Loss: 0.024840096882363393\n",
      "Epoch: 9, Batch: 830, Avg. Loss: 0.024823385188184995\n",
      "Epoch: 9, Batch: 840, Avg. Loss: 0.024806746552216628\n",
      "Epoch: 9, Batch: 850, Avg. Loss: 0.024789740764421548\n",
      "Epoch: 9, Batch: 860, Avg. Loss: 0.024773252313159515\n",
      "Epoch: 9, Batch: 870, Avg. Loss: 0.02475644649427947\n",
      "Epoch: 9, Batch: 880, Avg. Loss: 0.024739213608306432\n",
      "Epoch: 9, Batch: 890, Avg. Loss: 0.024722115057836787\n",
      "Epoch: 9, Batch: 900, Avg. Loss: 0.024705721064504808\n",
      "Epoch: 9, Batch: 910, Avg. Loss: 0.024688496358338044\n",
      "Epoch: 9, Batch: 920, Avg. Loss: 0.024671145098062237\n",
      "Epoch: 9, Batch: 930, Avg. Loss: 0.024654245307653058\n",
      "Epoch: 9, Batch: 940, Avg. Loss: 0.024637152125191776\n",
      "Epoch: 9, Batch: 950, Avg. Loss: 0.024619732200937584\n",
      "Epoch: 9, Batch: 960, Avg. Loss: 0.02460238270502851\n",
      "Epoch: 9, Batch: 970, Avg. Loss: 0.024585228025178155\n",
      "Epoch: 9, Batch: 980, Avg. Loss: 0.024568303468689903\n",
      "Epoch: 9, Batch: 990, Avg. Loss: 0.024552165787741068\n",
      "Epoch: 9, Batch: 1000, Avg. Loss: 0.024537078946807993\n",
      "Epoch: 9, Batch: 1010, Avg. Loss: 0.02452060035981077\n",
      "Epoch: 9, Batch: 1020, Avg. Loss: 0.024505843695524297\n",
      "Epoch: 9, Batch: 1030, Avg. Loss: 0.024492544326288463\n",
      "Epoch: 9, Batch: 1040, Avg. Loss: 0.024477716001156113\n",
      "Epoch: 9, Batch: 1050, Avg. Loss: 0.02446353112440846\n",
      "Epoch: 9, Batch: 1060, Avg. Loss: 0.024449397757745194\n",
      "Epoch: 9, Batch: 1070, Avg. Loss: 0.02443606139827172\n",
      "Epoch: 9, Batch: 1080, Avg. Loss: 0.02442621888969806\n",
      "Epoch: 9, Batch: 1090, Avg. Loss: 0.024441296693569113\n",
      "Epoch: 9, Batch: 1100, Avg. Loss: 0.024438530727076323\n",
      "Epoch: 9, Batch: 1110, Avg. Loss: 0.02459607443979976\n",
      "Epoch: 9, Batch: 1120, Avg. Loss: 0.024612655742251275\n",
      "Epoch: 9, Batch: 1130, Avg. Loss: 0.02524456584850323\n",
      "Epoch: 9, Batch: 1140, Avg. Loss: 0.027600741852396776\n",
      "Epoch: 9, Batch: 1150, Avg. Loss: 0.02888212912039364\n",
      "Epoch: 9, Batch: 1160, Avg. Loss: 0.02996629577514318\n",
      "Epoch: 9, Batch: 1170, Avg. Loss: 0.030393202884342524\n",
      "Epoch: 9, Batch: 1180, Avg. Loss: 0.030644409986920314\n",
      "Epoch: 9, Batch: 1190, Avg. Loss: 0.030671656179280426\n",
      "Epoch: 9, Batch: 1200, Avg. Loss: 0.030696618179736016\n",
      "Epoch: 9, Batch: 1210, Avg. Loss: 0.03070601187581695\n",
      "Epoch: 9, Batch: 1220, Avg. Loss: 0.03070825433867352\n",
      "Epoch: 9, Batch: 1230, Avg. Loss: 0.03071614567161701\n",
      "Epoch: 9, Batch: 1240, Avg. Loss: 0.03071031397788218\n",
      "Epoch: 9, Batch: 1250, Avg. Loss: 0.03071473747822787\n",
      "Epoch: 9, Batch: 1260, Avg. Loss: 0.03072855494917729\n",
      "Epoch: 9, Batch: 1270, Avg. Loss: 0.030730171080104538\n",
      "Epoch: 9, Batch: 1280, Avg. Loss: 0.03073590720493103\n",
      "Epoch: 9, Batch: 1290, Avg. Loss: 0.030727140824145027\n",
      "Epoch: 9, Batch: 1300, Avg. Loss: 0.0307169281001907\n",
      "Epoch: 9, Batch: 1310, Avg. Loss: 0.030711413327627734\n",
      "Epoch: 9, Batch: 1320, Avg. Loss: 0.030714846143060453\n",
      "Epoch: 9, Batch: 1330, Avg. Loss: 0.030707787745363226\n",
      "Epoch: 9, Batch: 1340, Avg. Loss: 0.03070009352493431\n",
      "Epoch: 9, Batch: 1350, Avg. Loss: 0.030697082800134983\n",
      "Epoch: 9, Batch: 1360, Avg. Loss: 0.030695559411994346\n",
      "Epoch: 9, Batch: 1370, Avg. Loss: 0.03068956487458059\n",
      "Epoch: 9, Batch: 1380, Avg. Loss: 0.03068602830323805\n",
      "Epoch: 9, Batch: 1390, Avg. Loss: 0.030682825571759484\n",
      "Epoch: 9, Batch: 1400, Avg. Loss: 0.030681980316734584\n",
      "Epoch: 9, Batch: 1410, Avg. Loss: 0.03067768539413447\n",
      "Epoch: 9, Batch: 1420, Avg. Loss: 0.030669644480756694\n",
      "Epoch: 9, Batch: 1430, Avg. Loss: 0.03066209685029515\n",
      "Epoch: 9, Batch: 1440, Avg. Loss: 0.030652343931020235\n",
      "Epoch: 9, Batch: 1450, Avg. Loss: 0.030640242378578302\n",
      "Epoch: 9, Batch: 1460, Avg. Loss: 0.03062928229244313\n",
      "Epoch: 9, Batch: 1470, Avg. Loss: 0.03061956400943316\n",
      "Epoch: 9, Batch: 1480, Avg. Loss: 0.030613568915750105\n",
      "Epoch: 9, Batch: 1490, Avg. Loss: 0.030601047531450782\n",
      "Epoch: 9, Batch: 1500, Avg. Loss: 0.03058935467333838\n",
      "Epoch: 9, Batch: 1510, Avg. Loss: 0.030581489430185772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Batch: 1520, Avg. Loss: 0.030570767558999037\n",
      "Epoch: 9, Batch: 1530, Avg. Loss: 0.03055941222053134\n",
      "Epoch: 9, Batch: 1540, Avg. Loss: 0.030543256507713716\n",
      "Epoch: 10, Batch: 10, Avg. Loss: 0.03052531810189402\n",
      "Epoch: 10, Batch: 20, Avg. Loss: 0.030514818160641322\n",
      "Epoch: 10, Batch: 30, Avg. Loss: 0.030501625234923543\n",
      "Epoch: 10, Batch: 40, Avg. Loss: 0.030486783562224663\n",
      "Epoch: 10, Batch: 50, Avg. Loss: 0.03046997967076549\n",
      "Epoch: 10, Batch: 60, Avg. Loss: 0.03045744262812126\n",
      "Epoch: 10, Batch: 70, Avg. Loss: 0.03044575128944475\n",
      "Epoch: 10, Batch: 80, Avg. Loss: 0.03042931567658952\n",
      "Epoch: 10, Batch: 90, Avg. Loss: 0.030413169329652684\n",
      "Epoch: 10, Batch: 100, Avg. Loss: 0.03039629152714044\n",
      "Epoch: 10, Batch: 110, Avg. Loss: 0.030382496127947668\n",
      "Epoch: 10, Batch: 120, Avg. Loss: 0.03036856359827132\n",
      "Epoch: 10, Batch: 130, Avg. Loss: 0.03035151537291099\n",
      "Epoch: 10, Batch: 140, Avg. Loss: 0.030335006904078126\n",
      "Epoch: 10, Batch: 150, Avg. Loss: 0.030319147499723535\n",
      "Epoch: 10, Batch: 160, Avg. Loss: 0.0303023861839629\n",
      "Epoch: 10, Batch: 170, Avg. Loss: 0.030286864220776148\n",
      "Epoch: 10, Batch: 180, Avg. Loss: 0.030270650347076523\n",
      "Epoch: 10, Batch: 190, Avg. Loss: 0.030253986171782456\n",
      "Epoch: 10, Batch: 200, Avg. Loss: 0.030236670548535186\n",
      "Epoch: 10, Batch: 210, Avg. Loss: 0.030221374339131135\n",
      "Epoch: 10, Batch: 220, Avg. Loss: 0.030206503884214107\n",
      "Epoch: 10, Batch: 230, Avg. Loss: 0.03019483988723939\n",
      "Epoch: 10, Batch: 240, Avg. Loss: 0.030178986439348503\n",
      "Epoch: 10, Batch: 250, Avg. Loss: 0.030163162255963154\n",
      "Epoch: 10, Batch: 260, Avg. Loss: 0.030147461250189977\n",
      "Epoch: 10, Batch: 270, Avg. Loss: 0.03013105873767335\n",
      "Epoch: 10, Batch: 280, Avg. Loss: 0.030114955721270188\n",
      "Epoch: 10, Batch: 290, Avg. Loss: 0.030098260095168358\n",
      "Epoch: 10, Batch: 300, Avg. Loss: 0.03008049545088438\n",
      "Epoch: 10, Batch: 310, Avg. Loss: 0.03006324025436142\n",
      "Epoch: 10, Batch: 320, Avg. Loss: 0.03004692531560343\n",
      "Epoch: 10, Batch: 330, Avg. Loss: 0.030029393858703863\n",
      "Epoch: 10, Batch: 340, Avg. Loss: 0.03001217698531281\n",
      "Epoch: 10, Batch: 350, Avg. Loss: 0.029993832389013226\n",
      "Epoch: 10, Batch: 360, Avg. Loss: 0.02997669650880561\n",
      "Epoch: 10, Batch: 370, Avg. Loss: 0.029960680375657585\n",
      "Epoch: 10, Batch: 380, Avg. Loss: 0.029944210832733284\n",
      "Epoch: 10, Batch: 390, Avg. Loss: 0.029926742944345556\n",
      "Epoch: 10, Batch: 400, Avg. Loss: 0.029908852600959972\n",
      "Epoch: 10, Batch: 410, Avg. Loss: 0.02989174635685774\n",
      "Epoch: 10, Batch: 420, Avg. Loss: 0.029875032173555093\n",
      "Epoch: 10, Batch: 430, Avg. Loss: 0.02985745667777963\n",
      "Epoch: 10, Batch: 440, Avg. Loss: 0.029840467310943485\n",
      "Epoch: 10, Batch: 450, Avg. Loss: 0.029826311161950322\n",
      "Epoch: 10, Batch: 460, Avg. Loss: 0.02980969217301952\n",
      "Epoch: 10, Batch: 470, Avg. Loss: 0.029795239052822862\n",
      "Epoch: 10, Batch: 480, Avg. Loss: 0.029778827070695894\n",
      "Epoch: 10, Batch: 490, Avg. Loss: 0.029761057876345635\n",
      "Epoch: 10, Batch: 500, Avg. Loss: 0.029744088343043887\n",
      "Epoch: 10, Batch: 510, Avg. Loss: 0.02972791906414247\n",
      "Epoch: 10, Batch: 520, Avg. Loss: 0.029710865731279436\n",
      "Epoch: 10, Batch: 530, Avg. Loss: 0.02969345066711744\n",
      "Epoch: 10, Batch: 540, Avg. Loss: 0.029676061176124507\n",
      "Epoch: 10, Batch: 550, Avg. Loss: 0.029658855537111842\n",
      "Epoch: 10, Batch: 560, Avg. Loss: 0.02964123561747609\n",
      "Epoch: 10, Batch: 570, Avg. Loss: 0.029624150273036554\n",
      "Epoch: 10, Batch: 580, Avg. Loss: 0.029606923307023154\n",
      "Epoch: 10, Batch: 590, Avg. Loss: 0.02959057703522655\n",
      "Epoch: 10, Batch: 600, Avg. Loss: 0.029573357371632684\n",
      "Epoch: 10, Batch: 610, Avg. Loss: 0.029556422728142413\n",
      "Epoch: 10, Batch: 620, Avg. Loss: 0.029539960850795452\n",
      "Epoch: 10, Batch: 630, Avg. Loss: 0.029524136642868494\n",
      "Epoch: 10, Batch: 640, Avg. Loss: 0.029506963456758805\n",
      "Epoch: 10, Batch: 650, Avg. Loss: 0.029490821709168707\n",
      "Epoch: 10, Batch: 660, Avg. Loss: 0.02947626021241965\n",
      "Epoch: 10, Batch: 670, Avg. Loss: 0.02945903873143743\n",
      "Epoch: 10, Batch: 680, Avg. Loss: 0.029442006968371303\n",
      "Epoch: 10, Batch: 690, Avg. Loss: 0.029425944128829866\n",
      "Epoch: 10, Batch: 700, Avg. Loss: 0.029408893798402856\n",
      "Epoch: 10, Batch: 710, Avg. Loss: 0.02939273069416098\n",
      "Epoch: 10, Batch: 720, Avg. Loss: 0.029375393106797446\n",
      "Epoch: 10, Batch: 730, Avg. Loss: 0.0293585209761364\n",
      "Epoch: 10, Batch: 740, Avg. Loss: 0.029341543002075563\n",
      "Epoch: 10, Batch: 750, Avg. Loss: 0.02932441263310767\n",
      "Epoch: 10, Batch: 760, Avg. Loss: 0.029308697464321687\n",
      "Epoch: 10, Batch: 770, Avg. Loss: 0.029291339510758845\n",
      "Epoch: 10, Batch: 780, Avg. Loss: 0.029277676133005498\n",
      "Epoch: 10, Batch: 790, Avg. Loss: 0.029261417404960872\n",
      "Epoch: 10, Batch: 800, Avg. Loss: 0.029245511420201254\n",
      "Epoch: 10, Batch: 810, Avg. Loss: 0.02922974012390773\n",
      "Epoch: 10, Batch: 820, Avg. Loss: 0.029212734901114763\n",
      "Epoch: 10, Batch: 830, Avg. Loss: 0.029196138093971865\n",
      "Epoch: 10, Batch: 840, Avg. Loss: 0.029180758772213922\n",
      "Epoch: 10, Batch: 850, Avg. Loss: 0.02916413950494394\n",
      "Epoch: 10, Batch: 860, Avg. Loss: 0.029147434746568775\n",
      "Epoch: 10, Batch: 870, Avg. Loss: 0.029132258406392133\n",
      "Epoch: 10, Batch: 880, Avg. Loss: 0.029115983364624863\n",
      "Epoch: 10, Batch: 890, Avg. Loss: 0.02909968104706057\n",
      "Epoch: 10, Batch: 900, Avg. Loss: 0.029082702358361297\n",
      "Epoch: 10, Batch: 910, Avg. Loss: 0.029065997301504228\n",
      "Epoch: 10, Batch: 920, Avg. Loss: 0.029049197180960495\n",
      "Epoch: 10, Batch: 930, Avg. Loss: 0.02903224057917121\n",
      "Epoch: 10, Batch: 940, Avg. Loss: 0.02901565830502971\n",
      "Epoch: 10, Batch: 950, Avg. Loss: 0.028999439672916707\n",
      "Epoch: 10, Batch: 960, Avg. Loss: 0.028981934383411026\n",
      "Epoch: 10, Batch: 970, Avg. Loss: 0.028964678976251827\n",
      "Epoch: 10, Batch: 980, Avg. Loss: 0.02894859550751521\n",
      "Epoch: 10, Batch: 990, Avg. Loss: 0.02893305265694816\n",
      "Epoch: 10, Batch: 1000, Avg. Loss: 0.028918420306260018\n",
      "Epoch: 10, Batch: 1010, Avg. Loss: 0.028902342776903383\n",
      "Epoch: 10, Batch: 1020, Avg. Loss: 0.02888607440597952\n",
      "Epoch: 10, Batch: 1030, Avg. Loss: 0.028872745907885956\n",
      "Epoch: 10, Batch: 1040, Avg. Loss: 0.028858583143827225\n",
      "Epoch: 10, Batch: 1050, Avg. Loss: 0.028843814786597883\n",
      "Epoch: 10, Batch: 1060, Avg. Loss: 0.028828557837446332\n",
      "Epoch: 10, Batch: 1070, Avg. Loss: 0.02881201765078721\n",
      "Epoch: 10, Batch: 1080, Avg. Loss: 0.028799443963870866\n",
      "Epoch: 10, Batch: 1090, Avg. Loss: 0.028786999209363517\n",
      "Epoch: 10, Batch: 1100, Avg. Loss: 0.028773124737472736\n",
      "Epoch: 10, Batch: 1110, Avg. Loss: 0.028762307892243343\n",
      "Epoch: 10, Batch: 1120, Avg. Loss: 0.02875127785076226\n",
      "Epoch: 10, Batch: 1130, Avg. Loss: 0.02874897965114236\n",
      "Epoch: 10, Batch: 1140, Avg. Loss: 0.028753576375977884\n",
      "Epoch: 10, Batch: 1150, Avg. Loss: 0.028741123356496114\n",
      "Epoch: 10, Batch: 1160, Avg. Loss: 0.02873554280386879\n",
      "Epoch: 10, Batch: 1170, Avg. Loss: 0.028729973901801306\n",
      "Epoch: 10, Batch: 1180, Avg. Loss: 0.02871999060206251\n",
      "Epoch: 10, Batch: 1190, Avg. Loss: 0.02870713782239323\n",
      "Epoch: 10, Batch: 1200, Avg. Loss: 0.028693031173261333\n",
      "Epoch: 10, Batch: 1210, Avg. Loss: 0.028678532272473994\n",
      "Epoch: 10, Batch: 1220, Avg. Loss: 0.028664303010267123\n",
      "Epoch: 10, Batch: 1230, Avg. Loss: 0.028648845726801302\n",
      "Epoch: 10, Batch: 1240, Avg. Loss: 0.02863313358661853\n",
      "Epoch: 10, Batch: 1250, Avg. Loss: 0.028619061621452994\n",
      "Epoch: 10, Batch: 1260, Avg. Loss: 0.02860630942010255\n",
      "Epoch: 10, Batch: 1270, Avg. Loss: 0.028593831684918195\n",
      "Epoch: 10, Batch: 1280, Avg. Loss: 0.02858195756847671\n",
      "Epoch: 10, Batch: 1290, Avg. Loss: 0.028568490418688863\n",
      "Epoch: 10, Batch: 1300, Avg. Loss: 0.028552905300377126\n",
      "Epoch: 10, Batch: 1310, Avg. Loss: 0.02853722220061949\n",
      "Epoch: 10, Batch: 1320, Avg. Loss: 0.02852495142066171\n",
      "Epoch: 10, Batch: 1330, Avg. Loss: 0.028509831891883802\n",
      "Epoch: 10, Batch: 1340, Avg. Loss: 0.028494146194007964\n",
      "Epoch: 10, Batch: 1350, Avg. Loss: 0.028479701388559302\n",
      "Epoch: 10, Batch: 1360, Avg. Loss: 0.028464850836830303\n",
      "Epoch: 10, Batch: 1370, Avg. Loss: 0.028449957504145276\n",
      "Epoch: 10, Batch: 1380, Avg. Loss: 0.028435729108344247\n",
      "Epoch: 10, Batch: 1390, Avg. Loss: 0.028421384041460555\n",
      "Epoch: 10, Batch: 1400, Avg. Loss: 0.028407020867913548\n",
      "Epoch: 10, Batch: 1410, Avg. Loss: 0.028393194354948165\n",
      "Epoch: 10, Batch: 1420, Avg. Loss: 0.02837798895747919\n",
      "Epoch: 10, Batch: 1430, Avg. Loss: 0.028362545375182836\n",
      "Epoch: 10, Batch: 1440, Avg. Loss: 0.028347911813040555\n",
      "Epoch: 10, Batch: 1450, Avg. Loss: 0.028332286330741238\n",
      "Epoch: 10, Batch: 1460, Avg. Loss: 0.02831643239283053\n",
      "Epoch: 10, Batch: 1470, Avg. Loss: 0.028301225218480833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Batch: 1480, Avg. Loss: 0.028285415721818766\n",
      "Epoch: 10, Batch: 1490, Avg. Loss: 0.028270154520038105\n",
      "Epoch: 10, Batch: 1500, Avg. Loss: 0.02825645059006925\n",
      "Epoch: 10, Batch: 1510, Avg. Loss: 0.028241501186294274\n",
      "Epoch: 10, Batch: 1520, Avg. Loss: 0.028225692727397826\n",
      "Epoch: 10, Batch: 1530, Avg. Loss: 0.028210071777238947\n",
      "Epoch: 10, Batch: 1540, Avg. Loss: 0.028194107840050702\n",
      "Epoch: 11, Batch: 10, Avg. Loss: 0.028170830205192274\n",
      "Epoch: 11, Batch: 20, Avg. Loss: 0.0281553426540685\n",
      "Epoch: 11, Batch: 30, Avg. Loss: 0.028139717052862577\n",
      "Epoch: 11, Batch: 40, Avg. Loss: 0.02812388804520789\n",
      "Epoch: 11, Batch: 50, Avg. Loss: 0.028107451645038854\n",
      "Epoch: 11, Batch: 60, Avg. Loss: 0.02809146323603472\n",
      "Epoch: 11, Batch: 70, Avg. Loss: 0.028074945939162516\n",
      "Epoch: 11, Batch: 80, Avg. Loss: 0.028059101957091045\n",
      "Epoch: 11, Batch: 90, Avg. Loss: 0.02804247361741425\n",
      "Epoch: 11, Batch: 100, Avg. Loss: 0.028025656405636076\n",
      "Epoch: 11, Batch: 110, Avg. Loss: 0.02800912158387379\n",
      "Epoch: 11, Batch: 120, Avg. Loss: 0.027992775897473912\n",
      "Epoch: 11, Batch: 130, Avg. Loss: 0.0279768220165406\n",
      "Epoch: 11, Batch: 140, Avg. Loss: 0.027960563883061965\n",
      "Epoch: 11, Batch: 150, Avg. Loss: 0.027944006014986075\n",
      "Epoch: 11, Batch: 160, Avg. Loss: 0.027927432756330417\n",
      "Epoch: 11, Batch: 170, Avg. Loss: 0.02791100969994346\n",
      "Epoch: 11, Batch: 180, Avg. Loss: 0.027894315684684885\n",
      "Epoch: 11, Batch: 190, Avg. Loss: 0.027878358879818106\n",
      "Epoch: 11, Batch: 200, Avg. Loss: 0.027861771625628938\n",
      "Epoch: 11, Batch: 210, Avg. Loss: 0.027845085355064043\n",
      "Epoch: 11, Batch: 220, Avg. Loss: 0.027828955539130645\n",
      "Epoch: 11, Batch: 230, Avg. Loss: 0.02781312409750193\n",
      "Epoch: 11, Batch: 240, Avg. Loss: 0.027796845151812705\n",
      "Epoch: 11, Batch: 250, Avg. Loss: 0.027780386667982496\n",
      "Epoch: 11, Batch: 260, Avg. Loss: 0.027764013022230898\n",
      "Epoch: 11, Batch: 270, Avg. Loss: 0.027747947427495926\n",
      "Epoch: 11, Batch: 280, Avg. Loss: 0.02773164748002827\n",
      "Epoch: 11, Batch: 290, Avg. Loss: 0.027715596236422325\n",
      "Epoch: 11, Batch: 300, Avg. Loss: 0.02769918776290252\n",
      "Epoch: 11, Batch: 310, Avg. Loss: 0.027682606019123906\n",
      "Epoch: 11, Batch: 320, Avg. Loss: 0.02766603404884936\n",
      "Epoch: 11, Batch: 330, Avg. Loss: 0.027649616055923\n",
      "Epoch: 11, Batch: 340, Avg. Loss: 0.02763339452767342\n",
      "Epoch: 11, Batch: 350, Avg. Loss: 0.02761688894454617\n",
      "Epoch: 11, Batch: 360, Avg. Loss: 0.02760061285515149\n",
      "Epoch: 11, Batch: 370, Avg. Loss: 0.02758412545637343\n",
      "Epoch: 11, Batch: 380, Avg. Loss: 0.02756791049673899\n",
      "Epoch: 11, Batch: 390, Avg. Loss: 0.027551864077923694\n",
      "Epoch: 11, Batch: 400, Avg. Loss: 0.027535815528865547\n",
      "Epoch: 11, Batch: 410, Avg. Loss: 0.027519826527475016\n",
      "Epoch: 11, Batch: 420, Avg. Loss: 0.027503847558168957\n",
      "Epoch: 11, Batch: 430, Avg. Loss: 0.027487651511526264\n",
      "Epoch: 11, Batch: 440, Avg. Loss: 0.027471270039846396\n",
      "Epoch: 11, Batch: 450, Avg. Loss: 0.027455284394181732\n",
      "Epoch: 11, Batch: 460, Avg. Loss: 0.027438956585808635\n",
      "Epoch: 11, Batch: 470, Avg. Loss: 0.027423238482396386\n",
      "Epoch: 11, Batch: 480, Avg. Loss: 0.02740690482568885\n",
      "Epoch: 11, Batch: 490, Avg. Loss: 0.0273904800794179\n",
      "Epoch: 11, Batch: 500, Avg. Loss: 0.027374483270365613\n",
      "Epoch: 11, Batch: 510, Avg. Loss: 0.02735852985180477\n",
      "Epoch: 11, Batch: 520, Avg. Loss: 0.027342774107687554\n",
      "Epoch: 11, Batch: 530, Avg. Loss: 0.027326821587115382\n",
      "Epoch: 11, Batch: 540, Avg. Loss: 0.02731084753777942\n",
      "Epoch: 11, Batch: 550, Avg. Loss: 0.027295028676774146\n",
      "Epoch: 11, Batch: 560, Avg. Loss: 0.027278931937258437\n",
      "Epoch: 11, Batch: 570, Avg. Loss: 0.02726297884491929\n",
      "Epoch: 11, Batch: 580, Avg. Loss: 0.02724742196229953\n",
      "Epoch: 11, Batch: 590, Avg. Loss: 0.02723186740812825\n",
      "Epoch: 11, Batch: 600, Avg. Loss: 0.027216307360718592\n",
      "Epoch: 11, Batch: 610, Avg. Loss: 0.027200650530206344\n",
      "Epoch: 11, Batch: 620, Avg. Loss: 0.02718498469953137\n",
      "Epoch: 11, Batch: 630, Avg. Loss: 0.027169378908799374\n",
      "Epoch: 11, Batch: 640, Avg. Loss: 0.027153888424131266\n",
      "Epoch: 11, Batch: 650, Avg. Loss: 0.027138062598816105\n",
      "Epoch: 11, Batch: 660, Avg. Loss: 0.027123045557849373\n",
      "Epoch: 11, Batch: 670, Avg. Loss: 0.027107578256009245\n",
      "Epoch: 11, Batch: 680, Avg. Loss: 0.02709160742974673\n",
      "Epoch: 11, Batch: 690, Avg. Loss: 0.027075823588312616\n",
      "Epoch: 11, Batch: 700, Avg. Loss: 0.027060454237441953\n",
      "Epoch: 11, Batch: 710, Avg. Loss: 0.027045107319062747\n",
      "Epoch: 11, Batch: 720, Avg. Loss: 0.0270294131751956\n",
      "Epoch: 11, Batch: 730, Avg. Loss: 0.02701368285458503\n",
      "Epoch: 11, Batch: 740, Avg. Loss: 0.0269982859298922\n",
      "Epoch: 11, Batch: 750, Avg. Loss: 0.026982514722025126\n",
      "Epoch: 11, Batch: 760, Avg. Loss: 0.02696683254723013\n",
      "Epoch: 11, Batch: 770, Avg. Loss: 0.02695171170277166\n",
      "Epoch: 11, Batch: 780, Avg. Loss: 0.02693782374521427\n",
      "Epoch: 11, Batch: 790, Avg. Loss: 0.026922377000098918\n",
      "Epoch: 11, Batch: 800, Avg. Loss: 0.02690695936003556\n",
      "Epoch: 11, Batch: 810, Avg. Loss: 0.026891664024342184\n",
      "Epoch: 11, Batch: 820, Avg. Loss: 0.026876128421763994\n",
      "Epoch: 11, Batch: 830, Avg. Loss: 0.02686091658678869\n",
      "Epoch: 11, Batch: 840, Avg. Loss: 0.026846332449696163\n",
      "Epoch: 11, Batch: 850, Avg. Loss: 0.026831072520323282\n",
      "Epoch: 11, Batch: 860, Avg. Loss: 0.026816223709158375\n",
      "Epoch: 11, Batch: 870, Avg. Loss: 0.026801324862855923\n",
      "Epoch: 11, Batch: 880, Avg. Loss: 0.02678589515291974\n",
      "Epoch: 11, Batch: 890, Avg. Loss: 0.026770467704854313\n",
      "Epoch: 11, Batch: 900, Avg. Loss: 0.02675505798370551\n",
      "Epoch: 11, Batch: 910, Avg. Loss: 0.026739958708814786\n",
      "Epoch: 11, Batch: 920, Avg. Loss: 0.026725025664158672\n",
      "Epoch: 11, Batch: 930, Avg. Loss: 0.0267099081450626\n",
      "Epoch: 11, Batch: 940, Avg. Loss: 0.026694989048038294\n",
      "Epoch: 11, Batch: 950, Avg. Loss: 0.0266794692445203\n",
      "Epoch: 11, Batch: 960, Avg. Loss: 0.026664011215444563\n",
      "Epoch: 11, Batch: 970, Avg. Loss: 0.0266486381908802\n",
      "Epoch: 11, Batch: 980, Avg. Loss: 0.026633934484263766\n",
      "Epoch: 11, Batch: 990, Avg. Loss: 0.02662028866395559\n",
      "Epoch: 11, Batch: 1000, Avg. Loss: 0.026606560829495807\n",
      "Epoch: 11, Batch: 1010, Avg. Loss: 0.026592136866500807\n",
      "Epoch: 11, Batch: 1020, Avg. Loss: 0.026578019551869274\n",
      "Epoch: 11, Batch: 1030, Avg. Loss: 0.026566145687305625\n",
      "Epoch: 11, Batch: 1040, Avg. Loss: 0.026553968044918014\n",
      "Epoch: 11, Batch: 1050, Avg. Loss: 0.02654030704476881\n",
      "Epoch: 11, Batch: 1060, Avg. Loss: 0.026526432340719133\n",
      "Epoch: 11, Batch: 1070, Avg. Loss: 0.02651252497270932\n",
      "Epoch: 11, Batch: 1080, Avg. Loss: 0.0264996101922864\n",
      "Epoch: 11, Batch: 1090, Avg. Loss: 0.026488010322806614\n",
      "Epoch: 11, Batch: 1100, Avg. Loss: 0.02647504351798543\n",
      "Epoch: 11, Batch: 1110, Avg. Loss: 0.02646378241002852\n",
      "Epoch: 11, Batch: 1120, Avg. Loss: 0.026450482102898933\n",
      "Epoch: 11, Batch: 1130, Avg. Loss: 0.026440492865211287\n",
      "Epoch: 11, Batch: 1140, Avg. Loss: 0.02643618064313516\n",
      "Epoch: 11, Batch: 1150, Avg. Loss: 0.02642460760750965\n",
      "Epoch: 11, Batch: 1160, Avg. Loss: 0.02641330195585747\n",
      "Epoch: 11, Batch: 1170, Avg. Loss: 0.026401850877519947\n",
      "Epoch: 11, Batch: 1180, Avg. Loss: 0.02638981423808463\n",
      "Epoch: 11, Batch: 1190, Avg. Loss: 0.02637916164201306\n",
      "Epoch: 11, Batch: 1200, Avg. Loss: 0.026366341381307813\n",
      "Epoch: 11, Batch: 1210, Avg. Loss: 0.02635307094462044\n",
      "Epoch: 11, Batch: 1220, Avg. Loss: 0.026339793581836773\n",
      "Epoch: 11, Batch: 1230, Avg. Loss: 0.026326789684158667\n",
      "Epoch: 11, Batch: 1240, Avg. Loss: 0.026313173063463778\n",
      "Epoch: 11, Batch: 1250, Avg. Loss: 0.026301293686736806\n",
      "Epoch: 11, Batch: 1260, Avg. Loss: 0.026289887840088462\n",
      "Epoch: 11, Batch: 1270, Avg. Loss: 0.02627834452167377\n",
      "Epoch: 11, Batch: 1280, Avg. Loss: 0.026266776889384662\n",
      "Epoch: 11, Batch: 1290, Avg. Loss: 0.026254556354236572\n",
      "Epoch: 11, Batch: 1300, Avg. Loss: 0.02624206321430476\n",
      "Epoch: 11, Batch: 1310, Avg. Loss: 0.02622970530011078\n",
      "Epoch: 11, Batch: 1320, Avg. Loss: 0.02621883641571168\n",
      "Epoch: 11, Batch: 1330, Avg. Loss: 0.02620591223424864\n",
      "Epoch: 11, Batch: 1340, Avg. Loss: 0.026192475478420633\n",
      "Epoch: 11, Batch: 1350, Avg. Loss: 0.02618024509559498\n",
      "Epoch: 11, Batch: 1360, Avg. Loss: 0.026167271459349724\n",
      "Epoch: 11, Batch: 1370, Avg. Loss: 0.0261542771892495\n",
      "Epoch: 11, Batch: 1380, Avg. Loss: 0.02614133363946122\n",
      "Epoch: 11, Batch: 1390, Avg. Loss: 0.0261281324213303\n",
      "Epoch: 11, Batch: 1400, Avg. Loss: 0.02611563687955568\n",
      "Epoch: 11, Batch: 1410, Avg. Loss: 0.026102473775803017\n",
      "Epoch: 11, Batch: 1420, Avg. Loss: 0.026089147186494382\n",
      "Epoch: 11, Batch: 1430, Avg. Loss: 0.026076214072072015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Batch: 1440, Avg. Loss: 0.02606330517654288\n",
      "Epoch: 11, Batch: 1450, Avg. Loss: 0.026049685560650244\n",
      "Epoch: 11, Batch: 1460, Avg. Loss: 0.0260360953404706\n",
      "Epoch: 11, Batch: 1470, Avg. Loss: 0.02602336500361239\n",
      "Epoch: 11, Batch: 1480, Avg. Loss: 0.02601035206654484\n",
      "Epoch: 11, Batch: 1490, Avg. Loss: 0.02599737939656153\n",
      "Epoch: 11, Batch: 1500, Avg. Loss: 0.02598544820303572\n",
      "Epoch: 11, Batch: 1510, Avg. Loss: 0.0259725445872804\n",
      "Epoch: 11, Batch: 1520, Avg. Loss: 0.02595901125592261\n",
      "Epoch: 11, Batch: 1530, Avg. Loss: 0.025944994323666298\n",
      "Epoch: 11, Batch: 1540, Avg. Loss: 0.025930788998852736\n",
      "Epoch: 12, Batch: 10, Avg. Loss: 0.025908996399451042\n",
      "Epoch: 12, Batch: 20, Avg. Loss: 0.025895196099052064\n",
      "Epoch: 12, Batch: 30, Avg. Loss: 0.025881187988613004\n",
      "Epoch: 12, Batch: 40, Avg. Loss: 0.025867244158290933\n",
      "Epoch: 12, Batch: 50, Avg. Loss: 0.025852977036765652\n",
      "Epoch: 12, Batch: 60, Avg. Loss: 0.02583897441987564\n",
      "Epoch: 12, Batch: 70, Avg. Loss: 0.02582487582922797\n",
      "Epoch: 12, Batch: 80, Avg. Loss: 0.0258111259308213\n",
      "Epoch: 12, Batch: 90, Avg. Loss: 0.025797258880857377\n",
      "Epoch: 12, Batch: 100, Avg. Loss: 0.02578298094058224\n",
      "Epoch: 12, Batch: 110, Avg. Loss: 0.02576889150485934\n",
      "Epoch: 12, Batch: 120, Avg. Loss: 0.025754678730234286\n",
      "Epoch: 12, Batch: 130, Avg. Loss: 0.025740577698937152\n",
      "Epoch: 12, Batch: 140, Avg. Loss: 0.025726529858270637\n",
      "Epoch: 12, Batch: 150, Avg. Loss: 0.025712608464670383\n",
      "Epoch: 12, Batch: 160, Avg. Loss: 0.02569863623168326\n",
      "Epoch: 12, Batch: 170, Avg. Loss: 0.025684439506869102\n",
      "Epoch: 12, Batch: 180, Avg. Loss: 0.025670075887056262\n",
      "Epoch: 12, Batch: 190, Avg. Loss: 0.025655978723382024\n",
      "Epoch: 12, Batch: 200, Avg. Loss: 0.025641753852558347\n",
      "Epoch: 12, Batch: 210, Avg. Loss: 0.025627615151961634\n",
      "Epoch: 12, Batch: 220, Avg. Loss: 0.02561355276323898\n",
      "Epoch: 12, Batch: 230, Avg. Loss: 0.025599762884442552\n",
      "Epoch: 12, Batch: 240, Avg. Loss: 0.02558578783433242\n",
      "Epoch: 12, Batch: 250, Avg. Loss: 0.025571704967405472\n",
      "Epoch: 12, Batch: 260, Avg. Loss: 0.02555785819235171\n",
      "Epoch: 12, Batch: 270, Avg. Loss: 0.025543821234801548\n",
      "Epoch: 12, Batch: 280, Avg. Loss: 0.02552992694257109\n",
      "Epoch: 12, Batch: 290, Avg. Loss: 0.025515944098840834\n",
      "Epoch: 12, Batch: 300, Avg. Loss: 0.025501852060587915\n",
      "Epoch: 12, Batch: 310, Avg. Loss: 0.025487697055107686\n",
      "Epoch: 12, Batch: 320, Avg. Loss: 0.025473581435524053\n",
      "Epoch: 12, Batch: 330, Avg. Loss: 0.025459462114774175\n",
      "Epoch: 12, Batch: 340, Avg. Loss: 0.025445491459763495\n",
      "Epoch: 12, Batch: 350, Avg. Loss: 0.025431568389337883\n",
      "Epoch: 12, Batch: 360, Avg. Loss: 0.02541758576553506\n",
      "Epoch: 12, Batch: 370, Avg. Loss: 0.025403651398492696\n",
      "Epoch: 12, Batch: 380, Avg. Loss: 0.025389893766067255\n",
      "Epoch: 12, Batch: 390, Avg. Loss: 0.025376111247066006\n",
      "Epoch: 12, Batch: 400, Avg. Loss: 0.025362499016402647\n",
      "Epoch: 12, Batch: 410, Avg. Loss: 0.025348832360696755\n",
      "Epoch: 12, Batch: 420, Avg. Loss: 0.025335390049195213\n",
      "Epoch: 12, Batch: 430, Avg. Loss: 0.02532175077290822\n",
      "Epoch: 12, Batch: 440, Avg. Loss: 0.02530796782955579\n",
      "Epoch: 12, Batch: 450, Avg. Loss: 0.025294286054573723\n",
      "Epoch: 12, Batch: 460, Avg. Loss: 0.025280546305364922\n",
      "Epoch: 12, Batch: 470, Avg. Loss: 0.025266981912556656\n",
      "Epoch: 12, Batch: 480, Avg. Loss: 0.025253141190023313\n",
      "Epoch: 12, Batch: 490, Avg. Loss: 0.02523926993250315\n",
      "Epoch: 12, Batch: 500, Avg. Loss: 0.02522567617365955\n",
      "Epoch: 12, Batch: 510, Avg. Loss: 0.025212034709299173\n",
      "Epoch: 12, Batch: 520, Avg. Loss: 0.02519846865292685\n",
      "Epoch: 12, Batch: 530, Avg. Loss: 0.025184898199081023\n",
      "Epoch: 12, Batch: 540, Avg. Loss: 0.025171425681834007\n",
      "Epoch: 12, Batch: 550, Avg. Loss: 0.025157590203856047\n",
      "Epoch: 12, Batch: 560, Avg. Loss: 0.025143770546198513\n",
      "Epoch: 12, Batch: 570, Avg. Loss: 0.025130095484724155\n",
      "Epoch: 12, Batch: 580, Avg. Loss: 0.025116758819778453\n",
      "Epoch: 12, Batch: 590, Avg. Loss: 0.025103593542958835\n",
      "Epoch: 12, Batch: 600, Avg. Loss: 0.02509017850912989\n",
      "Epoch: 12, Batch: 610, Avg. Loss: 0.02507664830619316\n",
      "Epoch: 12, Batch: 620, Avg. Loss: 0.0250631737603479\n",
      "Epoch: 12, Batch: 630, Avg. Loss: 0.025050053383410993\n",
      "Epoch: 12, Batch: 640, Avg. Loss: 0.02503695703901755\n",
      "Epoch: 12, Batch: 650, Avg. Loss: 0.02502343794102311\n",
      "Epoch: 12, Batch: 660, Avg. Loss: 0.02501025046492603\n",
      "Epoch: 12, Batch: 670, Avg. Loss: 0.024996907339338147\n",
      "Epoch: 12, Batch: 680, Avg. Loss: 0.024983391728318528\n",
      "Epoch: 12, Batch: 690, Avg. Loss: 0.024969970791748803\n",
      "Epoch: 12, Batch: 700, Avg. Loss: 0.02495688574059991\n",
      "Epoch: 12, Batch: 710, Avg. Loss: 0.02494368877153052\n",
      "Epoch: 12, Batch: 720, Avg. Loss: 0.024930357989639686\n",
      "Epoch: 12, Batch: 730, Avg. Loss: 0.024916908539249924\n",
      "Epoch: 12, Batch: 740, Avg. Loss: 0.024903562053247577\n",
      "Epoch: 12, Batch: 750, Avg. Loss: 0.02489023442452812\n",
      "Epoch: 12, Batch: 760, Avg. Loss: 0.024876785844399237\n",
      "Epoch: 12, Batch: 770, Avg. Loss: 0.024863632106331586\n",
      "Epoch: 12, Batch: 780, Avg. Loss: 0.024850968554841014\n",
      "Epoch: 12, Batch: 790, Avg. Loss: 0.024837702741108435\n",
      "Epoch: 12, Batch: 800, Avg. Loss: 0.024824572141171175\n",
      "Epoch: 12, Batch: 810, Avg. Loss: 0.024811498073059133\n",
      "Epoch: 12, Batch: 820, Avg. Loss: 0.024798411637367315\n",
      "Epoch: 12, Batch: 830, Avg. Loss: 0.024785528343886212\n",
      "Epoch: 12, Batch: 840, Avg. Loss: 0.02477293787525898\n",
      "Epoch: 12, Batch: 850, Avg. Loss: 0.0247599278257565\n",
      "Epoch: 12, Batch: 860, Avg. Loss: 0.024747313653108026\n",
      "Epoch: 12, Batch: 870, Avg. Loss: 0.024734689316707435\n",
      "Epoch: 12, Batch: 880, Avg. Loss: 0.02472157963055229\n",
      "Epoch: 12, Batch: 890, Avg. Loss: 0.02470844589180722\n",
      "Epoch: 12, Batch: 900, Avg. Loss: 0.024695479545682263\n",
      "Epoch: 12, Batch: 910, Avg. Loss: 0.024682532545144232\n",
      "Epoch: 12, Batch: 920, Avg. Loss: 0.024669661959927112\n",
      "Epoch: 12, Batch: 930, Avg. Loss: 0.02465668852008358\n",
      "Epoch: 12, Batch: 940, Avg. Loss: 0.024643757090162057\n",
      "Epoch: 12, Batch: 950, Avg. Loss: 0.024630788481631298\n",
      "Epoch: 12, Batch: 960, Avg. Loss: 0.024617718896598417\n",
      "Epoch: 12, Batch: 970, Avg. Loss: 0.024604739626440168\n",
      "Epoch: 12, Batch: 980, Avg. Loss: 0.024591852714998138\n",
      "Epoch: 12, Batch: 990, Avg. Loss: 0.024579365113951944\n",
      "Epoch: 12, Batch: 1000, Avg. Loss: 0.024567128102976846\n",
      "Epoch: 12, Batch: 1010, Avg. Loss: 0.024554605364248666\n",
      "Epoch: 12, Batch: 1020, Avg. Loss: 0.024542490191507088\n",
      "Epoch: 12, Batch: 1030, Avg. Loss: 0.024532213288869027\n",
      "Epoch: 12, Batch: 1040, Avg. Loss: 0.024521586651930338\n",
      "Epoch: 12, Batch: 1050, Avg. Loss: 0.024509667121446494\n",
      "Epoch: 12, Batch: 1060, Avg. Loss: 0.024497816269917514\n",
      "Epoch: 12, Batch: 1070, Avg. Loss: 0.02448653553713649\n",
      "Epoch: 12, Batch: 1080, Avg. Loss: 0.024474482769722042\n",
      "Epoch: 12, Batch: 1090, Avg. Loss: 0.02446348474338292\n",
      "Epoch: 12, Batch: 1100, Avg. Loss: 0.024452011033493842\n",
      "Epoch: 12, Batch: 1110, Avg. Loss: 0.0244426424320207\n",
      "Epoch: 12, Batch: 1120, Avg. Loss: 0.024432411050330848\n",
      "Epoch: 12, Batch: 1130, Avg. Loss: 0.024428185428866205\n",
      "Epoch: 12, Batch: 1140, Avg. Loss: 0.024432677303207462\n",
      "Epoch: 12, Batch: 1150, Avg. Loss: 0.02442480550161123\n",
      "Epoch: 12, Batch: 1160, Avg. Loss: 0.0244221470815126\n",
      "Epoch: 12, Batch: 1170, Avg. Loss: 0.02441497253680325\n",
      "Epoch: 12, Batch: 1180, Avg. Loss: 0.024406837497545027\n",
      "Epoch: 12, Batch: 1190, Avg. Loss: 0.024397887755412503\n",
      "Epoch: 12, Batch: 1200, Avg. Loss: 0.02438823487823527\n",
      "Epoch: 12, Batch: 1210, Avg. Loss: 0.02437839517315222\n",
      "Epoch: 12, Batch: 1220, Avg. Loss: 0.024368188276657623\n",
      "Epoch: 12, Batch: 1230, Avg. Loss: 0.024357660108116054\n",
      "Epoch: 12, Batch: 1240, Avg. Loss: 0.024347687322546458\n",
      "Epoch: 12, Batch: 1250, Avg. Loss: 0.024338281217910082\n",
      "Epoch: 12, Batch: 1260, Avg. Loss: 0.024329041560532186\n",
      "Epoch: 12, Batch: 1270, Avg. Loss: 0.02431939754334741\n",
      "Epoch: 12, Batch: 1280, Avg. Loss: 0.024310236888038544\n",
      "Epoch: 12, Batch: 1290, Avg. Loss: 0.02430065514497985\n",
      "Epoch: 12, Batch: 1300, Avg. Loss: 0.024290696186622402\n",
      "Epoch: 12, Batch: 1310, Avg. Loss: 0.024280213879947273\n",
      "Epoch: 12, Batch: 1320, Avg. Loss: 0.024270128888433155\n",
      "Epoch: 12, Batch: 1330, Avg. Loss: 0.02425964400884313\n",
      "Epoch: 12, Batch: 1340, Avg. Loss: 0.02424824712884621\n",
      "Epoch: 12, Batch: 1350, Avg. Loss: 0.02423758913451898\n",
      "Epoch: 12, Batch: 1360, Avg. Loss: 0.02422765530618273\n",
      "Epoch: 12, Batch: 1370, Avg. Loss: 0.02421632124693608\n",
      "Epoch: 12, Batch: 1380, Avg. Loss: 0.02420508390416824\n",
      "Epoch: 12, Batch: 1390, Avg. Loss: 0.024194485723588677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Batch: 1400, Avg. Loss: 0.024183554674820182\n",
      "Epoch: 12, Batch: 1410, Avg. Loss: 0.02417260166566438\n",
      "Epoch: 12, Batch: 1420, Avg. Loss: 0.024161744121347743\n",
      "Epoch: 12, Batch: 1430, Avg. Loss: 0.02415061331912509\n",
      "Epoch: 12, Batch: 1440, Avg. Loss: 0.024139573523573658\n",
      "Epoch: 12, Batch: 1450, Avg. Loss: 0.024127997928067503\n",
      "Epoch: 12, Batch: 1460, Avg. Loss: 0.024116587175661962\n",
      "Epoch: 12, Batch: 1470, Avg. Loss: 0.024105687968754896\n",
      "Epoch: 12, Batch: 1480, Avg. Loss: 0.02409431413745292\n",
      "Epoch: 12, Batch: 1490, Avg. Loss: 0.02408358594963189\n",
      "Epoch: 12, Batch: 1500, Avg. Loss: 0.024073069370187536\n",
      "Epoch: 12, Batch: 1510, Avg. Loss: 0.024062305514700334\n",
      "Epoch: 12, Batch: 1520, Avg. Loss: 0.02405072104916361\n",
      "Epoch: 12, Batch: 1530, Avg. Loss: 0.02403887881287081\n",
      "Epoch: 12, Batch: 1540, Avg. Loss: 0.02402692602053257\n",
      "Epoch: 13, Batch: 10, Avg. Loss: 0.024008830985983814\n",
      "Epoch: 13, Batch: 20, Avg. Loss: 0.023997149257748797\n",
      "Epoch: 13, Batch: 30, Avg. Loss: 0.02398567123439614\n",
      "Epoch: 13, Batch: 40, Avg. Loss: 0.023974018656624976\n",
      "Epoch: 13, Batch: 50, Avg. Loss: 0.02396186380025148\n",
      "Epoch: 13, Batch: 60, Avg. Loss: 0.023949990245173535\n",
      "Epoch: 13, Batch: 70, Avg. Loss: 0.023937983811075442\n",
      "Epoch: 13, Batch: 80, Avg. Loss: 0.023926505689213486\n",
      "Epoch: 13, Batch: 90, Avg. Loss: 0.02391481221990745\n",
      "Epoch: 13, Batch: 100, Avg. Loss: 0.023902877983214648\n",
      "Epoch: 13, Batch: 110, Avg. Loss: 0.02389097643456366\n",
      "Epoch: 13, Batch: 120, Avg. Loss: 0.023878954719674272\n",
      "Epoch: 13, Batch: 130, Avg. Loss: 0.023866969900542483\n",
      "Epoch: 13, Batch: 140, Avg. Loss: 0.02385497867340774\n",
      "Epoch: 13, Batch: 150, Avg. Loss: 0.023843002860165753\n",
      "Epoch: 13, Batch: 160, Avg. Loss: 0.023831056083531536\n",
      "Epoch: 13, Batch: 170, Avg. Loss: 0.02381915256648266\n",
      "Epoch: 13, Batch: 180, Avg. Loss: 0.02380706720994509\n",
      "Epoch: 13, Batch: 190, Avg. Loss: 0.023795065810449528\n",
      "Epoch: 13, Batch: 200, Avg. Loss: 0.023783082743463028\n",
      "Epoch: 13, Batch: 210, Avg. Loss: 0.02377098909598317\n",
      "Epoch: 13, Batch: 220, Avg. Loss: 0.023759227633014912\n",
      "Epoch: 13, Batch: 230, Avg. Loss: 0.023747271071823877\n",
      "Epoch: 13, Batch: 240, Avg. Loss: 0.023735294178239805\n",
      "Epoch: 13, Batch: 250, Avg. Loss: 0.023723265974803263\n",
      "Epoch: 13, Batch: 260, Avg. Loss: 0.023711334894997632\n",
      "Epoch: 13, Batch: 270, Avg. Loss: 0.023699384880920625\n",
      "Epoch: 13, Batch: 280, Avg. Loss: 0.0236875723008018\n",
      "Epoch: 13, Batch: 290, Avg. Loss: 0.023675791840451342\n",
      "Epoch: 13, Batch: 300, Avg. Loss: 0.02366369790841483\n",
      "Epoch: 13, Batch: 310, Avg. Loss: 0.02365168352740808\n",
      "Epoch: 13, Batch: 320, Avg. Loss: 0.02363966660233352\n",
      "Epoch: 13, Batch: 330, Avg. Loss: 0.023627776973197602\n",
      "Epoch: 13, Batch: 340, Avg. Loss: 0.023615786191313173\n",
      "Epoch: 13, Batch: 350, Avg. Loss: 0.02360384760966019\n",
      "Epoch: 13, Batch: 360, Avg. Loss: 0.02359201732089925\n",
      "Epoch: 13, Batch: 370, Avg. Loss: 0.023580110715087706\n",
      "Epoch: 13, Batch: 380, Avg. Loss: 0.023568371084298052\n",
      "Epoch: 13, Batch: 390, Avg. Loss: 0.023556784194671263\n",
      "Epoch: 13, Batch: 400, Avg. Loss: 0.02354518163007848\n",
      "Epoch: 13, Batch: 410, Avg. Loss: 0.023533337769769663\n",
      "Epoch: 13, Batch: 420, Avg. Loss: 0.023521589096441423\n",
      "Epoch: 13, Batch: 430, Avg. Loss: 0.023509858332102326\n",
      "Epoch: 13, Batch: 440, Avg. Loss: 0.02349824420155592\n",
      "Epoch: 13, Batch: 450, Avg. Loss: 0.023486535453554128\n",
      "Epoch: 13, Batch: 460, Avg. Loss: 0.02347476182537919\n",
      "Epoch: 13, Batch: 470, Avg. Loss: 0.023463230062827925\n",
      "Epoch: 13, Batch: 480, Avg. Loss: 0.023451371282023157\n",
      "Epoch: 13, Batch: 490, Avg. Loss: 0.02343958374915481\n",
      "Epoch: 13, Batch: 500, Avg. Loss: 0.023428004483973832\n",
      "Epoch: 13, Batch: 510, Avg. Loss: 0.02341639398544705\n",
      "Epoch: 13, Batch: 520, Avg. Loss: 0.02340468428045919\n",
      "Epoch: 13, Batch: 530, Avg. Loss: 0.023392983495692403\n",
      "Epoch: 13, Batch: 540, Avg. Loss: 0.02338149843161785\n",
      "Epoch: 13, Batch: 550, Avg. Loss: 0.023369848596529325\n",
      "Epoch: 13, Batch: 560, Avg. Loss: 0.023358175571736382\n",
      "Epoch: 13, Batch: 570, Avg. Loss: 0.023346579262359765\n",
      "Epoch: 13, Batch: 580, Avg. Loss: 0.023335186176495448\n",
      "Epoch: 13, Batch: 590, Avg. Loss: 0.023323782618064524\n",
      "Epoch: 13, Batch: 600, Avg. Loss: 0.02331248313217409\n",
      "Epoch: 13, Batch: 610, Avg. Loss: 0.023301108783961577\n",
      "Epoch: 13, Batch: 620, Avg. Loss: 0.023289783234862506\n",
      "Epoch: 13, Batch: 630, Avg. Loss: 0.023278552617163037\n",
      "Epoch: 13, Batch: 640, Avg. Loss: 0.023267039951971928\n",
      "Epoch: 13, Batch: 650, Avg. Loss: 0.023255549241659918\n",
      "Epoch: 13, Batch: 660, Avg. Loss: 0.02324417814332655\n",
      "Epoch: 13, Batch: 670, Avg. Loss: 0.023232794191799647\n",
      "Epoch: 13, Batch: 680, Avg. Loss: 0.023221175674480118\n",
      "Epoch: 13, Batch: 690, Avg. Loss: 0.023209841088685648\n",
      "Epoch: 13, Batch: 700, Avg. Loss: 0.023198714749816107\n",
      "Epoch: 13, Batch: 710, Avg. Loss: 0.02318754013003521\n",
      "Epoch: 13, Batch: 720, Avg. Loss: 0.023176152589631908\n",
      "Epoch: 13, Batch: 730, Avg. Loss: 0.02316509377782408\n",
      "Epoch: 13, Batch: 740, Avg. Loss: 0.023153946089495264\n",
      "Epoch: 13, Batch: 750, Avg. Loss: 0.023142528704772816\n",
      "Epoch: 13, Batch: 760, Avg. Loss: 0.023131266145268167\n",
      "Epoch: 13, Batch: 770, Avg. Loss: 0.023120141647228268\n",
      "Epoch: 13, Batch: 780, Avg. Loss: 0.02310977784411971\n",
      "Epoch: 13, Batch: 790, Avg. Loss: 0.023098578249747773\n",
      "Epoch: 13, Batch: 800, Avg. Loss: 0.023087632931057096\n",
      "Epoch: 13, Batch: 810, Avg. Loss: 0.023076587355062135\n",
      "Epoch: 13, Batch: 820, Avg. Loss: 0.023065585998151037\n",
      "Epoch: 13, Batch: 830, Avg. Loss: 0.023054636026520456\n",
      "Epoch: 13, Batch: 840, Avg. Loss: 0.02304392114632257\n",
      "Epoch: 13, Batch: 850, Avg. Loss: 0.023032770979160976\n",
      "Epoch: 13, Batch: 860, Avg. Loss: 0.023022127405840632\n",
      "Epoch: 13, Batch: 870, Avg. Loss: 0.023011547447541164\n",
      "Epoch: 13, Batch: 880, Avg. Loss: 0.023000405365338587\n",
      "Epoch: 13, Batch: 890, Avg. Loss: 0.022989298174780733\n",
      "Epoch: 13, Batch: 900, Avg. Loss: 0.022978383419978637\n",
      "Epoch: 13, Batch: 910, Avg. Loss: 0.022967280005876514\n",
      "Epoch: 13, Batch: 920, Avg. Loss: 0.02295630351036247\n",
      "Epoch: 13, Batch: 930, Avg. Loss: 0.022945266144631896\n",
      "Epoch: 13, Batch: 940, Avg. Loss: 0.022934070030904568\n",
      "Epoch: 13, Batch: 950, Avg. Loss: 0.02292314572146419\n",
      "Epoch: 13, Batch: 960, Avg. Loss: 0.022911858128829798\n",
      "Epoch: 13, Batch: 970, Avg. Loss: 0.02290072263004715\n",
      "Epoch: 13, Batch: 980, Avg. Loss: 0.022889959036792723\n",
      "Epoch: 13, Batch: 990, Avg. Loss: 0.022879222549808727\n",
      "Epoch: 13, Batch: 1000, Avg. Loss: 0.022868979964988358\n",
      "Epoch: 13, Batch: 1010, Avg. Loss: 0.02285828354293736\n",
      "Epoch: 13, Batch: 1020, Avg. Loss: 0.022848394960578198\n",
      "Epoch: 13, Batch: 1030, Avg. Loss: 0.022839384829483108\n",
      "Epoch: 13, Batch: 1040, Avg. Loss: 0.02283010957411707\n",
      "Epoch: 13, Batch: 1050, Avg. Loss: 0.022820222756142636\n",
      "Epoch: 13, Batch: 1060, Avg. Loss: 0.022810119752875845\n",
      "Epoch: 13, Batch: 1070, Avg. Loss: 0.022800434025279072\n",
      "Epoch: 13, Batch: 1080, Avg. Loss: 0.02279081589373592\n",
      "Epoch: 13, Batch: 1090, Avg. Loss: 0.022781518910892192\n",
      "Epoch: 13, Batch: 1100, Avg. Loss: 0.02277236461340496\n",
      "Epoch: 13, Batch: 1110, Avg. Loss: 0.022764237828816664\n",
      "Epoch: 13, Batch: 1120, Avg. Loss: 0.022755092467609334\n",
      "Epoch: 13, Batch: 1130, Avg. Loss: 0.022748309257720645\n",
      "Epoch: 13, Batch: 1140, Avg. Loss: 0.022754404498269397\n",
      "Epoch: 13, Batch: 1150, Avg. Loss: 0.022749758250329696\n",
      "Epoch: 13, Batch: 1160, Avg. Loss: 0.022749517670270832\n",
      "Epoch: 13, Batch: 1170, Avg. Loss: 0.022754377177536072\n",
      "Epoch: 13, Batch: 1180, Avg. Loss: 0.022748961170105286\n",
      "Epoch: 13, Batch: 1190, Avg. Loss: 0.02274497004280481\n",
      "Epoch: 13, Batch: 1200, Avg. Loss: 0.022737858478657207\n",
      "Epoch: 13, Batch: 1210, Avg. Loss: 0.02273019961190361\n",
      "Epoch: 13, Batch: 1220, Avg. Loss: 0.0227213836410752\n",
      "Epoch: 13, Batch: 1230, Avg. Loss: 0.02271338372014975\n",
      "Epoch: 13, Batch: 1240, Avg. Loss: 0.022704629381527723\n",
      "Epoch: 13, Batch: 1250, Avg. Loss: 0.0226983257060487\n",
      "Epoch: 13, Batch: 1260, Avg. Loss: 0.022690636650427146\n",
      "Epoch: 13, Batch: 1270, Avg. Loss: 0.022682869879856002\n",
      "Epoch: 13, Batch: 1280, Avg. Loss: 0.02267522320206534\n",
      "Epoch: 13, Batch: 1290, Avg. Loss: 0.02266784624290491\n",
      "Epoch: 13, Batch: 1300, Avg. Loss: 0.022659371002198354\n",
      "Epoch: 13, Batch: 1310, Avg. Loss: 0.022651226619048868\n",
      "Epoch: 13, Batch: 1320, Avg. Loss: 0.022643719044596302\n",
      "Epoch: 13, Batch: 1330, Avg. Loss: 0.02263478712798126\n",
      "Epoch: 13, Batch: 1340, Avg. Loss: 0.022625316484236362\n",
      "Epoch: 13, Batch: 1350, Avg. Loss: 0.022616899960260558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Batch: 1360, Avg. Loss: 0.02260734933294645\n",
      "Epoch: 13, Batch: 1370, Avg. Loss: 0.022597758772870267\n",
      "Epoch: 13, Batch: 1380, Avg. Loss: 0.022588326466621834\n",
      "Epoch: 13, Batch: 1390, Avg. Loss: 0.022579338914043807\n",
      "Epoch: 13, Batch: 1400, Avg. Loss: 0.022570756441667724\n",
      "Epoch: 13, Batch: 1410, Avg. Loss: 0.022563145395084337\n",
      "Epoch: 13, Batch: 1420, Avg. Loss: 0.02255476824198357\n",
      "Epoch: 13, Batch: 1430, Avg. Loss: 0.022545888372809735\n",
      "Epoch: 13, Batch: 1440, Avg. Loss: 0.022537518115011427\n",
      "Epoch: 13, Batch: 1450, Avg. Loss: 0.02252857430432501\n",
      "Epoch: 13, Batch: 1460, Avg. Loss: 0.0225191767991202\n",
      "Epoch: 13, Batch: 1470, Avg. Loss: 0.022511308329280548\n",
      "Epoch: 13, Batch: 1480, Avg. Loss: 0.022503672297714324\n",
      "Epoch: 13, Batch: 1490, Avg. Loss: 0.02249528433216206\n",
      "Epoch: 13, Batch: 1500, Avg. Loss: 0.022486764581315804\n",
      "Epoch: 13, Batch: 1510, Avg. Loss: 0.022477273693895784\n",
      "Epoch: 13, Batch: 1520, Avg. Loss: 0.022467944648134088\n",
      "Epoch: 13, Batch: 1530, Avg. Loss: 0.02245836193902607\n",
      "Epoch: 13, Batch: 1540, Avg. Loss: 0.022448575595617627\n",
      "Epoch: 14, Batch: 10, Avg. Loss: 0.022433298904760795\n",
      "Epoch: 14, Batch: 20, Avg. Loss: 0.022423652939113664\n",
      "Epoch: 14, Batch: 30, Avg. Loss: 0.022413887535685986\n",
      "Epoch: 14, Batch: 40, Avg. Loss: 0.022403963352954758\n",
      "Epoch: 14, Batch: 50, Avg. Loss: 0.02239385556130124\n",
      "Epoch: 14, Batch: 60, Avg. Loss: 0.022383816199496708\n",
      "Epoch: 14, Batch: 70, Avg. Loss: 0.022373463423841056\n",
      "Epoch: 14, Batch: 80, Avg. Loss: 0.02236354405245168\n",
      "Epoch: 14, Batch: 90, Avg. Loss: 0.022353346520586546\n",
      "Epoch: 14, Batch: 100, Avg. Loss: 0.022342939153715734\n",
      "Epoch: 14, Batch: 110, Avg. Loss: 0.022332625677121354\n",
      "Epoch: 14, Batch: 120, Avg. Loss: 0.02232231185678759\n",
      "Epoch: 14, Batch: 130, Avg. Loss: 0.02231190516349249\n",
      "Epoch: 14, Batch: 140, Avg. Loss: 0.02230165420032753\n",
      "Epoch: 14, Batch: 150, Avg. Loss: 0.022291333186840467\n",
      "Epoch: 14, Batch: 160, Avg. Loss: 0.022281002720392008\n",
      "Epoch: 14, Batch: 170, Avg. Loss: 0.02227081796639927\n",
      "Epoch: 14, Batch: 180, Avg. Loss: 0.022260461285261577\n",
      "Epoch: 14, Batch: 190, Avg. Loss: 0.02225017795950449\n",
      "Epoch: 14, Batch: 200, Avg. Loss: 0.02223978502757428\n",
      "Epoch: 14, Batch: 210, Avg. Loss: 0.022229406248891775\n",
      "Epoch: 14, Batch: 220, Avg. Loss: 0.02221909214405515\n",
      "Epoch: 14, Batch: 230, Avg. Loss: 0.022208746401765362\n",
      "Epoch: 14, Batch: 240, Avg. Loss: 0.02219849212883196\n",
      "Epoch: 14, Batch: 250, Avg. Loss: 0.02218815997220694\n",
      "Epoch: 14, Batch: 260, Avg. Loss: 0.022177960888551188\n",
      "Epoch: 14, Batch: 270, Avg. Loss: 0.022167695127236667\n",
      "Epoch: 14, Batch: 280, Avg. Loss: 0.022157668602415664\n",
      "Epoch: 14, Batch: 290, Avg. Loss: 0.02214745960586173\n",
      "Epoch: 14, Batch: 300, Avg. Loss: 0.02213710901121072\n",
      "Epoch: 14, Batch: 310, Avg. Loss: 0.022126642212803607\n",
      "Epoch: 14, Batch: 320, Avg. Loss: 0.02211650500531109\n",
      "Epoch: 14, Batch: 330, Avg. Loss: 0.022106148422834388\n",
      "Epoch: 14, Batch: 340, Avg. Loss: 0.022095861420257098\n",
      "Epoch: 14, Batch: 350, Avg. Loss: 0.022085534816072425\n",
      "Epoch: 14, Batch: 360, Avg. Loss: 0.022075272576143914\n",
      "Epoch: 14, Batch: 370, Avg. Loss: 0.02206515100918756\n",
      "Epoch: 14, Batch: 380, Avg. Loss: 0.022055234023423607\n",
      "Epoch: 14, Batch: 390, Avg. Loss: 0.022045047135187977\n",
      "Epoch: 14, Batch: 400, Avg. Loss: 0.022035329925955915\n",
      "Epoch: 14, Batch: 410, Avg. Loss: 0.022025323434125693\n",
      "Epoch: 14, Batch: 420, Avg. Loss: 0.022015367788851693\n",
      "Epoch: 14, Batch: 430, Avg. Loss: 0.022005366039757557\n",
      "Epoch: 14, Batch: 440, Avg. Loss: 0.021995325393902972\n",
      "Epoch: 14, Batch: 450, Avg. Loss: 0.021985171883037342\n",
      "Epoch: 14, Batch: 460, Avg. Loss: 0.021974940910530403\n",
      "Epoch: 14, Batch: 470, Avg. Loss: 0.021964986732384978\n",
      "Epoch: 14, Batch: 480, Avg. Loss: 0.021954791146231778\n",
      "Epoch: 14, Batch: 490, Avg. Loss: 0.02194457614789075\n",
      "Epoch: 14, Batch: 500, Avg. Loss: 0.021934657424605694\n",
      "Epoch: 14, Batch: 510, Avg. Loss: 0.021924514469743486\n",
      "Epoch: 14, Batch: 520, Avg. Loss: 0.02191433775921143\n",
      "Epoch: 14, Batch: 530, Avg. Loss: 0.021904126728558706\n",
      "Epoch: 14, Batch: 540, Avg. Loss: 0.02189401384156725\n",
      "Epoch: 14, Batch: 550, Avg. Loss: 0.021883866709641892\n",
      "Epoch: 14, Batch: 560, Avg. Loss: 0.021873780736076365\n",
      "Epoch: 14, Batch: 570, Avg. Loss: 0.02186365988403691\n",
      "Epoch: 14, Batch: 580, Avg. Loss: 0.021853684527652954\n",
      "Epoch: 14, Batch: 590, Avg. Loss: 0.021844012715696057\n",
      "Epoch: 14, Batch: 600, Avg. Loss: 0.021834186138077014\n",
      "Epoch: 14, Batch: 610, Avg. Loss: 0.02182431053559559\n",
      "Epoch: 14, Batch: 620, Avg. Loss: 0.021814376246730365\n",
      "Epoch: 14, Batch: 630, Avg. Loss: 0.02180469388498753\n",
      "Epoch: 14, Batch: 640, Avg. Loss: 0.021794918371080826\n",
      "Epoch: 14, Batch: 650, Avg. Loss: 0.02178494048925324\n",
      "Epoch: 14, Batch: 660, Avg. Loss: 0.02177525406595182\n",
      "Epoch: 14, Batch: 670, Avg. Loss: 0.02176558633819796\n",
      "Epoch: 14, Batch: 680, Avg. Loss: 0.02175555965596904\n",
      "Epoch: 14, Batch: 690, Avg. Loss: 0.021746053095407496\n",
      "Epoch: 14, Batch: 700, Avg. Loss: 0.021736471536908126\n",
      "Epoch: 14, Batch: 710, Avg. Loss: 0.02172690559940245\n",
      "Epoch: 14, Batch: 720, Avg. Loss: 0.021717264307123802\n",
      "Epoch: 14, Batch: 730, Avg. Loss: 0.021707564877861824\n",
      "Epoch: 14, Batch: 740, Avg. Loss: 0.02169792573024779\n",
      "Epoch: 14, Batch: 750, Avg. Loss: 0.021688051696424824\n",
      "Epoch: 14, Batch: 760, Avg. Loss: 0.021678302777803856\n",
      "Epoch: 14, Batch: 770, Avg. Loss: 0.02166868591416269\n",
      "Epoch: 14, Batch: 780, Avg. Loss: 0.021659545419947385\n",
      "Epoch: 14, Batch: 790, Avg. Loss: 0.021649798944718617\n",
      "Epoch: 14, Batch: 800, Avg. Loss: 0.021640302715353093\n",
      "Epoch: 14, Batch: 810, Avg. Loss: 0.021630968724649584\n",
      "Epoch: 14, Batch: 820, Avg. Loss: 0.021621337701175726\n",
      "Epoch: 14, Batch: 830, Avg. Loss: 0.021611695308565146\n",
      "Epoch: 14, Batch: 840, Avg. Loss: 0.021602431574775923\n",
      "Epoch: 14, Batch: 850, Avg. Loss: 0.021592743599827657\n",
      "Epoch: 14, Batch: 860, Avg. Loss: 0.02158353734009857\n",
      "Epoch: 14, Batch: 870, Avg. Loss: 0.021574131605435486\n",
      "Epoch: 14, Batch: 880, Avg. Loss: 0.02156459256797223\n",
      "Epoch: 14, Batch: 890, Avg. Loss: 0.021555124274043533\n",
      "Epoch: 14, Batch: 900, Avg. Loss: 0.02154547682011985\n",
      "Epoch: 14, Batch: 910, Avg. Loss: 0.021535766288196683\n",
      "Epoch: 14, Batch: 920, Avg. Loss: 0.02152632794906439\n",
      "Epoch: 14, Batch: 930, Avg. Loss: 0.021516748287675167\n",
      "Epoch: 14, Batch: 940, Avg. Loss: 0.021507068177676874\n",
      "Epoch: 14, Batch: 950, Avg. Loss: 0.021497533187145736\n",
      "Epoch: 14, Batch: 960, Avg. Loss: 0.021487934480423302\n",
      "Epoch: 14, Batch: 970, Avg. Loss: 0.02147844286449089\n",
      "Epoch: 14, Batch: 980, Avg. Loss: 0.021468914797350656\n",
      "Epoch: 14, Batch: 990, Avg. Loss: 0.021459624700752956\n",
      "Epoch: 14, Batch: 1000, Avg. Loss: 0.021450874022229817\n",
      "Epoch: 14, Batch: 1010, Avg. Loss: 0.02144158658502879\n",
      "Epoch: 14, Batch: 1020, Avg. Loss: 0.021433091267948994\n",
      "Epoch: 14, Batch: 1030, Avg. Loss: 0.021424970555735494\n",
      "Epoch: 14, Batch: 1040, Avg. Loss: 0.021417181722409605\n",
      "Epoch: 14, Batch: 1050, Avg. Loss: 0.021408841928838292\n",
      "Epoch: 14, Batch: 1060, Avg. Loss: 0.02140032764618979\n",
      "Epoch: 14, Batch: 1070, Avg. Loss: 0.02139241553139684\n",
      "Epoch: 14, Batch: 1080, Avg. Loss: 0.021384095391213574\n",
      "Epoch: 14, Batch: 1090, Avg. Loss: 0.021376936643288363\n",
      "Epoch: 14, Batch: 1100, Avg. Loss: 0.021369078590922377\n",
      "Epoch: 14, Batch: 1110, Avg. Loss: 0.021361419892612685\n",
      "Epoch: 14, Batch: 1120, Avg. Loss: 0.02135323104339016\n",
      "Epoch: 14, Batch: 1130, Avg. Loss: 0.021348229180931565\n",
      "Epoch: 14, Batch: 1140, Avg. Loss: 0.02134508207693462\n",
      "Epoch: 14, Batch: 1150, Avg. Loss: 0.021342860887732934\n",
      "Epoch: 14, Batch: 1160, Avg. Loss: 0.02133898853474373\n",
      "Epoch: 14, Batch: 1170, Avg. Loss: 0.021336820743146995\n",
      "Epoch: 14, Batch: 1180, Avg. Loss: 0.021334189470511557\n",
      "Epoch: 14, Batch: 1190, Avg. Loss: 0.021329214649444577\n",
      "Epoch: 14, Batch: 1200, Avg. Loss: 0.021324425761656586\n",
      "Epoch: 14, Batch: 1210, Avg. Loss: 0.021318860451327287\n",
      "Epoch: 14, Batch: 1220, Avg. Loss: 0.02131275831659253\n",
      "Epoch: 14, Batch: 1230, Avg. Loss: 0.02130830148315887\n",
      "Epoch: 14, Batch: 1240, Avg. Loss: 0.021301467392035242\n",
      "Epoch: 14, Batch: 1250, Avg. Loss: 0.021297288548754838\n",
      "Epoch: 14, Batch: 1260, Avg. Loss: 0.021293225697520297\n",
      "Epoch: 14, Batch: 1270, Avg. Loss: 0.021286529683587668\n",
      "Epoch: 14, Batch: 1280, Avg. Loss: 0.021280902987807913\n",
      "Epoch: 14, Batch: 1290, Avg. Loss: 0.021274084900699892\n",
      "Epoch: 14, Batch: 1300, Avg. Loss: 0.021266701081207772\n",
      "Epoch: 14, Batch: 1310, Avg. Loss: 0.02125999264217632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Batch: 1320, Avg. Loss: 0.021253496230701812\n",
      "Epoch: 14, Batch: 1330, Avg. Loss: 0.021245893458194293\n",
      "Epoch: 14, Batch: 1340, Avg. Loss: 0.0212382354065219\n",
      "Epoch: 14, Batch: 1350, Avg. Loss: 0.021230621086806383\n",
      "Epoch: 14, Batch: 1360, Avg. Loss: 0.021223113483699426\n",
      "Epoch: 14, Batch: 1370, Avg. Loss: 0.0212149781376392\n",
      "Epoch: 14, Batch: 1380, Avg. Loss: 0.021206616226530826\n",
      "Epoch: 14, Batch: 1390, Avg. Loss: 0.021199241855297795\n",
      "Epoch: 14, Batch: 1400, Avg. Loss: 0.021191312109023027\n",
      "Epoch: 14, Batch: 1410, Avg. Loss: 0.021183851704366485\n",
      "Epoch: 14, Batch: 1420, Avg. Loss: 0.021176415899661488\n",
      "Epoch: 14, Batch: 1430, Avg. Loss: 0.021168734400100822\n",
      "Epoch: 14, Batch: 1440, Avg. Loss: 0.021161514998787684\n",
      "Epoch: 14, Batch: 1450, Avg. Loss: 0.021154753093949194\n",
      "Epoch: 14, Batch: 1460, Avg. Loss: 0.021146863396044328\n",
      "Epoch: 14, Batch: 1470, Avg. Loss: 0.02113913332642596\n",
      "Epoch: 14, Batch: 1480, Avg. Loss: 0.021132226207356383\n",
      "Epoch: 14, Batch: 1490, Avg. Loss: 0.021124363649431977\n",
      "Epoch: 14, Batch: 1500, Avg. Loss: 0.021117095903485927\n",
      "Epoch: 14, Batch: 1510, Avg. Loss: 0.021108642425479315\n",
      "Epoch: 14, Batch: 1520, Avg. Loss: 0.021100018167910618\n",
      "Epoch: 14, Batch: 1530, Avg. Loss: 0.02109132832552904\n",
      "Epoch: 14, Batch: 1540, Avg. Loss: 0.0210829368118455\n",
      "Epoch: 15, Batch: 10, Avg. Loss: 0.021069938261468846\n",
      "Epoch: 15, Batch: 20, Avg. Loss: 0.02106129979451112\n",
      "Epoch: 15, Batch: 30, Avg. Loss: 0.02105281340548315\n",
      "Epoch: 15, Batch: 40, Avg. Loss: 0.02104402154953927\n",
      "Epoch: 15, Batch: 50, Avg. Loss: 0.021035012811409973\n",
      "Epoch: 15, Batch: 60, Avg. Loss: 0.02102619385002395\n",
      "Epoch: 15, Batch: 70, Avg. Loss: 0.021017041284507716\n",
      "Epoch: 15, Batch: 80, Avg. Loss: 0.021008686232133783\n",
      "Epoch: 15, Batch: 90, Avg. Loss: 0.020999723502563145\n",
      "Epoch: 15, Batch: 100, Avg. Loss: 0.02099063155091828\n",
      "Epoch: 15, Batch: 110, Avg. Loss: 0.020981820402769198\n",
      "Epoch: 15, Batch: 120, Avg. Loss: 0.02097278838627918\n",
      "Epoch: 15, Batch: 130, Avg. Loss: 0.020963785398800165\n",
      "Epoch: 15, Batch: 140, Avg. Loss: 0.020954921981031993\n",
      "Epoch: 15, Batch: 150, Avg. Loss: 0.02094595724529438\n",
      "Epoch: 15, Batch: 160, Avg. Loss: 0.020937056764481182\n",
      "Epoch: 15, Batch: 170, Avg. Loss: 0.020928095689484758\n",
      "Epoch: 15, Batch: 180, Avg. Loss: 0.020918959296074145\n",
      "Epoch: 15, Batch: 190, Avg. Loss: 0.020909974392659646\n",
      "Epoch: 15, Batch: 200, Avg. Loss: 0.02090093479824793\n",
      "Epoch: 15, Batch: 210, Avg. Loss: 0.020891934191314413\n",
      "Epoch: 15, Batch: 220, Avg. Loss: 0.020882940771311097\n",
      "Epoch: 15, Batch: 230, Avg. Loss: 0.02087403891202914\n",
      "Epoch: 15, Batch: 240, Avg. Loss: 0.020865051471226862\n",
      "Epoch: 15, Batch: 250, Avg. Loss: 0.020856003780813894\n",
      "Epoch: 15, Batch: 260, Avg. Loss: 0.020847016923321568\n",
      "Epoch: 15, Batch: 270, Avg. Loss: 0.020838162676828337\n",
      "Epoch: 15, Batch: 280, Avg. Loss: 0.02082961814192686\n",
      "Epoch: 15, Batch: 290, Avg. Loss: 0.020820938349119234\n",
      "Epoch: 15, Batch: 300, Avg. Loss: 0.02081184076209836\n",
      "Epoch: 15, Batch: 310, Avg. Loss: 0.020802944685345184\n",
      "Epoch: 15, Batch: 320, Avg. Loss: 0.020794676648829554\n",
      "Epoch: 15, Batch: 330, Avg. Loss: 0.020785772379961764\n",
      "Epoch: 15, Batch: 340, Avg. Loss: 0.02077681469872036\n",
      "Epoch: 15, Batch: 350, Avg. Loss: 0.020767872795036477\n",
      "Epoch: 15, Batch: 360, Avg. Loss: 0.020758999225057164\n",
      "Epoch: 15, Batch: 370, Avg. Loss: 0.020749904878944958\n",
      "Epoch: 15, Batch: 380, Avg. Loss: 0.020741044302973983\n",
      "Epoch: 15, Batch: 390, Avg. Loss: 0.020732218699501984\n",
      "Epoch: 15, Batch: 400, Avg. Loss: 0.020723477194921545\n",
      "Epoch: 15, Batch: 410, Avg. Loss: 0.020714532968095327\n",
      "Epoch: 15, Batch: 420, Avg. Loss: 0.02070571244479513\n",
      "Epoch: 15, Batch: 430, Avg. Loss: 0.02069691085748557\n",
      "Epoch: 15, Batch: 440, Avg. Loss: 0.020688132328732123\n",
      "Epoch: 15, Batch: 450, Avg. Loss: 0.020679287007687664\n",
      "Epoch: 15, Batch: 460, Avg. Loss: 0.020670391462631003\n",
      "Epoch: 15, Batch: 470, Avg. Loss: 0.020661794777741643\n",
      "Epoch: 15, Batch: 480, Avg. Loss: 0.02065281451543965\n",
      "Epoch: 15, Batch: 490, Avg. Loss: 0.02064402973098835\n",
      "Epoch: 15, Batch: 500, Avg. Loss: 0.020635223740305502\n",
      "Epoch: 15, Batch: 510, Avg. Loss: 0.020626396446915076\n",
      "Epoch: 15, Batch: 520, Avg. Loss: 0.02061755999746698\n",
      "Epoch: 15, Batch: 530, Avg. Loss: 0.02060867839019347\n",
      "Epoch: 15, Batch: 540, Avg. Loss: 0.020599894103494592\n",
      "Epoch: 15, Batch: 550, Avg. Loss: 0.020590996802191962\n",
      "Epoch: 15, Batch: 560, Avg. Loss: 0.02058209505745274\n",
      "Epoch: 15, Batch: 570, Avg. Loss: 0.020573197481061786\n",
      "Epoch: 15, Batch: 580, Avg. Loss: 0.020564514792937166\n",
      "Epoch: 15, Batch: 590, Avg. Loss: 0.020556010992546277\n",
      "Epoch: 15, Batch: 600, Avg. Loss: 0.020547373897516208\n",
      "Epoch: 15, Batch: 610, Avg. Loss: 0.020538657102102887\n",
      "Epoch: 15, Batch: 620, Avg. Loss: 0.020530004348054912\n",
      "Epoch: 15, Batch: 630, Avg. Loss: 0.0205215631383135\n",
      "Epoch: 15, Batch: 640, Avg. Loss: 0.020512973784246154\n",
      "Epoch: 15, Batch: 650, Avg. Loss: 0.020504222702409942\n",
      "Epoch: 15, Batch: 660, Avg. Loss: 0.020495750754117997\n",
      "Epoch: 15, Batch: 670, Avg. Loss: 0.020487213647480344\n",
      "Epoch: 15, Batch: 680, Avg. Loss: 0.020478514266255517\n",
      "Epoch: 15, Batch: 690, Avg. Loss: 0.020470582319080707\n",
      "Epoch: 15, Batch: 700, Avg. Loss: 0.020462303754341552\n",
      "Epoch: 15, Batch: 710, Avg. Loss: 0.02045399834775447\n",
      "Epoch: 15, Batch: 720, Avg. Loss: 0.020445395888587358\n",
      "Epoch: 15, Batch: 730, Avg. Loss: 0.020436834321717258\n",
      "Epoch: 15, Batch: 740, Avg. Loss: 0.020428658120151082\n",
      "Epoch: 15, Batch: 750, Avg. Loss: 0.020420085291401397\n",
      "Epoch: 15, Batch: 760, Avg. Loss: 0.020411642996249174\n",
      "Epoch: 15, Batch: 770, Avg. Loss: 0.020403289165744406\n",
      "Epoch: 15, Batch: 780, Avg. Loss: 0.020395132533831992\n",
      "Epoch: 15, Batch: 790, Avg. Loss: 0.020386674038848907\n",
      "Epoch: 15, Batch: 800, Avg. Loss: 0.020378339081402515\n",
      "Epoch: 15, Batch: 810, Avg. Loss: 0.020369952492077303\n",
      "Epoch: 15, Batch: 820, Avg. Loss: 0.020361567409471374\n",
      "Epoch: 15, Batch: 830, Avg. Loss: 0.020353314975702332\n",
      "Epoch: 15, Batch: 840, Avg. Loss: 0.020345105945790073\n",
      "Epoch: 15, Batch: 850, Avg. Loss: 0.020336862885665927\n",
      "Epoch: 15, Batch: 860, Avg. Loss: 0.020328719407574063\n",
      "Epoch: 15, Batch: 870, Avg. Loss: 0.020320549524758966\n",
      "Epoch: 15, Batch: 880, Avg. Loss: 0.020312061603049002\n",
      "Epoch: 15, Batch: 890, Avg. Loss: 0.020303527850558806\n",
      "Epoch: 15, Batch: 900, Avg. Loss: 0.020295253195180756\n",
      "Epoch: 15, Batch: 910, Avg. Loss: 0.020286857052772098\n",
      "Epoch: 15, Batch: 920, Avg. Loss: 0.02027837818199868\n",
      "Epoch: 15, Batch: 930, Avg. Loss: 0.020270040146912804\n",
      "Epoch: 15, Batch: 940, Avg. Loss: 0.020261580965168058\n",
      "Epoch: 15, Batch: 950, Avg. Loss: 0.020253024330931877\n",
      "Epoch: 15, Batch: 960, Avg. Loss: 0.020244577255303237\n",
      "Epoch: 15, Batch: 970, Avg. Loss: 0.02023610721798686\n",
      "Epoch: 15, Batch: 980, Avg. Loss: 0.020227965458369218\n",
      "Epoch: 15, Batch: 990, Avg. Loss: 0.020219961765719236\n",
      "Epoch: 15, Batch: 1000, Avg. Loss: 0.020212440050830116\n",
      "Epoch: 15, Batch: 1010, Avg. Loss: 0.02020432049804875\n",
      "Epoch: 15, Batch: 1020, Avg. Loss: 0.020196990679336713\n",
      "Epoch: 15, Batch: 1030, Avg. Loss: 0.020189803707581243\n",
      "Epoch: 15, Batch: 1040, Avg. Loss: 0.020182479141467518\n",
      "Epoch: 15, Batch: 1050, Avg. Loss: 0.02017512454791053\n",
      "Epoch: 15, Batch: 1060, Avg. Loss: 0.020168186506217316\n",
      "Epoch: 15, Batch: 1070, Avg. Loss: 0.02016119347900684\n",
      "Epoch: 15, Batch: 1080, Avg. Loss: 0.020154038863614943\n",
      "Epoch: 15, Batch: 1090, Avg. Loss: 0.020148200465390304\n",
      "Epoch: 15, Batch: 1100, Avg. Loss: 0.020141750633869294\n",
      "Epoch: 15, Batch: 1110, Avg. Loss: 0.020135977149614893\n",
      "Epoch: 15, Batch: 1120, Avg. Loss: 0.020129928956387797\n",
      "Epoch: 15, Batch: 1130, Avg. Loss: 0.020124885995619797\n",
      "Epoch: 15, Batch: 1140, Avg. Loss: 0.02012119121839016\n",
      "Epoch: 15, Batch: 1150, Avg. Loss: 0.020115547425158045\n",
      "Epoch: 15, Batch: 1160, Avg. Loss: 0.02011207673504771\n",
      "Epoch: 15, Batch: 1170, Avg. Loss: 0.020106945893863564\n",
      "Epoch: 15, Batch: 1180, Avg. Loss: 0.02010246181253863\n",
      "Epoch: 15, Batch: 1190, Avg. Loss: 0.020098627663339454\n",
      "Epoch: 15, Batch: 1200, Avg. Loss: 0.02009439753343836\n",
      "Epoch: 15, Batch: 1210, Avg. Loss: 0.020088496610536747\n",
      "Epoch: 15, Batch: 1220, Avg. Loss: 0.020082323843152526\n",
      "Epoch: 15, Batch: 1230, Avg. Loss: 0.020082147935734745\n",
      "Epoch: 15, Batch: 1240, Avg. Loss: 0.020076202491401095\n",
      "Epoch: 15, Batch: 1250, Avg. Loss: 0.020072144005071048\n",
      "Epoch: 15, Batch: 1260, Avg. Loss: 0.020069550369429798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Batch: 1270, Avg. Loss: 0.020065930818509978\n",
      "Epoch: 15, Batch: 1280, Avg. Loss: 0.020064149673579917\n",
      "Epoch: 15, Batch: 1290, Avg. Loss: 0.020058978129704974\n",
      "Epoch: 15, Batch: 1300, Avg. Loss: 0.020053802120300418\n",
      "Epoch: 15, Batch: 1310, Avg. Loss: 0.020049254731193837\n",
      "Epoch: 15, Batch: 1320, Avg. Loss: 0.020044995744758168\n",
      "Epoch: 15, Batch: 1330, Avg. Loss: 0.020039749388965224\n",
      "Epoch: 15, Batch: 1340, Avg. Loss: 0.02003390203293708\n",
      "Epoch: 15, Batch: 1350, Avg. Loss: 0.020027141895929235\n",
      "Epoch: 15, Batch: 1360, Avg. Loss: 0.020020796385117758\n",
      "Epoch: 15, Batch: 1370, Avg. Loss: 0.020014264406515807\n",
      "Epoch: 15, Batch: 1380, Avg. Loss: 0.020007261005710868\n",
      "Epoch: 15, Batch: 1390, Avg. Loss: 0.020000639516875744\n",
      "Epoch: 15, Batch: 1400, Avg. Loss: 0.019993880528415765\n",
      "Epoch: 15, Batch: 1410, Avg. Loss: 0.019987659500199643\n",
      "Epoch: 15, Batch: 1420, Avg. Loss: 0.019981007558166165\n",
      "Epoch: 15, Batch: 1430, Avg. Loss: 0.019974113200203408\n",
      "Epoch: 15, Batch: 1440, Avg. Loss: 0.01996893810649439\n",
      "Epoch: 15, Batch: 1450, Avg. Loss: 0.019962336257081523\n",
      "Epoch: 15, Batch: 1460, Avg. Loss: 0.01995542800072301\n",
      "Epoch: 15, Batch: 1470, Avg. Loss: 0.01995004264994196\n",
      "Epoch: 15, Batch: 1480, Avg. Loss: 0.01994488317953455\n",
      "Epoch: 15, Batch: 1490, Avg. Loss: 0.019938757092043145\n",
      "Epoch: 15, Batch: 1500, Avg. Loss: 0.019932307174942997\n",
      "Epoch: 15, Batch: 1510, Avg. Loss: 0.01992517975364563\n",
      "Epoch: 15, Batch: 1520, Avg. Loss: 0.019918119112377796\n",
      "Epoch: 15, Batch: 1530, Avg. Loss: 0.019910760406751302\n",
      "Epoch: 15, Batch: 1540, Avg. Loss: 0.019903393709745\n",
      "Epoch: 16, Batch: 10, Avg. Loss: 0.01989213763691753\n",
      "Epoch: 16, Batch: 20, Avg. Loss: 0.01988497701696729\n",
      "Epoch: 16, Batch: 30, Avg. Loss: 0.01987784473091972\n",
      "Epoch: 16, Batch: 40, Avg. Loss: 0.019870311541878696\n",
      "Epoch: 16, Batch: 50, Avg. Loss: 0.0198623367319832\n",
      "Epoch: 16, Batch: 60, Avg. Loss: 0.019854731773552167\n",
      "Epoch: 16, Batch: 70, Avg. Loss: 0.019846920853024697\n",
      "Epoch: 16, Batch: 80, Avg. Loss: 0.019839631503306875\n",
      "Epoch: 16, Batch: 90, Avg. Loss: 0.019831960750166296\n",
      "Epoch: 16, Batch: 100, Avg. Loss: 0.01982413293767219\n",
      "Epoch: 16, Batch: 110, Avg. Loss: 0.019816347663213503\n",
      "Epoch: 16, Batch: 120, Avg. Loss: 0.019808464632155867\n",
      "Epoch: 16, Batch: 130, Avg. Loss: 0.019800546080493694\n",
      "Epoch: 16, Batch: 140, Avg. Loss: 0.019792729962524572\n",
      "Epoch: 16, Batch: 150, Avg. Loss: 0.01978497371068125\n",
      "Epoch: 16, Batch: 160, Avg. Loss: 0.019776983598489912\n",
      "Epoch: 16, Batch: 170, Avg. Loss: 0.019769204570605076\n",
      "Epoch: 16, Batch: 180, Avg. Loss: 0.01976120177726065\n",
      "Epoch: 16, Batch: 190, Avg. Loss: 0.0197532424161027\n",
      "Epoch: 16, Batch: 200, Avg. Loss: 0.019745227339988954\n",
      "Epoch: 16, Batch: 210, Avg. Loss: 0.019737311284464033\n",
      "Epoch: 16, Batch: 220, Avg. Loss: 0.019729452568035415\n",
      "Epoch: 16, Batch: 230, Avg. Loss: 0.01972162890236279\n",
      "Epoch: 16, Batch: 240, Avg. Loss: 0.019713775173366345\n",
      "Epoch: 16, Batch: 250, Avg. Loss: 0.019705920276349417\n",
      "Epoch: 16, Batch: 260, Avg. Loss: 0.019697987032879765\n",
      "Epoch: 16, Batch: 270, Avg. Loss: 0.019690025952544\n",
      "Epoch: 16, Batch: 280, Avg. Loss: 0.019682304137976468\n",
      "Epoch: 16, Batch: 290, Avg. Loss: 0.01967442612045428\n",
      "Epoch: 16, Batch: 300, Avg. Loss: 0.019666391701414664\n",
      "Epoch: 16, Batch: 310, Avg. Loss: 0.019658343483738298\n",
      "Epoch: 16, Batch: 320, Avg. Loss: 0.019650484615405937\n",
      "Epoch: 16, Batch: 330, Avg. Loss: 0.019642511934581587\n",
      "Epoch: 16, Batch: 340, Avg. Loss: 0.019634539265027624\n",
      "Epoch: 16, Batch: 350, Avg. Loss: 0.019626642838956966\n",
      "Epoch: 16, Batch: 360, Avg. Loss: 0.01961881277924565\n",
      "Epoch: 16, Batch: 370, Avg. Loss: 0.0196108331755036\n",
      "Epoch: 16, Batch: 380, Avg. Loss: 0.01960319151093019\n",
      "Epoch: 16, Batch: 390, Avg. Loss: 0.019595389778255205\n",
      "Epoch: 16, Batch: 400, Avg. Loss: 0.019587622330039158\n",
      "Epoch: 16, Batch: 410, Avg. Loss: 0.01957977103534849\n",
      "Epoch: 16, Batch: 420, Avg. Loss: 0.01957200855202843\n",
      "Epoch: 16, Batch: 430, Avg. Loss: 0.0195641498321562\n",
      "Epoch: 16, Batch: 440, Avg. Loss: 0.019556402356342848\n",
      "Epoch: 16, Batch: 450, Avg. Loss: 0.01954866543182207\n",
      "Epoch: 16, Batch: 460, Avg. Loss: 0.01954085301530269\n",
      "Epoch: 16, Batch: 470, Avg. Loss: 0.019533432902302757\n",
      "Epoch: 16, Batch: 480, Avg. Loss: 0.019525548217420034\n",
      "Epoch: 16, Batch: 490, Avg. Loss: 0.019517658872716732\n",
      "Epoch: 16, Batch: 500, Avg. Loss: 0.019509842063117445\n",
      "Epoch: 16, Batch: 510, Avg. Loss: 0.019502084772601003\n",
      "Epoch: 16, Batch: 520, Avg. Loss: 0.019494279020490408\n",
      "Epoch: 16, Batch: 530, Avg. Loss: 0.019486419704971943\n",
      "Epoch: 16, Batch: 540, Avg. Loss: 0.01947868945400784\n",
      "Epoch: 16, Batch: 550, Avg. Loss: 0.01947089471431795\n",
      "Epoch: 16, Batch: 560, Avg. Loss: 0.019463035833865744\n",
      "Epoch: 16, Batch: 570, Avg. Loss: 0.019455204388154235\n",
      "Epoch: 16, Batch: 580, Avg. Loss: 0.019447644875581333\n",
      "Epoch: 16, Batch: 590, Avg. Loss: 0.01944007836180777\n",
      "Epoch: 16, Batch: 600, Avg. Loss: 0.019432465656425587\n",
      "Epoch: 16, Batch: 610, Avg. Loss: 0.019424860311659183\n",
      "Epoch: 16, Batch: 620, Avg. Loss: 0.019417243658325735\n",
      "Epoch: 16, Batch: 630, Avg. Loss: 0.019409639675460545\n",
      "Epoch: 16, Batch: 640, Avg. Loss: 0.019401979352497083\n",
      "Epoch: 16, Batch: 650, Avg. Loss: 0.019394198472137962\n",
      "Epoch: 16, Batch: 660, Avg. Loss: 0.019386825039929906\n",
      "Epoch: 16, Batch: 670, Avg. Loss: 0.019379214068256805\n",
      "Epoch: 16, Batch: 680, Avg. Loss: 0.019371442187034592\n",
      "Epoch: 16, Batch: 690, Avg. Loss: 0.01936484137153117\n",
      "Epoch: 16, Batch: 700, Avg. Loss: 0.01935754649103181\n",
      "Epoch: 16, Batch: 710, Avg. Loss: 0.019350225325604712\n",
      "Epoch: 16, Batch: 720, Avg. Loss: 0.019342595800439764\n",
      "Epoch: 16, Batch: 730, Avg. Loss: 0.019334918300305773\n",
      "Epoch: 16, Batch: 740, Avg. Loss: 0.019327629443790188\n",
      "Epoch: 16, Batch: 750, Avg. Loss: 0.0193199878039592\n",
      "Epoch: 16, Batch: 760, Avg. Loss: 0.019312390214585598\n",
      "Epoch: 16, Batch: 770, Avg. Loss: 0.019304784696484657\n",
      "Epoch: 16, Batch: 780, Avg. Loss: 0.01929771333833691\n",
      "Epoch: 16, Batch: 790, Avg. Loss: 0.019290101098333193\n",
      "Epoch: 16, Batch: 800, Avg. Loss: 0.01928266292206529\n",
      "Epoch: 16, Batch: 810, Avg. Loss: 0.01927527815422988\n",
      "Epoch: 16, Batch: 820, Avg. Loss: 0.01926800022180825\n",
      "Epoch: 16, Batch: 830, Avg. Loss: 0.019260579433814713\n",
      "Epoch: 16, Batch: 840, Avg. Loss: 0.01925349067624376\n",
      "Epoch: 16, Batch: 850, Avg. Loss: 0.01924635461469873\n",
      "Epoch: 16, Batch: 860, Avg. Loss: 0.01923923668382975\n",
      "Epoch: 16, Batch: 870, Avg. Loss: 0.019232020713260693\n",
      "Epoch: 16, Batch: 880, Avg. Loss: 0.0192245566201794\n",
      "Epoch: 16, Batch: 890, Avg. Loss: 0.019217090507261225\n",
      "Epoch: 16, Batch: 900, Avg. Loss: 0.01920974864488714\n",
      "Epoch: 16, Batch: 910, Avg. Loss: 0.019202362654768677\n",
      "Epoch: 16, Batch: 920, Avg. Loss: 0.019194883894861847\n",
      "Epoch: 16, Batch: 930, Avg. Loss: 0.019187330626778742\n",
      "Epoch: 16, Batch: 940, Avg. Loss: 0.01917985983141774\n",
      "Epoch: 16, Batch: 950, Avg. Loss: 0.01917232992933171\n",
      "Epoch: 16, Batch: 960, Avg. Loss: 0.019164691266082196\n",
      "Epoch: 16, Batch: 970, Avg. Loss: 0.019157245288270622\n",
      "Epoch: 16, Batch: 980, Avg. Loss: 0.019150144189319436\n",
      "Epoch: 16, Batch: 990, Avg. Loss: 0.019143131954538612\n",
      "Epoch: 16, Batch: 1000, Avg. Loss: 0.01913680422732535\n",
      "Epoch: 16, Batch: 1010, Avg. Loss: 0.01912957306646254\n",
      "Epoch: 16, Batch: 1020, Avg. Loss: 0.01912275382325037\n",
      "Epoch: 16, Batch: 1030, Avg. Loss: 0.01911657001520831\n",
      "Epoch: 16, Batch: 1040, Avg. Loss: 0.01910984424115923\n",
      "Epoch: 16, Batch: 1050, Avg. Loss: 0.01910490044335828\n",
      "Epoch: 16, Batch: 1060, Avg. Loss: 0.019099070110663228\n",
      "Epoch: 16, Batch: 1070, Avg. Loss: 0.019093348983196756\n",
      "Epoch: 16, Batch: 1080, Avg. Loss: 0.019088608464827943\n",
      "Epoch: 16, Batch: 1090, Avg. Loss: 0.019084185358968463\n",
      "Epoch: 16, Batch: 1100, Avg. Loss: 0.019078464897257088\n",
      "Epoch: 16, Batch: 1110, Avg. Loss: 0.01907305844957915\n",
      "Epoch: 16, Batch: 1120, Avg. Loss: 0.019067068799381925\n",
      "Epoch: 16, Batch: 1130, Avg. Loss: 0.019061570526271783\n",
      "Epoch: 16, Batch: 1140, Avg. Loss: 0.01905686329810681\n",
      "Epoch: 16, Batch: 1150, Avg. Loss: 0.019050927853081833\n",
      "Epoch: 16, Batch: 1160, Avg. Loss: 0.019045365216656028\n",
      "Epoch: 16, Batch: 1170, Avg. Loss: 0.019039730393811172\n",
      "Epoch: 16, Batch: 1180, Avg. Loss: 0.019035188395688032\n",
      "Epoch: 16, Batch: 1190, Avg. Loss: 0.019029315288283086\n",
      "Epoch: 16, Batch: 1200, Avg. Loss: 0.01902373515289313\n",
      "Epoch: 16, Batch: 1210, Avg. Loss: 0.01901749758652377\n",
      "Epoch: 16, Batch: 1220, Avg. Loss: 0.01901144627776207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Batch: 1230, Avg. Loss: 0.019006926698359974\n",
      "Epoch: 16, Batch: 1240, Avg. Loss: 0.019001070491709793\n",
      "Epoch: 16, Batch: 1250, Avg. Loss: 0.01899589507954012\n",
      "Epoch: 16, Batch: 1260, Avg. Loss: 0.01899099474808151\n",
      "Epoch: 16, Batch: 1270, Avg. Loss: 0.018986190289114628\n",
      "Epoch: 16, Batch: 1280, Avg. Loss: 0.018982021601865916\n",
      "Epoch: 16, Batch: 1290, Avg. Loss: 0.018976739490496664\n",
      "Epoch: 16, Batch: 1300, Avg. Loss: 0.01897166338526325\n",
      "Epoch: 16, Batch: 1310, Avg. Loss: 0.01896597401082994\n",
      "Epoch: 16, Batch: 1320, Avg. Loss: 0.018960355285164703\n",
      "Epoch: 16, Batch: 1330, Avg. Loss: 0.018953979897206256\n",
      "Epoch: 16, Batch: 1340, Avg. Loss: 0.018947628839605424\n",
      "Epoch: 16, Batch: 1350, Avg. Loss: 0.01894172796334122\n",
      "Epoch: 16, Batch: 1360, Avg. Loss: 0.01893580395375267\n",
      "Epoch: 16, Batch: 1370, Avg. Loss: 0.01892947294341508\n",
      "Epoch: 16, Batch: 1380, Avg. Loss: 0.018923089920822582\n",
      "Epoch: 16, Batch: 1390, Avg. Loss: 0.018916533609871784\n",
      "Epoch: 16, Batch: 1400, Avg. Loss: 0.018910485113693268\n",
      "Epoch: 16, Batch: 1410, Avg. Loss: 0.018904513944352472\n",
      "Epoch: 16, Batch: 1420, Avg. Loss: 0.018898168177054545\n",
      "Epoch: 16, Batch: 1430, Avg. Loss: 0.018892058235552057\n",
      "Epoch: 16, Batch: 1440, Avg. Loss: 0.018887913787193608\n",
      "Epoch: 16, Batch: 1450, Avg. Loss: 0.018881720275456888\n",
      "Epoch: 16, Batch: 1460, Avg. Loss: 0.018875310059383758\n",
      "Epoch: 16, Batch: 1470, Avg. Loss: 0.01886995810461461\n",
      "Epoch: 16, Batch: 1480, Avg. Loss: 0.018864411805962936\n",
      "Epoch: 16, Batch: 1490, Avg. Loss: 0.01885842102661625\n",
      "Epoch: 16, Batch: 1500, Avg. Loss: 0.018852968370637404\n",
      "Epoch: 16, Batch: 1510, Avg. Loss: 0.018846886030131835\n",
      "Epoch: 16, Batch: 1520, Avg. Loss: 0.018840588360353645\n",
      "Epoch: 16, Batch: 1530, Avg. Loss: 0.01883387869863264\n",
      "Epoch: 16, Batch: 1540, Avg. Loss: 0.018827098368509\n",
      "Epoch: 17, Batch: 10, Avg. Loss: 0.018816738451366135\n",
      "Epoch: 17, Batch: 20, Avg. Loss: 0.0188100161635303\n",
      "Epoch: 17, Batch: 30, Avg. Loss: 0.018803331024081056\n",
      "Epoch: 17, Batch: 40, Avg. Loss: 0.018796396182834026\n",
      "Epoch: 17, Batch: 50, Avg. Loss: 0.018789336920799662\n",
      "Epoch: 17, Batch: 60, Avg. Loss: 0.018782529669972882\n",
      "Epoch: 17, Batch: 70, Avg. Loss: 0.018775489366268095\n",
      "Epoch: 17, Batch: 80, Avg. Loss: 0.018768838940623832\n",
      "Epoch: 17, Batch: 90, Avg. Loss: 0.01876195097102503\n",
      "Epoch: 17, Batch: 100, Avg. Loss: 0.018754898558542288\n",
      "Epoch: 17, Batch: 110, Avg. Loss: 0.01874788517197087\n",
      "Epoch: 17, Batch: 120, Avg. Loss: 0.01874079464161732\n",
      "Epoch: 17, Batch: 130, Avg. Loss: 0.01873369195431739\n",
      "Epoch: 17, Batch: 140, Avg. Loss: 0.0187267154373975\n",
      "Epoch: 17, Batch: 150, Avg. Loss: 0.018719697267241445\n",
      "Epoch: 17, Batch: 160, Avg. Loss: 0.01871264307548172\n",
      "Epoch: 17, Batch: 170, Avg. Loss: 0.01870559975061277\n",
      "Epoch: 17, Batch: 180, Avg. Loss: 0.018698440160802875\n",
      "Epoch: 17, Batch: 190, Avg. Loss: 0.018691497177605817\n",
      "Epoch: 17, Batch: 200, Avg. Loss: 0.018684397575240794\n",
      "Epoch: 17, Batch: 210, Avg. Loss: 0.018677319821889114\n",
      "Epoch: 17, Batch: 220, Avg. Loss: 0.018670266241393032\n",
      "Epoch: 17, Batch: 230, Avg. Loss: 0.01866347116600325\n",
      "Epoch: 17, Batch: 240, Avg. Loss: 0.01865651263218318\n",
      "Epoch: 17, Batch: 250, Avg. Loss: 0.018649546643506376\n",
      "Epoch: 17, Batch: 260, Avg. Loss: 0.01864249466947146\n",
      "Epoch: 17, Batch: 270, Avg. Loss: 0.018635629069970645\n",
      "Epoch: 17, Batch: 280, Avg. Loss: 0.018628840646105246\n",
      "Epoch: 17, Batch: 290, Avg. Loss: 0.01862182510595724\n",
      "Epoch: 17, Batch: 300, Avg. Loss: 0.018614653805771143\n",
      "Epoch: 17, Batch: 310, Avg. Loss: 0.018607555720695064\n",
      "Epoch: 17, Batch: 320, Avg. Loss: 0.018600696893294362\n",
      "Epoch: 17, Batch: 330, Avg. Loss: 0.01859364158337429\n",
      "Epoch: 17, Batch: 340, Avg. Loss: 0.01858659639114357\n",
      "Epoch: 17, Batch: 350, Avg. Loss: 0.018579531671734914\n",
      "Epoch: 17, Batch: 360, Avg. Loss: 0.018572542523382536\n",
      "Epoch: 17, Batch: 370, Avg. Loss: 0.018565467375551956\n",
      "Epoch: 17, Batch: 380, Avg. Loss: 0.018558569673436796\n",
      "Epoch: 17, Batch: 390, Avg. Loss: 0.018551626234116096\n",
      "Epoch: 17, Batch: 400, Avg. Loss: 0.018544637297162443\n",
      "Epoch: 17, Batch: 410, Avg. Loss: 0.018537596854828312\n",
      "Epoch: 17, Batch: 420, Avg. Loss: 0.018530763376628814\n",
      "Epoch: 17, Batch: 430, Avg. Loss: 0.018523784957209202\n",
      "Epoch: 17, Batch: 440, Avg. Loss: 0.01851694429984368\n",
      "Epoch: 17, Batch: 450, Avg. Loss: 0.01851009913603798\n",
      "Epoch: 17, Batch: 460, Avg. Loss: 0.018503178158209135\n",
      "Epoch: 17, Batch: 470, Avg. Loss: 0.018496440907412717\n",
      "Epoch: 17, Batch: 480, Avg. Loss: 0.01848940002850772\n",
      "Epoch: 17, Batch: 490, Avg. Loss: 0.018482368641182904\n",
      "Epoch: 17, Batch: 500, Avg. Loss: 0.018475356149758548\n",
      "Epoch: 17, Batch: 510, Avg. Loss: 0.018468416672154587\n",
      "Epoch: 17, Batch: 520, Avg. Loss: 0.01846146974600943\n",
      "Epoch: 17, Batch: 530, Avg. Loss: 0.018454438144682637\n",
      "Epoch: 17, Batch: 540, Avg. Loss: 0.018447514359848934\n",
      "Epoch: 17, Batch: 550, Avg. Loss: 0.018440497287985712\n",
      "Epoch: 17, Batch: 560, Avg. Loss: 0.018433465831960802\n",
      "Epoch: 17, Batch: 570, Avg. Loss: 0.018426546883353725\n",
      "Epoch: 17, Batch: 580, Avg. Loss: 0.018419805373058828\n",
      "Epoch: 17, Batch: 590, Avg. Loss: 0.018413077823076256\n",
      "Epoch: 17, Batch: 600, Avg. Loss: 0.018406223327801688\n",
      "Epoch: 17, Batch: 610, Avg. Loss: 0.018399375434246516\n",
      "Epoch: 17, Batch: 620, Avg. Loss: 0.018392581177193206\n",
      "Epoch: 17, Batch: 630, Avg. Loss: 0.01838575725416422\n",
      "Epoch: 17, Batch: 640, Avg. Loss: 0.018378861389890214\n",
      "Epoch: 17, Batch: 650, Avg. Loss: 0.018371898665733136\n",
      "Epoch: 17, Batch: 660, Avg. Loss: 0.01836536189541656\n",
      "Epoch: 17, Batch: 670, Avg. Loss: 0.01835859811483333\n",
      "Epoch: 17, Batch: 680, Avg. Loss: 0.018351722051487402\n",
      "Epoch: 17, Batch: 690, Avg. Loss: 0.018346608562540122\n",
      "Epoch: 17, Batch: 700, Avg. Loss: 0.018340319437244408\n",
      "Epoch: 17, Batch: 710, Avg. Loss: 0.018333987150077103\n",
      "Epoch: 17, Batch: 720, Avg. Loss: 0.018327369903129852\n",
      "Epoch: 17, Batch: 730, Avg. Loss: 0.01832074441888824\n",
      "Epoch: 17, Batch: 740, Avg. Loss: 0.0183144879149183\n",
      "Epoch: 17, Batch: 750, Avg. Loss: 0.01830765450518006\n",
      "Epoch: 17, Batch: 760, Avg. Loss: 0.018301071012906567\n",
      "Epoch: 17, Batch: 770, Avg. Loss: 0.018294339525306173\n",
      "Epoch: 17, Batch: 780, Avg. Loss: 0.018288097076251723\n",
      "Epoch: 17, Batch: 790, Avg. Loss: 0.01828128311027345\n",
      "Epoch: 17, Batch: 800, Avg. Loss: 0.018274503643194794\n",
      "Epoch: 17, Batch: 810, Avg. Loss: 0.018268031156330435\n",
      "Epoch: 17, Batch: 820, Avg. Loss: 0.01826123127803793\n",
      "Epoch: 17, Batch: 830, Avg. Loss: 0.018254582502555843\n",
      "Epoch: 17, Batch: 840, Avg. Loss: 0.018248017206898448\n",
      "Epoch: 17, Batch: 850, Avg. Loss: 0.018241363674913125\n",
      "Epoch: 17, Batch: 860, Avg. Loss: 0.01823489313012669\n",
      "Epoch: 17, Batch: 870, Avg. Loss: 0.01822829567637919\n",
      "Epoch: 17, Batch: 880, Avg. Loss: 0.0182216832382683\n",
      "Epoch: 17, Batch: 890, Avg. Loss: 0.018215050470626847\n",
      "Epoch: 17, Batch: 900, Avg. Loss: 0.018208433511271275\n",
      "Epoch: 17, Batch: 910, Avg. Loss: 0.018201733253839537\n",
      "Epoch: 17, Batch: 920, Avg. Loss: 0.018195016642177434\n",
      "Epoch: 17, Batch: 930, Avg. Loss: 0.01818832173843411\n",
      "Epoch: 17, Batch: 940, Avg. Loss: 0.01818167173616523\n",
      "Epoch: 17, Batch: 950, Avg. Loss: 0.018174945208942608\n",
      "Epoch: 17, Batch: 960, Avg. Loss: 0.018168236356955126\n",
      "Epoch: 17, Batch: 970, Avg. Loss: 0.01816161894483593\n",
      "Epoch: 17, Batch: 980, Avg. Loss: 0.01815507543790292\n",
      "Epoch: 17, Batch: 990, Avg. Loss: 0.018148749675900397\n",
      "Epoch: 17, Batch: 1000, Avg. Loss: 0.018142757434306107\n",
      "Epoch: 17, Batch: 1010, Avg. Loss: 0.018136294702353572\n",
      "Epoch: 17, Batch: 1020, Avg. Loss: 0.018130098218481588\n",
      "Epoch: 17, Batch: 1030, Avg. Loss: 0.01812497829620244\n",
      "Epoch: 17, Batch: 1040, Avg. Loss: 0.01811889423338821\n",
      "Epoch: 17, Batch: 1050, Avg. Loss: 0.018113812168328525\n",
      "Epoch: 17, Batch: 1060, Avg. Loss: 0.018108297778695336\n",
      "Epoch: 17, Batch: 1070, Avg. Loss: 0.01810262219311236\n",
      "Epoch: 17, Batch: 1080, Avg. Loss: 0.01809805592832917\n",
      "Epoch: 17, Batch: 1090, Avg. Loss: 0.018093465441424986\n",
      "Epoch: 17, Batch: 1100, Avg. Loss: 0.018088711459647112\n",
      "Epoch: 17, Batch: 1110, Avg. Loss: 0.01808380325660679\n",
      "Epoch: 17, Batch: 1120, Avg. Loss: 0.01807823699893705\n",
      "Epoch: 17, Batch: 1130, Avg. Loss: 0.01807350417032511\n",
      "Epoch: 17, Batch: 1140, Avg. Loss: 0.018068793947183064\n",
      "Epoch: 17, Batch: 1150, Avg. Loss: 0.018063270393646576\n",
      "Epoch: 17, Batch: 1160, Avg. Loss: 0.018057759458291394\n",
      "Epoch: 17, Batch: 1170, Avg. Loss: 0.01805198057182115\n",
      "Epoch: 17, Batch: 1180, Avg. Loss: 0.0180465743742632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Batch: 1190, Avg. Loss: 0.018040891194769083\n",
      "Epoch: 17, Batch: 1200, Avg. Loss: 0.018035873563100872\n",
      "Epoch: 17, Batch: 1210, Avg. Loss: 0.018030360082203354\n",
      "Epoch: 17, Batch: 1220, Avg. Loss: 0.018024564346452332\n",
      "Epoch: 17, Batch: 1230, Avg. Loss: 0.01801926121913756\n",
      "Epoch: 17, Batch: 1240, Avg. Loss: 0.0180137346064783\n",
      "Epoch: 17, Batch: 1250, Avg. Loss: 0.018008067155478095\n",
      "Epoch: 17, Batch: 1260, Avg. Loss: 0.01800286641932573\n",
      "Epoch: 17, Batch: 1270, Avg. Loss: 0.01799787010340048\n",
      "Epoch: 17, Batch: 1280, Avg. Loss: 0.01799311501966942\n",
      "Epoch: 17, Batch: 1290, Avg. Loss: 0.01798806663172044\n",
      "Epoch: 17, Batch: 1300, Avg. Loss: 0.017982898605591702\n",
      "Epoch: 17, Batch: 1310, Avg. Loss: 0.017977340205679017\n",
      "Epoch: 17, Batch: 1320, Avg. Loss: 0.017972394389363898\n",
      "Epoch: 17, Batch: 1330, Avg. Loss: 0.01796674778649109\n",
      "Epoch: 17, Batch: 1340, Avg. Loss: 0.017960781638976187\n",
      "Epoch: 17, Batch: 1350, Avg. Loss: 0.01795522184302414\n",
      "Epoch: 17, Batch: 1360, Avg. Loss: 0.017949314372486702\n",
      "Epoch: 17, Batch: 1370, Avg. Loss: 0.017943356814667147\n",
      "Epoch: 17, Batch: 1380, Avg. Loss: 0.017937472053600138\n",
      "Epoch: 17, Batch: 1390, Avg. Loss: 0.01793156253263292\n",
      "Epoch: 17, Batch: 1400, Avg. Loss: 0.017925920090122283\n",
      "Epoch: 17, Batch: 1410, Avg. Loss: 0.017920358589364846\n",
      "Epoch: 17, Batch: 1420, Avg. Loss: 0.01791441894766717\n",
      "Epoch: 17, Batch: 1430, Avg. Loss: 0.017908639193836586\n",
      "Epoch: 17, Batch: 1440, Avg. Loss: 0.01790462409016483\n",
      "Epoch: 17, Batch: 1450, Avg. Loss: 0.017898589911426427\n",
      "Epoch: 17, Batch: 1460, Avg. Loss: 0.017892940533566083\n",
      "Epoch: 17, Batch: 1470, Avg. Loss: 0.017887504972609717\n",
      "Epoch: 17, Batch: 1480, Avg. Loss: 0.01788197437112944\n",
      "Epoch: 17, Batch: 1490, Avg. Loss: 0.0178765555107642\n",
      "Epoch: 17, Batch: 1500, Avg. Loss: 0.017871386589740152\n",
      "Epoch: 17, Batch: 1510, Avg. Loss: 0.017865613287159167\n",
      "Epoch: 17, Batch: 1520, Avg. Loss: 0.017859447045995135\n",
      "Epoch: 17, Batch: 1530, Avg. Loss: 0.017853228746062898\n",
      "Epoch: 17, Batch: 1540, Avg. Loss: 0.017847127754267368\n",
      "Epoch: 18, Batch: 10, Avg. Loss: 0.017837718542086812\n",
      "Epoch: 18, Batch: 20, Avg. Loss: 0.01783154002168612\n",
      "Epoch: 18, Batch: 30, Avg. Loss: 0.0178254425234346\n",
      "Epoch: 18, Batch: 40, Avg. Loss: 0.01781913006436861\n",
      "Epoch: 18, Batch: 50, Avg. Loss: 0.01781281009027031\n",
      "Epoch: 18, Batch: 60, Avg. Loss: 0.01780671297165442\n",
      "Epoch: 18, Batch: 70, Avg. Loss: 0.017800464525346626\n",
      "Epoch: 18, Batch: 80, Avg. Loss: 0.01779436857058345\n",
      "Epoch: 18, Batch: 90, Avg. Loss: 0.017788217227040673\n",
      "Epoch: 18, Batch: 100, Avg. Loss: 0.017781863118715276\n",
      "Epoch: 18, Batch: 110, Avg. Loss: 0.017775680226112596\n",
      "Epoch: 18, Batch: 120, Avg. Loss: 0.017769405298773043\n",
      "Epoch: 18, Batch: 130, Avg. Loss: 0.01776301738840353\n",
      "Epoch: 18, Batch: 140, Avg. Loss: 0.01775670599097829\n",
      "Epoch: 18, Batch: 150, Avg. Loss: 0.017750411227872933\n",
      "Epoch: 18, Batch: 160, Avg. Loss: 0.0177440611775846\n",
      "Epoch: 18, Batch: 170, Avg. Loss: 0.017737786003365244\n",
      "Epoch: 18, Batch: 180, Avg. Loss: 0.017731374465780717\n",
      "Epoch: 18, Batch: 190, Avg. Loss: 0.01772499614860638\n",
      "Epoch: 18, Batch: 200, Avg. Loss: 0.017718676024691273\n",
      "Epoch: 18, Batch: 210, Avg. Loss: 0.01771234633072137\n",
      "Epoch: 18, Batch: 220, Avg. Loss: 0.01770616876222021\n",
      "Epoch: 18, Batch: 230, Avg. Loss: 0.017699872882566338\n",
      "Epoch: 18, Batch: 240, Avg. Loss: 0.017693549305769586\n",
      "Epoch: 18, Batch: 250, Avg. Loss: 0.01768725901452371\n",
      "Epoch: 18, Batch: 260, Avg. Loss: 0.017680949331242663\n",
      "Epoch: 18, Batch: 270, Avg. Loss: 0.01767472534895846\n",
      "Epoch: 18, Batch: 280, Avg. Loss: 0.01766857575461202\n",
      "Epoch: 18, Batch: 290, Avg. Loss: 0.017662273816876857\n",
      "Epoch: 18, Batch: 300, Avg. Loss: 0.017655961005191026\n",
      "Epoch: 18, Batch: 310, Avg. Loss: 0.017649527111212322\n",
      "Epoch: 18, Batch: 320, Avg. Loss: 0.017643170312229384\n",
      "Epoch: 18, Batch: 330, Avg. Loss: 0.017636786759555493\n",
      "Epoch: 18, Batch: 340, Avg. Loss: 0.017630395948236937\n",
      "Epoch: 18, Batch: 350, Avg. Loss: 0.017624046688821483\n",
      "Epoch: 18, Batch: 360, Avg. Loss: 0.017617729282233534\n",
      "Epoch: 18, Batch: 370, Avg. Loss: 0.01761135065556268\n",
      "Epoch: 18, Batch: 380, Avg. Loss: 0.017605075091118253\n",
      "Epoch: 18, Batch: 390, Avg. Loss: 0.017598855573935996\n",
      "Epoch: 18, Batch: 400, Avg. Loss: 0.017592546773540482\n",
      "Epoch: 18, Batch: 410, Avg. Loss: 0.01758619359755475\n",
      "Epoch: 18, Batch: 420, Avg. Loss: 0.017579942324458652\n",
      "Epoch: 18, Batch: 430, Avg. Loss: 0.017573717895470506\n",
      "Epoch: 18, Batch: 440, Avg. Loss: 0.01756754827889356\n",
      "Epoch: 18, Batch: 450, Avg. Loss: 0.017561278457546063\n",
      "Epoch: 18, Batch: 460, Avg. Loss: 0.01755499713676365\n",
      "Epoch: 18, Batch: 470, Avg. Loss: 0.01754886119101323\n",
      "Epoch: 18, Batch: 480, Avg. Loss: 0.017542519970000774\n",
      "Epoch: 18, Batch: 490, Avg. Loss: 0.017536189882542175\n",
      "Epoch: 18, Batch: 500, Avg. Loss: 0.017529900899865076\n",
      "Epoch: 18, Batch: 510, Avg. Loss: 0.017523711666344834\n",
      "Epoch: 18, Batch: 520, Avg. Loss: 0.017517539285949096\n",
      "Epoch: 18, Batch: 530, Avg. Loss: 0.01751132184644043\n",
      "Epoch: 18, Batch: 540, Avg. Loss: 0.01750508809440125\n",
      "Epoch: 18, Batch: 550, Avg. Loss: 0.017498827640559524\n",
      "Epoch: 18, Batch: 560, Avg. Loss: 0.01749259304249162\n",
      "Epoch: 18, Batch: 570, Avg. Loss: 0.01748628040223535\n",
      "Epoch: 18, Batch: 580, Avg. Loss: 0.017480164313036905\n",
      "Epoch: 18, Batch: 590, Avg. Loss: 0.01747413574908555\n",
      "Epoch: 18, Batch: 600, Avg. Loss: 0.017467985974069955\n",
      "Epoch: 18, Batch: 610, Avg. Loss: 0.017461877276272977\n",
      "Epoch: 18, Batch: 620, Avg. Loss: 0.017455750086255432\n",
      "Epoch: 18, Batch: 630, Avg. Loss: 0.01744980600276487\n",
      "Epoch: 18, Batch: 640, Avg. Loss: 0.017443650396397878\n",
      "Epoch: 18, Batch: 650, Avg. Loss: 0.017437567808737654\n",
      "Epoch: 18, Batch: 660, Avg. Loss: 0.017431610949910643\n",
      "Epoch: 18, Batch: 670, Avg. Loss: 0.017425548284985645\n",
      "Epoch: 18, Batch: 680, Avg. Loss: 0.0174194538073768\n",
      "Epoch: 18, Batch: 690, Avg. Loss: 0.017415216053071766\n",
      "Epoch: 18, Batch: 700, Avg. Loss: 0.01740946844788539\n",
      "Epoch: 18, Batch: 710, Avg. Loss: 0.017404254605869376\n",
      "Epoch: 18, Batch: 720, Avg. Loss: 0.01739831247852373\n",
      "Epoch: 18, Batch: 730, Avg. Loss: 0.017392667166865942\n",
      "Epoch: 18, Batch: 740, Avg. Loss: 0.017387031084833634\n",
      "Epoch: 18, Batch: 750, Avg. Loss: 0.01738143730838605\n",
      "Epoch: 18, Batch: 760, Avg. Loss: 0.01737544132446441\n",
      "Epoch: 18, Batch: 770, Avg. Loss: 0.017369541670364284\n",
      "Epoch: 18, Batch: 780, Avg. Loss: 0.017363948180403436\n",
      "Epoch: 18, Batch: 790, Avg. Loss: 0.017357989209819524\n",
      "Epoch: 18, Batch: 800, Avg. Loss: 0.01735214223874666\n",
      "Epoch: 18, Batch: 810, Avg. Loss: 0.01734615322115168\n",
      "Epoch: 18, Batch: 820, Avg. Loss: 0.017340122444047354\n",
      "Epoch: 18, Batch: 830, Avg. Loss: 0.017334076564113433\n",
      "Epoch: 18, Batch: 840, Avg. Loss: 0.0173281672465945\n",
      "Epoch: 18, Batch: 850, Avg. Loss: 0.017322144458655764\n",
      "Epoch: 18, Batch: 860, Avg. Loss: 0.017316256413329605\n",
      "Epoch: 18, Batch: 870, Avg. Loss: 0.01731036549655907\n",
      "Epoch: 18, Batch: 880, Avg. Loss: 0.017304420134026943\n",
      "Epoch: 18, Batch: 890, Avg. Loss: 0.01729853581122806\n",
      "Epoch: 18, Batch: 900, Avg. Loss: 0.017292666817640613\n",
      "Epoch: 18, Batch: 910, Avg. Loss: 0.017286776289523815\n",
      "Epoch: 18, Batch: 920, Avg. Loss: 0.017280800518802108\n",
      "Epoch: 18, Batch: 930, Avg. Loss: 0.017274897207620418\n",
      "Epoch: 18, Batch: 940, Avg. Loss: 0.01726892922471299\n",
      "Epoch: 18, Batch: 950, Avg. Loss: 0.017262898329644883\n",
      "Epoch: 18, Batch: 960, Avg. Loss: 0.017256912780512945\n",
      "Epoch: 18, Batch: 970, Avg. Loss: 0.01725101401055887\n",
      "Epoch: 18, Batch: 980, Avg. Loss: 0.017245138807871457\n",
      "Epoch: 18, Batch: 990, Avg. Loss: 0.01723931474680775\n",
      "Epoch: 18, Batch: 1000, Avg. Loss: 0.017233965208998142\n",
      "Epoch: 18, Batch: 1010, Avg. Loss: 0.017228222710890476\n",
      "Epoch: 18, Batch: 1020, Avg. Loss: 0.017222899248913574\n",
      "Epoch: 18, Batch: 1030, Avg. Loss: 0.01721816219406408\n",
      "Epoch: 18, Batch: 1040, Avg. Loss: 0.01721252869846686\n",
      "Epoch: 18, Batch: 1050, Avg. Loss: 0.01720775946337786\n",
      "Epoch: 18, Batch: 1060, Avg. Loss: 0.017202558637907102\n",
      "Epoch: 18, Batch: 1070, Avg. Loss: 0.017197810136493033\n",
      "Epoch: 18, Batch: 1080, Avg. Loss: 0.01719315373773386\n",
      "Epoch: 18, Batch: 1090, Avg. Loss: 0.01718942809798794\n",
      "Epoch: 18, Batch: 1100, Avg. Loss: 0.01718457979988591\n",
      "Epoch: 18, Batch: 1110, Avg. Loss: 0.01717979973459403\n",
      "Epoch: 18, Batch: 1120, Avg. Loss: 0.017174638248303672\n",
      "Epoch: 18, Batch: 1130, Avg. Loss: 0.017169800435833948\n",
      "Epoch: 18, Batch: 1140, Avg. Loss: 0.01716499724409837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Batch: 1150, Avg. Loss: 0.017159621120130534\n",
      "Epoch: 18, Batch: 1160, Avg. Loss: 0.01715465763457933\n",
      "Epoch: 18, Batch: 1170, Avg. Loss: 0.017149144658010842\n",
      "Epoch: 18, Batch: 1180, Avg. Loss: 0.017144005057607273\n",
      "Epoch: 18, Batch: 1190, Avg. Loss: 0.017138773218589365\n",
      "Epoch: 18, Batch: 1200, Avg. Loss: 0.017133902747866813\n",
      "Epoch: 18, Batch: 1210, Avg. Loss: 0.01712873817937296\n",
      "Epoch: 18, Batch: 1220, Avg. Loss: 0.017123230567656713\n",
      "Epoch: 18, Batch: 1230, Avg. Loss: 0.01711799528520763\n",
      "Epoch: 18, Batch: 1240, Avg. Loss: 0.01711245629612976\n",
      "Epoch: 18, Batch: 1250, Avg. Loss: 0.017107238980139005\n",
      "Epoch: 18, Batch: 1260, Avg. Loss: 0.017102367526199015\n",
      "Epoch: 18, Batch: 1270, Avg. Loss: 0.017097076333612928\n",
      "Epoch: 18, Batch: 1280, Avg. Loss: 0.01709228808105045\n",
      "Epoch: 18, Batch: 1290, Avg. Loss: 0.01708741972993104\n",
      "Epoch: 18, Batch: 1300, Avg. Loss: 0.01708243722621413\n",
      "Epoch: 18, Batch: 1310, Avg. Loss: 0.017077464012489774\n",
      "Epoch: 18, Batch: 1320, Avg. Loss: 0.017072825913711884\n",
      "Epoch: 18, Batch: 1330, Avg. Loss: 0.017067518722150468\n",
      "Epoch: 18, Batch: 1340, Avg. Loss: 0.01706216988340519\n",
      "Epoch: 18, Batch: 1350, Avg. Loss: 0.01705720953942696\n",
      "Epoch: 18, Batch: 1360, Avg. Loss: 0.01705196614251507\n",
      "Epoch: 18, Batch: 1370, Avg. Loss: 0.017046669215899545\n",
      "Epoch: 18, Batch: 1380, Avg. Loss: 0.01704154826057581\n",
      "Epoch: 18, Batch: 1390, Avg. Loss: 0.01703605815516496\n",
      "Epoch: 18, Batch: 1400, Avg. Loss: 0.017030918452199378\n",
      "Epoch: 18, Batch: 1410, Avg. Loss: 0.017025853100637364\n",
      "Epoch: 18, Batch: 1420, Avg. Loss: 0.017020492979287042\n",
      "Epoch: 18, Batch: 1430, Avg. Loss: 0.0170150529951852\n",
      "Epoch: 18, Batch: 1440, Avg. Loss: 0.01701122011193812\n",
      "Epoch: 18, Batch: 1450, Avg. Loss: 0.01700600154194653\n",
      "Epoch: 18, Batch: 1460, Avg. Loss: 0.01700056873910692\n",
      "Epoch: 18, Batch: 1470, Avg. Loss: 0.016995526256140855\n",
      "Epoch: 18, Batch: 1480, Avg. Loss: 0.016990339859747447\n",
      "Epoch: 18, Batch: 1490, Avg. Loss: 0.016985402518354002\n",
      "Epoch: 18, Batch: 1500, Avg. Loss: 0.01698063019501838\n",
      "Epoch: 18, Batch: 1510, Avg. Loss: 0.01697525556198683\n",
      "Epoch: 18, Batch: 1520, Avg. Loss: 0.01696976863035587\n",
      "Epoch: 18, Batch: 1530, Avg. Loss: 0.016964346767152833\n",
      "Epoch: 18, Batch: 1540, Avg. Loss: 0.016958801974884016\n",
      "Epoch: 19, Batch: 10, Avg. Loss: 0.016950274173458395\n",
      "Epoch: 19, Batch: 20, Avg. Loss: 0.01694477982555268\n",
      "Epoch: 19, Batch: 30, Avg. Loss: 0.016939255155182412\n",
      "Epoch: 19, Batch: 40, Avg. Loss: 0.016933588498905375\n",
      "Epoch: 19, Batch: 50, Avg. Loss: 0.01692787418497968\n",
      "Epoch: 19, Batch: 60, Avg. Loss: 0.016922144084046197\n",
      "Epoch: 19, Batch: 70, Avg. Loss: 0.016916440025853234\n",
      "Epoch: 19, Batch: 80, Avg. Loss: 0.01691084627622417\n",
      "Epoch: 19, Batch: 90, Avg. Loss: 0.016905256538593604\n",
      "Epoch: 19, Batch: 100, Avg. Loss: 0.016899501139293713\n",
      "Epoch: 19, Batch: 110, Avg. Loss: 0.016893842331445547\n",
      "Epoch: 19, Batch: 120, Avg. Loss: 0.01688805776210859\n",
      "Epoch: 19, Batch: 130, Avg. Loss: 0.016882414757609758\n",
      "Epoch: 19, Batch: 140, Avg. Loss: 0.016876776928052502\n",
      "Epoch: 19, Batch: 150, Avg. Loss: 0.01687107768319106\n",
      "Epoch: 19, Batch: 160, Avg. Loss: 0.016865379327062082\n",
      "Epoch: 19, Batch: 170, Avg. Loss: 0.016859690231534025\n",
      "Epoch: 19, Batch: 180, Avg. Loss: 0.016853908504807495\n",
      "Epoch: 19, Batch: 190, Avg. Loss: 0.016848130179466993\n",
      "Epoch: 19, Batch: 200, Avg. Loss: 0.016842372725108686\n",
      "Epoch: 19, Batch: 210, Avg. Loss: 0.01683666206414944\n",
      "Epoch: 19, Batch: 220, Avg. Loss: 0.016831009124198926\n",
      "Epoch: 19, Batch: 230, Avg. Loss: 0.016825315446769077\n",
      "Epoch: 19, Batch: 240, Avg. Loss: 0.01681972252622097\n",
      "Epoch: 19, Batch: 250, Avg. Loss: 0.016814062109059332\n",
      "Epoch: 19, Batch: 260, Avg. Loss: 0.016808355441346682\n",
      "Epoch: 19, Batch: 270, Avg. Loss: 0.01680271523204618\n",
      "Epoch: 19, Batch: 280, Avg. Loss: 0.016797076653696418\n",
      "Epoch: 19, Batch: 290, Avg. Loss: 0.01679138696731613\n",
      "Epoch: 19, Batch: 300, Avg. Loss: 0.016785634557741912\n",
      "Epoch: 19, Batch: 310, Avg. Loss: 0.016779881027792546\n",
      "Epoch: 19, Batch: 320, Avg. Loss: 0.01677429403815202\n",
      "Epoch: 19, Batch: 330, Avg. Loss: 0.016768572333957597\n",
      "Epoch: 19, Batch: 340, Avg. Loss: 0.0167628453463358\n",
      "Epoch: 19, Batch: 350, Avg. Loss: 0.016757147769058005\n",
      "Epoch: 19, Batch: 360, Avg. Loss: 0.01675152245117955\n",
      "Epoch: 19, Batch: 370, Avg. Loss: 0.01674580806116475\n",
      "Epoch: 19, Batch: 380, Avg. Loss: 0.016740163253541517\n",
      "Epoch: 19, Batch: 390, Avg. Loss: 0.016734523900346868\n",
      "Epoch: 19, Batch: 400, Avg. Loss: 0.016728837270707328\n",
      "Epoch: 19, Batch: 410, Avg. Loss: 0.016723135896410033\n",
      "Epoch: 19, Batch: 420, Avg. Loss: 0.016717571619860497\n",
      "Epoch: 19, Batch: 430, Avg. Loss: 0.016711910864326954\n",
      "Epoch: 19, Batch: 440, Avg. Loss: 0.016706309500736906\n",
      "Epoch: 19, Batch: 450, Avg. Loss: 0.016700700749624476\n",
      "Epoch: 19, Batch: 460, Avg. Loss: 0.01669505165968056\n",
      "Epoch: 19, Batch: 470, Avg. Loss: 0.016689482164457006\n",
      "Epoch: 19, Batch: 480, Avg. Loss: 0.016683796677928714\n",
      "Epoch: 19, Batch: 490, Avg. Loss: 0.01667808897761594\n",
      "Epoch: 19, Batch: 500, Avg. Loss: 0.016672453690646558\n",
      "Epoch: 19, Batch: 510, Avg. Loss: 0.016666812471986658\n",
      "Epoch: 19, Batch: 520, Avg. Loss: 0.016661165459496564\n",
      "Epoch: 19, Batch: 530, Avg. Loss: 0.016655482414433688\n",
      "Epoch: 19, Batch: 540, Avg. Loss: 0.01664982471758811\n",
      "Epoch: 19, Batch: 550, Avg. Loss: 0.016644136692975514\n",
      "Epoch: 19, Batch: 560, Avg. Loss: 0.01663848539375859\n",
      "Epoch: 19, Batch: 570, Avg. Loss: 0.016632871789897023\n",
      "Epoch: 19, Batch: 580, Avg. Loss: 0.016627393356874177\n",
      "Epoch: 19, Batch: 590, Avg. Loss: 0.016621916053300056\n",
      "Epoch: 19, Batch: 600, Avg. Loss: 0.016616365194562187\n",
      "Epoch: 19, Batch: 610, Avg. Loss: 0.016610909494283434\n",
      "Epoch: 19, Batch: 620, Avg. Loss: 0.016605424989744773\n",
      "Epoch: 19, Batch: 630, Avg. Loss: 0.01660018138570348\n",
      "Epoch: 19, Batch: 640, Avg. Loss: 0.016594635499828536\n",
      "Epoch: 19, Batch: 650, Avg. Loss: 0.016589043497268613\n",
      "Epoch: 19, Batch: 660, Avg. Loss: 0.016583645227944574\n",
      "Epoch: 19, Batch: 670, Avg. Loss: 0.016578151497073576\n",
      "Epoch: 19, Batch: 680, Avg. Loss: 0.016572598133319835\n",
      "Epoch: 19, Batch: 690, Avg. Loss: 0.016567504284348847\n",
      "Epoch: 19, Batch: 700, Avg. Loss: 0.016562169027240843\n",
      "Epoch: 19, Batch: 710, Avg. Loss: 0.016556866663575098\n",
      "Epoch: 19, Batch: 720, Avg. Loss: 0.016551399679018693\n",
      "Epoch: 19, Batch: 730, Avg. Loss: 0.01654591043971816\n",
      "Epoch: 19, Batch: 740, Avg. Loss: 0.016540501558542215\n",
      "Epoch: 19, Batch: 750, Avg. Loss: 0.016534998379715986\n",
      "Epoch: 19, Batch: 760, Avg. Loss: 0.016529555589535647\n",
      "Epoch: 19, Batch: 770, Avg. Loss: 0.01652410539034558\n",
      "Epoch: 19, Batch: 780, Avg. Loss: 0.01651886696730806\n",
      "Epoch: 19, Batch: 790, Avg. Loss: 0.016513372089453568\n",
      "Epoch: 19, Batch: 800, Avg. Loss: 0.016507876270928656\n",
      "Epoch: 19, Batch: 810, Avg. Loss: 0.016502398874230755\n",
      "Epoch: 19, Batch: 820, Avg. Loss: 0.016496931028058246\n",
      "Epoch: 19, Batch: 830, Avg. Loss: 0.016491518095442113\n",
      "Epoch: 19, Batch: 840, Avg. Loss: 0.016486225301092877\n",
      "Epoch: 19, Batch: 850, Avg. Loss: 0.016480748976859295\n",
      "Epoch: 19, Batch: 860, Avg. Loss: 0.016475409747946174\n",
      "Epoch: 19, Batch: 870, Avg. Loss: 0.016470047798886656\n",
      "Epoch: 19, Batch: 880, Avg. Loss: 0.016464622326415348\n",
      "Epoch: 19, Batch: 890, Avg. Loss: 0.016459253985942193\n",
      "Epoch: 19, Batch: 900, Avg. Loss: 0.016453821303838212\n",
      "Epoch: 19, Batch: 910, Avg. Loss: 0.016448388826266917\n",
      "Epoch: 19, Batch: 920, Avg. Loss: 0.01644299973308042\n",
      "Epoch: 19, Batch: 930, Avg. Loss: 0.016437599471695097\n",
      "Epoch: 19, Batch: 940, Avg. Loss: 0.016432172445786434\n",
      "Epoch: 19, Batch: 950, Avg. Loss: 0.016426737999903684\n",
      "Epoch: 19, Batch: 960, Avg. Loss: 0.016421240318347793\n",
      "Epoch: 19, Batch: 970, Avg. Loss: 0.016415792039433892\n",
      "Epoch: 19, Batch: 980, Avg. Loss: 0.016410473599871484\n",
      "Epoch: 19, Batch: 990, Avg. Loss: 0.016405165141628\n",
      "Epoch: 19, Batch: 1000, Avg. Loss: 0.016400156002631665\n",
      "Epoch: 19, Batch: 1010, Avg. Loss: 0.016394921259738217\n",
      "Epoch: 19, Batch: 1020, Avg. Loss: 0.016389915510248133\n",
      "Epoch: 19, Batch: 1030, Avg. Loss: 0.01638542932647731\n",
      "Epoch: 19, Batch: 1040, Avg. Loss: 0.016380424209891737\n",
      "Epoch: 19, Batch: 1050, Avg. Loss: 0.01637548488248611\n",
      "Epoch: 19, Batch: 1060, Avg. Loss: 0.016370742836753806\n",
      "Epoch: 19, Batch: 1070, Avg. Loss: 0.01636630828697568\n",
      "Epoch: 19, Batch: 1080, Avg. Loss: 0.016361966907052692\n",
      "Epoch: 19, Batch: 1090, Avg. Loss: 0.01635790839893947\n",
      "Epoch: 19, Batch: 1100, Avg. Loss: 0.01635310036911965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Batch: 1110, Avg. Loss: 0.016348625728008475\n",
      "Epoch: 19, Batch: 1120, Avg. Loss: 0.016343951335998884\n",
      "Epoch: 19, Batch: 1130, Avg. Loss: 0.016339377258963693\n",
      "Epoch: 19, Batch: 1140, Avg. Loss: 0.016334849294179152\n",
      "Epoch: 19, Batch: 1150, Avg. Loss: 0.01632983270676265\n",
      "Epoch: 19, Batch: 1160, Avg. Loss: 0.016325208050084198\n",
      "Epoch: 19, Batch: 1170, Avg. Loss: 0.016320562913358188\n",
      "Epoch: 19, Batch: 1180, Avg. Loss: 0.016315781733579185\n",
      "Epoch: 19, Batch: 1190, Avg. Loss: 0.0163109712457281\n",
      "Epoch: 19, Batch: 1200, Avg. Loss: 0.01630600749222503\n",
      "Epoch: 19, Batch: 1210, Avg. Loss: 0.016301131463459687\n",
      "Epoch: 19, Batch: 1220, Avg. Loss: 0.01629609170022684\n",
      "Epoch: 19, Batch: 1230, Avg. Loss: 0.01629134146582061\n",
      "Epoch: 19, Batch: 1240, Avg. Loss: 0.016286493389587257\n",
      "Epoch: 19, Batch: 1250, Avg. Loss: 0.016281805490625943\n",
      "Epoch: 19, Batch: 1260, Avg. Loss: 0.016277296953171247\n",
      "Epoch: 19, Batch: 1270, Avg. Loss: 0.016272316496964007\n",
      "Epoch: 19, Batch: 1280, Avg. Loss: 0.016267919677577434\n",
      "Epoch: 19, Batch: 1290, Avg. Loss: 0.016263275250994903\n",
      "Epoch: 19, Batch: 1300, Avg. Loss: 0.016258770400509565\n",
      "Epoch: 19, Batch: 1310, Avg. Loss: 0.016253970890832053\n",
      "Epoch: 19, Batch: 1320, Avg. Loss: 0.016249540891847405\n",
      "Epoch: 19, Batch: 1330, Avg. Loss: 0.01624484180014211\n",
      "Epoch: 19, Batch: 1340, Avg. Loss: 0.016239894693831015\n",
      "Epoch: 19, Batch: 1350, Avg. Loss: 0.016235207902193915\n",
      "Epoch: 19, Batch: 1360, Avg. Loss: 0.016230380937078976\n",
      "Epoch: 19, Batch: 1370, Avg. Loss: 0.016225420419079536\n",
      "Epoch: 19, Batch: 1380, Avg. Loss: 0.016220408803369286\n",
      "Epoch: 19, Batch: 1390, Avg. Loss: 0.01621556606540807\n",
      "Epoch: 19, Batch: 1400, Avg. Loss: 0.016210893823379757\n",
      "Epoch: 19, Batch: 1410, Avg. Loss: 0.016206137410780534\n",
      "Epoch: 19, Batch: 1420, Avg. Loss: 0.016201254221794185\n",
      "Epoch: 19, Batch: 1430, Avg. Loss: 0.01619628700623232\n",
      "Epoch: 19, Batch: 1440, Avg. Loss: 0.016191776920129682\n",
      "Epoch: 19, Batch: 1450, Avg. Loss: 0.016186938095691498\n",
      "Epoch: 19, Batch: 1460, Avg. Loss: 0.016181949919779188\n",
      "Epoch: 19, Batch: 1470, Avg. Loss: 0.01617742501046076\n",
      "Epoch: 19, Batch: 1480, Avg. Loss: 0.016172501370031355\n",
      "Epoch: 19, Batch: 1490, Avg. Loss: 0.01616784619518323\n",
      "Epoch: 19, Batch: 1500, Avg. Loss: 0.016163485570033935\n",
      "Epoch: 19, Batch: 1510, Avg. Loss: 0.01615862886181066\n",
      "Epoch: 19, Batch: 1520, Avg. Loss: 0.01615361033647894\n",
      "Epoch: 19, Batch: 1530, Avg. Loss: 0.016148676998869266\n",
      "Epoch: 19, Batch: 1540, Avg. Loss: 0.016143477678900023\n",
      "Epoch: 20, Batch: 10, Avg. Loss: 0.016135804184831674\n",
      "Epoch: 20, Batch: 20, Avg. Loss: 0.016130772169079235\n",
      "Epoch: 20, Batch: 30, Avg. Loss: 0.016125744556075135\n",
      "Epoch: 20, Batch: 40, Avg. Loss: 0.0161205649902558\n",
      "Epoch: 20, Batch: 50, Avg. Loss: 0.016115349674776572\n",
      "Epoch: 20, Batch: 60, Avg. Loss: 0.01611019299901908\n",
      "Epoch: 20, Batch: 70, Avg. Loss: 0.016104994167476298\n",
      "Epoch: 20, Batch: 80, Avg. Loss: 0.01609995536317466\n",
      "Epoch: 20, Batch: 90, Avg. Loss: 0.016094876029437582\n",
      "Epoch: 20, Batch: 100, Avg. Loss: 0.016089715855850714\n",
      "Epoch: 20, Batch: 110, Avg. Loss: 0.016084545850192002\n",
      "Epoch: 20, Batch: 120, Avg. Loss: 0.016079354256279158\n",
      "Epoch: 20, Batch: 130, Avg. Loss: 0.016074161112353966\n",
      "Epoch: 20, Batch: 140, Avg. Loss: 0.016069004156111476\n",
      "Epoch: 20, Batch: 150, Avg. Loss: 0.01606381630048839\n",
      "Epoch: 20, Batch: 160, Avg. Loss: 0.01605866510296853\n",
      "Epoch: 20, Batch: 170, Avg. Loss: 0.01605352588427515\n",
      "Epoch: 20, Batch: 180, Avg. Loss: 0.01604831386601107\n",
      "Epoch: 20, Batch: 190, Avg. Loss: 0.016043167526560615\n",
      "Epoch: 20, Batch: 200, Avg. Loss: 0.016038001650237686\n",
      "Epoch: 20, Batch: 210, Avg. Loss: 0.0160327975034158\n",
      "Epoch: 20, Batch: 220, Avg. Loss: 0.016027741121338333\n",
      "Epoch: 20, Batch: 230, Avg. Loss: 0.01602258340301998\n",
      "Epoch: 20, Batch: 240, Avg. Loss: 0.01601744384037784\n",
      "Epoch: 20, Batch: 250, Avg. Loss: 0.01601230372538439\n",
      "Epoch: 20, Batch: 260, Avg. Loss: 0.016007122853099322\n",
      "Epoch: 20, Batch: 270, Avg. Loss: 0.016002021654532725\n",
      "Epoch: 20, Batch: 280, Avg. Loss: 0.015996961911131237\n",
      "Epoch: 20, Batch: 290, Avg. Loss: 0.01599181963244961\n",
      "Epoch: 20, Batch: 300, Avg. Loss: 0.015986624523354837\n",
      "Epoch: 20, Batch: 310, Avg. Loss: 0.01598141789114515\n",
      "Epoch: 20, Batch: 320, Avg. Loss: 0.015976283043106825\n",
      "Epoch: 20, Batch: 330, Avg. Loss: 0.01597110574363399\n",
      "Epoch: 20, Batch: 340, Avg. Loss: 0.01596593648181685\n",
      "Epoch: 20, Batch: 350, Avg. Loss: 0.01596076101818883\n",
      "Epoch: 20, Batch: 360, Avg. Loss: 0.015955653343191864\n",
      "Epoch: 20, Batch: 370, Avg. Loss: 0.015950469687703066\n",
      "Epoch: 20, Batch: 380, Avg. Loss: 0.015945413941273807\n",
      "Epoch: 20, Batch: 390, Avg. Loss: 0.015940316844240448\n",
      "Epoch: 20, Batch: 400, Avg. Loss: 0.01593518898862784\n",
      "Epoch: 20, Batch: 410, Avg. Loss: 0.015930033361393772\n",
      "Epoch: 20, Batch: 420, Avg. Loss: 0.01592489950028575\n",
      "Epoch: 20, Batch: 430, Avg. Loss: 0.015919747881458914\n",
      "Epoch: 20, Batch: 440, Avg. Loss: 0.015914627090807137\n",
      "Epoch: 20, Batch: 450, Avg. Loss: 0.015909509721849215\n",
      "Epoch: 20, Batch: 460, Avg. Loss: 0.01590439050969016\n",
      "Epoch: 20, Batch: 470, Avg. Loss: 0.015899302884398024\n",
      "Epoch: 20, Batch: 480, Avg. Loss: 0.01589417590013524\n",
      "Epoch: 20, Batch: 490, Avg. Loss: 0.015889066796391748\n",
      "Epoch: 20, Batch: 500, Avg. Loss: 0.01588395568886113\n",
      "Epoch: 20, Batch: 510, Avg. Loss: 0.01587885253499292\n",
      "Epoch: 20, Batch: 520, Avg. Loss: 0.015873737848306806\n",
      "Epoch: 20, Batch: 530, Avg. Loss: 0.015868587415275186\n",
      "Epoch: 20, Batch: 540, Avg. Loss: 0.01586354152269238\n",
      "Epoch: 20, Batch: 550, Avg. Loss: 0.0158584431862034\n",
      "Epoch: 20, Batch: 560, Avg. Loss: 0.015853311368599177\n",
      "Epoch: 20, Batch: 570, Avg. Loss: 0.015848194963173846\n",
      "Epoch: 20, Batch: 580, Avg. Loss: 0.015843289819137598\n",
      "Epoch: 20, Batch: 590, Avg. Loss: 0.015838290848039414\n",
      "Epoch: 20, Batch: 600, Avg. Loss: 0.0158333133640661\n",
      "Epoch: 20, Batch: 610, Avg. Loss: 0.01582832768570217\n",
      "Epoch: 20, Batch: 620, Avg. Loss: 0.0158233371649201\n",
      "Epoch: 20, Batch: 630, Avg. Loss: 0.01581839268064531\n",
      "Epoch: 20, Batch: 640, Avg. Loss: 0.015813403260325605\n",
      "Epoch: 20, Batch: 650, Avg. Loss: 0.015808337419211442\n",
      "Epoch: 20, Batch: 660, Avg. Loss: 0.015803424065291217\n",
      "Epoch: 20, Batch: 670, Avg. Loss: 0.015798364643800075\n",
      "Epoch: 20, Batch: 680, Avg. Loss: 0.01579330451904171\n",
      "Epoch: 20, Batch: 690, Avg. Loss: 0.015788430622362338\n",
      "Epoch: 20, Batch: 700, Avg. Loss: 0.015783655798073833\n",
      "Epoch: 20, Batch: 710, Avg. Loss: 0.015778740393178833\n",
      "Epoch: 20, Batch: 720, Avg. Loss: 0.015773718743782673\n",
      "Epoch: 20, Batch: 730, Avg. Loss: 0.015768783653539748\n",
      "Epoch: 20, Batch: 740, Avg. Loss: 0.015763959272550615\n",
      "Epoch: 20, Batch: 750, Avg. Loss: 0.015758985698567624\n",
      "Epoch: 20, Batch: 760, Avg. Loss: 0.01575399262619995\n",
      "Epoch: 20, Batch: 770, Avg. Loss: 0.01574909748694795\n",
      "Epoch: 20, Batch: 780, Avg. Loss: 0.015744255262919046\n",
      "Epoch: 20, Batch: 790, Avg. Loss: 0.0157392730354179\n",
      "Epoch: 20, Batch: 800, Avg. Loss: 0.01573430659526047\n",
      "Epoch: 20, Batch: 810, Avg. Loss: 0.0157293947834457\n",
      "Epoch: 20, Batch: 820, Avg. Loss: 0.0157244783542483\n",
      "Epoch: 20, Batch: 830, Avg. Loss: 0.015719551474415335\n",
      "Epoch: 20, Batch: 840, Avg. Loss: 0.015714627885300585\n",
      "Epoch: 20, Batch: 850, Avg. Loss: 0.015709735999940515\n",
      "Epoch: 20, Batch: 860, Avg. Loss: 0.01570488488223714\n",
      "Epoch: 20, Batch: 870, Avg. Loss: 0.015700056258854494\n",
      "Epoch: 20, Batch: 880, Avg. Loss: 0.015695180625975292\n",
      "Epoch: 20, Batch: 890, Avg. Loss: 0.015690253224086084\n",
      "Epoch: 20, Batch: 900, Avg. Loss: 0.01568531000415282\n",
      "Epoch: 20, Batch: 910, Avg. Loss: 0.015680386237941282\n",
      "Epoch: 20, Batch: 920, Avg. Loss: 0.015675431751502898\n",
      "Epoch: 20, Batch: 930, Avg. Loss: 0.015670477241636477\n",
      "Epoch: 20, Batch: 940, Avg. Loss: 0.015665539757992612\n",
      "Epoch: 20, Batch: 950, Avg. Loss: 0.015660588971578335\n",
      "Epoch: 20, Batch: 960, Avg. Loss: 0.01565561063957926\n",
      "Epoch: 20, Batch: 970, Avg. Loss: 0.01565068534635333\n",
      "Epoch: 20, Batch: 980, Avg. Loss: 0.015645807968801828\n",
      "Epoch: 20, Batch: 990, Avg. Loss: 0.015640976133918152\n",
      "Epoch: 20, Batch: 1000, Avg. Loss: 0.015636332756953357\n",
      "Epoch: 20, Batch: 1010, Avg. Loss: 0.015631549766008893\n",
      "Epoch: 20, Batch: 1020, Avg. Loss: 0.01562704032120403\n",
      "Epoch: 20, Batch: 1030, Avg. Loss: 0.015622513172445075\n",
      "Epoch: 20, Batch: 1040, Avg. Loss: 0.015617992868288218\n",
      "Epoch: 20, Batch: 1050, Avg. Loss: 0.015613585127019207\n",
      "Epoch: 20, Batch: 1060, Avg. Loss: 0.01560913731398897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Batch: 1070, Avg. Loss: 0.015604521172527398\n",
      "Epoch: 20, Batch: 1080, Avg. Loss: 0.015600102965021434\n",
      "Epoch: 20, Batch: 1090, Avg. Loss: 0.015595932432611726\n",
      "Epoch: 20, Batch: 1100, Avg. Loss: 0.015591600814478352\n",
      "Epoch: 20, Batch: 1110, Avg. Loss: 0.015587414759615887\n",
      "Epoch: 20, Batch: 1120, Avg. Loss: 0.015582925950889117\n",
      "Epoch: 20, Batch: 1130, Avg. Loss: 0.015578645641608602\n",
      "Epoch: 20, Batch: 1140, Avg. Loss: 0.01557442236747857\n",
      "Epoch: 20, Batch: 1150, Avg. Loss: 0.01556982978463649\n",
      "Epoch: 20, Batch: 1160, Avg. Loss: 0.015565534849916874\n",
      "Epoch: 20, Batch: 1170, Avg. Loss: 0.015560872107543445\n",
      "Epoch: 20, Batch: 1180, Avg. Loss: 0.015556497724345834\n",
      "Epoch: 20, Batch: 1190, Avg. Loss: 0.015552112096519842\n",
      "Epoch: 20, Batch: 1200, Avg. Loss: 0.015547736592242751\n",
      "Epoch: 20, Batch: 1210, Avg. Loss: 0.01554330924676042\n",
      "Epoch: 20, Batch: 1220, Avg. Loss: 0.015538732408358116\n",
      "Epoch: 20, Batch: 1230, Avg. Loss: 0.015534323816721726\n",
      "Epoch: 20, Batch: 1240, Avg. Loss: 0.015529789637128578\n",
      "Epoch: 20, Batch: 1250, Avg. Loss: 0.01552522261831058\n",
      "Epoch: 20, Batch: 1260, Avg. Loss: 0.015520939751968848\n",
      "Epoch: 20, Batch: 1270, Avg. Loss: 0.015516472706690744\n",
      "Epoch: 20, Batch: 1280, Avg. Loss: 0.015512042124238812\n",
      "Epoch: 20, Batch: 1290, Avg. Loss: 0.01550846599965137\n",
      "Epoch: 20, Batch: 1300, Avg. Loss: 0.015504057992073444\n",
      "Epoch: 20, Batch: 1310, Avg. Loss: 0.015499569527162282\n",
      "Epoch: 20, Batch: 1320, Avg. Loss: 0.01549542558355502\n",
      "Epoch: 20, Batch: 1330, Avg. Loss: 0.015491013588630483\n",
      "Epoch: 20, Batch: 1340, Avg. Loss: 0.015486448183232708\n",
      "Epoch: 20, Batch: 1350, Avg. Loss: 0.015482319767668533\n",
      "Epoch: 20, Batch: 1360, Avg. Loss: 0.015477824503926277\n",
      "Epoch: 20, Batch: 1370, Avg. Loss: 0.015473337203532514\n",
      "Epoch: 20, Batch: 1380, Avg. Loss: 0.01546881388963549\n",
      "Epoch: 20, Batch: 1390, Avg. Loss: 0.015464437910159409\n",
      "Epoch: 20, Batch: 1400, Avg. Loss: 0.015460042611376814\n",
      "Epoch: 20, Batch: 1410, Avg. Loss: 0.015455699995763562\n",
      "Epoch: 20, Batch: 1420, Avg. Loss: 0.015451298993385225\n",
      "Epoch: 20, Batch: 1430, Avg. Loss: 0.015447022288201006\n",
      "Epoch: 20, Batch: 1440, Avg. Loss: 0.015442857663124502\n",
      "Epoch: 20, Batch: 1450, Avg. Loss: 0.015438466434219585\n",
      "Epoch: 20, Batch: 1460, Avg. Loss: 0.015433938110713221\n",
      "Epoch: 20, Batch: 1470, Avg. Loss: 0.015429674529989917\n",
      "Epoch: 20, Batch: 1480, Avg. Loss: 0.015425154975992722\n",
      "Epoch: 20, Batch: 1490, Avg. Loss: 0.01542086737730054\n",
      "Epoch: 20, Batch: 1500, Avg. Loss: 0.015416631198312356\n",
      "Epoch: 20, Batch: 1510, Avg. Loss: 0.015412099690979512\n",
      "Epoch: 20, Batch: 1520, Avg. Loss: 0.01540750173026485\n",
      "Epoch: 20, Batch: 1530, Avg. Loss: 0.015402871835348013\n",
      "Epoch: 20, Batch: 1540, Avg. Loss: 0.015398204654875524\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'def train(train_dataset, batch_size, epochs, learning_rate, wt_decay):\\n     # batch\\n     train_loader = data.DataLoader(train_dataset, batch_size)\\n     # \\n     model = FaceCNN()\\n     # \\n     loss_function = nn.CrossEntropyLoss()\\n     # \\n     optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=wt_decay)\\n     # \\n     # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\\n     # \\n     for epoch in range(epochs):\\n         # \\n         loss_rate = 0\\n         # scheduler.step() # \\n         model.train() # \\n         for images, labels in train_loader:\\n             # \\n             optimizer.zero_grad()\\n             # \\n             output = model.forward(images)\\n             # \\n             loss_rate = loss_function(output, labels)\\n             # \\n             loss_rate.backward()\\n             # \\n             optimizer.step()'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################################################\n",
    "# TODO - Train Your Model                                              #\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from exercise_code.networks.keypoint_nn import KeypointModel\n",
    "\n",
    "def train_net(n_epochs):\n",
    "\n",
    "    # prepare the net for training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    data_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        # train on batches of data, assumes you already have train_loader\n",
    "        for batch_i, data in enumerate(data_loader):\n",
    "            # get the input images and their corresponding labels\n",
    "            images = data['image']\n",
    "            key_pts = data['keypoints']\n",
    "\n",
    "            # flatten pts\n",
    "            key_pts = key_pts.view(key_pts.size(0), -1)\n",
    "\n",
    "            # convert variables to floats for regression loss\n",
    "            key_pts = key_pts.type(torch.FloatTensor)\n",
    "            images = images.type(torch.FloatTensor)\n",
    "            #print(key_pts[0], images[0])\n",
    "\n",
    "            # forward pass to get outputs\n",
    "            output_pts = model(images)\n",
    "\n",
    "            # calculate the loss between predicted and target keypoints\n",
    "            loss = criterion(output_pts, key_pts)\n",
    "\n",
    "            # zero the parameter (weight) gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backward pass to calculate the weight gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "\n",
    "            # print loss statistics\n",
    "            # to convert loss into a scalar and add it to the running_loss, use .item()\n",
    "            if batch_i % 10 == 9:    # print every 10 batches\n",
    "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1,\n",
    "                                                                   batch_i+1,\n",
    "                                                                   running_loss / (len(data_loader)*epoch+batch_i)))\n",
    "            \n",
    "            #return images, key_pts, output_pts\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    \n",
    "# Load model and run the solver\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-6)\n",
    "train_net(n_epochs=20)\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\"\"\"def train(train_dataset, batch_size, epochs, learning_rate, wt_decay):\n",
    "     # batch\n",
    "     train_loader = data.DataLoader(train_dataset, batch_size)\n",
    "     # \n",
    "     model = FaceCNN()\n",
    "     # \n",
    "     loss_function = nn.CrossEntropyLoss()\n",
    "     # \n",
    "     optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=wt_decay)\n",
    "     # \n",
    "     # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "     # \n",
    "     for epoch in range(epochs):\n",
    "         # \n",
    "         loss_rate = 0\n",
    "         # scheduler.step() # \n",
    "         model.train() # \n",
    "         for images, labels in train_loader:\n",
    "             # \n",
    "             optimizer.zero_grad()\n",
    "             # \n",
    "             output = model.forward(images)\n",
    "             # \n",
    "             loss_rate = loss_function(output, labels)\n",
    "             # \n",
    "             loss_rate.backward()\n",
    "             # \n",
    "             optimizer.step()\"\"\"\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're done training, run the cells below to visualize some predictions of your model, and to compute a validation score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [12, 1, 3, 3], but got 3-dimensional input of size [1, 96, 96] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c82372872620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_keypoint_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-75842e6d3db6>\u001b[0m in \u001b[0;36mshow_keypoint_predictions\u001b[0;34m(model, dataset, num_samples)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mpredicted_keypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mshow_all_keypoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_pts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_keypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exercise_09/exercise_code/networks/keypoint_nn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m#x=x.view(-1,1,96,96)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m#x = self.layers(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# \u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [12, 1, 3, 3], but got 3-dimensional input of size [1, 96, 96] instead"
     ]
    }
   ],
   "source": [
    "show_keypoint_predictions(model, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 179.99873681217838\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataset):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss = 0\n",
    "    for batch in dataloader:\n",
    "        image, keypoints = batch[\"image\"], batch[\"keypoints\"]\n",
    "        predicted_keypoints = model(image).view(-1,15,2)\n",
    "        loss += criterion(\n",
    "            torch.squeeze(keypoints),\n",
    "            torch.squeeze(predicted_keypoints)\n",
    "        ).item()\n",
    "    return 1.0 / (2 * (loss/len(dataloader)))\n",
    "\n",
    "\n",
    "print(\"Score:\", evaluate_model(model, val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Save Your Model for Submission\n",
    "\n",
    "If your model achieved a validation score of 100 or higher, save your model with the cell below and submit it to [the submission server](https://i2dl.vc.in.tum.de). Your validation set is of course different from the test set on our server, so results may vary. Nevertheless, you will have a reasonable close approximation about your performance.\n",
    "\n",
    "Before that, we will check again whether the number of parameters is below 5 Mio. and the file size is below 20 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/facial_keypoints.p'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(model, \"facial_keypoints.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats - you've now finished your first Convolution Neural Network! Simply run the following cell to create a zipped file for your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant folders: ['exercise_code', 'models']\n",
      "notebooks files: ['Optional-spatial_batchnorm.ipynb', '1_facial_keypoints.ipynb']\n",
      "Adding folder exercise_code\n",
      "Adding folder models\n",
      "Adding notebook Optional-spatial_batchnorm.ipynb\n",
      "Adding notebook 1_facial_keypoints.ipynb\n",
      "Zipping successful! Zip is stored under: /Users/meiqiliu/Downloads/exercise_09/exercise09.zip\n"
     ]
    }
   ],
   "source": [
    "# Now zip the folder for upload\n",
    "from exercise_code.util.submit import submit_exercise\n",
    "\n",
    "submit_exercise('exercise09')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Goals\n",
    "\n",
    "- Goal: Implement and train a convolution neural network for facial keypoint detection.\n",
    "- Passing Criteria: Reach **Score >= 100** on __our__ test dataset. The submission system will show you your score after you submit.\n",
    "\n",
    "- Submission start: __Thursday, June 10, 2021 - 13:00__\n",
    "- Submission deadline: __Wednesday, June 16, 2021 - 15:59__ \n",
    "- You can make **$\\infty$** submissions until the deadline. Your __best submission__ will be considered for bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
